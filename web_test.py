import os
import re
import streamlit as st
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, AIMessage
from streamlit_mermaid import st_mermaid

# --- é¡µé¢é…ç½® (å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤) ---
st.set_page_config(
    page_title="æ™ºæ…§åŒ–èŒä¸šå‘å±•è¾…å¯¼ç³»ç»Ÿ",
    page_icon="âœ¨",
    layout="wide"
)

# --- UI ç¾åŒ– CSS æ ·å¼ ---
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;500;700&display=swap');
    html, body, [class*="st-"] {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Noto Sans SC", "Helvetica Neue", "PingFang SC", "Microsoft YaHei", sans-serif;
        line-height: 1.65;
    }
    .stApp { background-color: #F8F9FA; }
    h1, h2, h3, h4, h5, h6 { color: #212529; font-weight: 700; }
    h1 { font-size: 32px; }
    h2 { font-size: 28px; border-bottom: 2px solid #E9ECEF; padding-bottom: 0.4em; }
    h3 { font-size: 22px; }
    .st-emotion-cache-z5fcl4 { background-color: #FFFFFF; border-radius: 12px; padding: 28px; box-shadow: 0 4px 12px rgba(0,0,0,0.05); border: 1px solid #E9ECEF; transition: transform 0.3s ease, box-shadow 0.3s ease; }
    .st-emotion-cache-z5fcl4:hover { transform: translateY(-5px); box-shadow: 0 12px 24px rgba(0,0,0,0.08); }
    .stButton>button { border-radius: 8px; border: none; color: white; font-weight: 500; padding: 12px 24px; background-image: linear-gradient(135deg, #5D9CEC 0%, #4A90E2 100%); transition: all 0.3s ease; box-shadow: 0 4px 15px rgba(74, 144, 226, 0.2); }
    .stButton>button:hover { transform: translateY(-2px); box-shadow: 0 6px 20px rgba(74, 144, 226, 0.3); }
    .stChatMessage { border-radius: 12px; border: 1px solid #E9ECEF; background-color: #FFFFFF; padding: 16px; margin-bottom: 1rem; }
    .stChatInputContainer { position: sticky; bottom: 0; background-color: #FFFFFF; padding: 12px 0px; border-top: 1px solid #E9ECEF; box-shadow: 0 -4px 12px rgba(0,0,0,0.05); }
</style>
""", unsafe_allow_html=True)

# --- åˆå§‹åŒ– ---
load_dotenv()
os.environ["LANGCHAIN_TRACING_V2"] = "false"

# --- å…¨å±€ç³»ç»Ÿè§’è‰² (ç®€ä½“ä¸­æ–‡) ---
GLOBAL_PERSONA = "æ ¸å¿ƒè§’è‰²: ä½ æ˜¯ä¸€ä½æ™ºæ…§ã€ä¸“ä¸šä¸”å¯Œæœ‰åŒç†å¿ƒçš„èŒä¸šå‘å±•æ•™ç»ƒä¸æˆ˜ç•¥è§„åˆ’å¸ˆã€‚\nè¯­è¨€è¦æ±‚: ä½ çš„æ‰€æœ‰å›ç­”éƒ½å¿…é¡»ä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚"


# --- LLM åˆå§‹åŒ– ---
@st.cache_resource
def get_llm_instance():
    api_key = None;
    key_name = "VOLCENGINE_API_KEY"
    try:
        api_key = st.secrets[key_name]
    except (KeyError, FileNotFoundError):
        api_key = os.getenv(key_name)
    if not api_key:
        st.error(f"é”™è¯¯ï¼šæœªæ‰¾åˆ° {key_name}ã€‚è¯·åœ¨ Streamlit Cloud Secrets æˆ–æœ¬åœ° .env æ–‡ä»¶ä¸­è®¾ç½®å®ƒã€‚")
        return None
    try:
        llm = ChatOpenAI(model="deepseek-r1-250528", temperature=0.7, api_key=api_key,
                         base_url="https://ark.cn-beijing.volces.com/api/v3")
        llm.invoke("Hello");
        return llm
    except Exception as e:
        st.error(f"åˆå§‹åŒ–æ¨¡å‹æ—¶å‡ºé”™: {e}"); return None


# --- ä¼šè¯çŠ¶æ€ç®¡ç† ---
def init_session_state():
    defaults = {"current_mode": "menu", "chat_history": {}, "exploration_stage": 1, "sim_started": False,
                "debrief_requested": False, "panoramic_stage": 1, "user_profile": None, "chosen_professions": None,
                "chosen_region": None}
    for key, value in defaults.items():
        if key not in st.session_state: st.session_state[key] = value


init_session_state()


def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in st.session_state.chat_history: st.session_state.chat_history[session_id] = ChatMessageHistory()
    return st.session_state.chat_history[session_id]


# --- UI æ¸²æŸ“å‡½æ•° ---
def render_menu():
    st.title("âœ¨ æ™ºæ…§åŒ–èŒä¸šå‘å±•è¾…å¯¼ç³»ç»Ÿ")
    st.markdown("---");
    st.subheader("æ¬¢è¿ä½¿ç”¨ï¼è¯·é€‰æ‹©ä¸€é¡¹åŠŸèƒ½å¼€å§‹æ¢ç´¢ï¼š");
    st.write("")
    modes_config = [
        ("exploration", ":compass: èŒä¸šç›®æ ‡æ¢ç´¢", "é€šè¿‡â€œæˆ‘-ç¤¾ä¼š-å®¶åº­â€æ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°æ¢ç´¢å†…åœ¨åŠ¨æœºä¸å¤–åœ¨æœºä¼šã€‚"),
        ("panoramic", ":globe_with_meridians: èŒä¸šè·¯å¾„å…¨æ™¯è§„åˆ’",
         "ä»æ‚¨çš„æ ¸å¿ƒèƒ½åŠ›å‡ºå‘ï¼Œè¿æ¥èŒä¸šã€ä¼ä¸šã€åœ°åŒºä¸äº§ä¸šé“¾ï¼Œç”Ÿæˆæ‚¨çš„ä¸ªäººå‘å±•è“å›¾ã€‚"),
        ("decision", ":balance_scale: Offer å†³ç­–åˆ†æ", "ç»“æ„åŒ–å¯¹æ¯”å¤šä¸ªOfferï¼Œè·å¾—æ¸…æ™°çš„å†³ç­–å»ºè®®ã€‚"),
        ("company_info", ":office: ä¼ä¸šä¿¡æ¯é€Ÿè§ˆ", "å¿«é€Ÿäº†è§£ç›®æ ‡å…¬å¸çš„æ ¸å¿ƒä¸šåŠ¡ã€è¿‘æœŸåŠ¨æ€ä¸çƒ­æ‹›æ–¹å‘ã€‚"),
        ("communication", ":family: å®¶åº­æ²Ÿé€šæ¨¡æ‹Ÿ", "ä¸AIæ‰®æ¼”çš„å®¶äººè¿›è¡Œå¯¹è¯ï¼Œå®‰å…¨åœ°ç»ƒä¹ å¦‚ä½•è¡¨è¾¾æ‚¨çš„èŒä¸šé€‰æ‹©ã€‚")
    ]
    col1, col2 = st.columns([2, 1], gap="large")
    with col1:
        with st.container(border=True):
            st.subheader(modes_config[0][1]);
            st.caption(modes_config[0][2])
            if st.button("å¼€å§‹æ¢ç´¢", use_container_width=True, key=f"menu_{modes_config[0][0]}"):
                st.session_state.current_mode = modes_config[0][0];
                st.rerun()
        st.write("")
        with st.container(border=True):
            st.subheader(modes_config[1][1]);
            st.caption(modes_config[1][2])
            if st.button("å¼€å§‹è§„åˆ’", use_container_width=True, key=f"menu_{modes_config[1][0]}"):
                st.session_state.current_mode = modes_config[1][0];
                st.rerun()
    with col2:
        for mode_key, title, caption in modes_config[2:]:
            with st.container(border=True):
                st.subheader(title);
                st.caption(caption)
                if st.button(f"å¼€å§‹{title.split(' ')[1][:2]}", use_container_width=True, key=f"menu_{mode_key}"):
                    st.session_state.current_mode = mode_key;
                    st.rerun()
            st.write("")


def render_exploration_mode(llm):
    st.header("æ¨¡å¼ä¸€: èŒä¸šç›®æ ‡æ¢ç´¢")
    history = get_session_history("exploration_session")
    stage = st.session_state.get('exploration_stage', 1)
    for msg in history.messages:
        avatar = "ğŸ§‘â€ğŸ’»" if isinstance(msg, HumanMessage) else "ğŸ¤–"
        st.chat_message(msg.type, avatar=avatar).markdown(msg.content, unsafe_allow_html=True)

    # ã€V10.1 æ ¸å¿ƒä¿®æ­£ã€‘æ¢å¤äº†åŸå§‹ç‰ˆæœ¬ä¸­å®Œæ•´ã€æ¸…æ™°çš„å¼•å¯¼æ–‡æ¡ˆå’Œé—®é¢˜
    prompts = {
        1: {
            "title": "> **ç¬¬ä¸€é˜¶æ®µï¼šåˆ†æâ€œæˆ‘â€(å¯æ§å› ç´ )**\n> \n> ä½ å¥½ï¼æˆ‘å°†å¼•å¯¼ä½ ä½¿ç”¨â€œèŒä¸šç›®æ ‡ç¼˜èµ·åˆ†ææ¡†æ¶â€ï¼Œä»â€œæˆ‘â€ã€â€œç¤¾ä¼šâ€ã€â€œå®¶åº­â€ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œç³»ç»Ÿæ€§åœ°æ¢ç´¢ä½ çš„èŒä¸šæ–¹å‘ã€‚\n> \n> é¦–å…ˆï¼Œæˆ‘ä»¬æ¥åˆ†æâ€œæˆ‘â€è¿™ä¸ªæ ¸å¿ƒã€‚è¯·åœ¨ä¸‹æ–¹å›ç­”ï¼š",
            "questions": ["1. ä½ çš„ä¸“ä¸šæ˜¯ä»€ä¹ˆï¼Ÿä½ å¯¹å®ƒçš„çœ‹æ³•å¦‚ä½•ï¼Ÿ", "2. ä½ çš„å­¦æ ¡æˆ–è¿‡å¾€ç»å†ï¼Œä¸ºä½ æä¾›äº†æ€æ ·çš„å¹³å°ä¸åŸºç¡€ï¼Ÿ"],
            "button_text": "æäº¤å…³äºâ€œæˆ‘â€çš„åˆ†æ"
        },
        2: {
            "title": "> **ç¬¬äºŒé˜¶æ®µï¼šåˆ†æâ€œç¤¾ä¼šâ€(å¤–éƒ¨æœºä¼š)**\n> \n> å¥½çš„ï¼Œæˆ‘ä»¬ç›˜ç‚¹äº†â€œæˆ‘â€çš„åŸºç¡€ã€‚æ¥ç€ï¼Œæˆ‘ä»¬æ¥åˆ†æå¤–éƒ¨çš„â€œç¤¾ä¼šâ€å› ç´ ã€‚è¯·æ€è€ƒï¼š",
            "questions": ["1. ä½ è§‚å¯Ÿåˆ°å½“ä¸‹æœ‰å“ªäº›ä½ æ„Ÿå…´è¶£çš„ç¤¾ä¼šæˆ–ç§‘æŠ€è¶‹åŠ¿ï¼Ÿï¼ˆä¾‹å¦‚ï¼šAIã€å¤§å¥åº·ã€å¯æŒç»­å‘å±•ç­‰ï¼‰",
                          "2. æ ¹æ®ä½ çš„è§‚å¯Ÿï¼Œè¿™äº›è¶‹åŠ¿å¯èƒ½å¸¦æ¥å“ªäº›æ–°çš„è¡Œä¸šæˆ–èŒä½æœºä¼šï¼Ÿ",
                          "3. åœ¨ä½ è¿‡å¾€çš„ç»å†ä¸­ï¼Œæœ‰æ²¡æœ‰ä¸€äº›å¶ç„¶çš„æœºç¼˜æˆ–æ‰“å·¥ç»éªŒï¼Œè®©ä½ å¯¹æŸä¸ªé¢†åŸŸäº§ç”Ÿäº†ç‰¹åˆ«çš„äº†è§£ï¼Ÿ"],
            "button_text": "æäº¤å…³äºâ€œç¤¾ä¼šâ€çš„åˆ†æ"
        },
        3: {
            "title": "> **ç¬¬ä¸‰é˜¶æ®µï¼šè§‰å¯Ÿâ€œå®¶åº­â€(ç¯å¢ƒå½±å“)**\n> \n> æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥æ¢è®¨éœ€è¦æŒç»­â€œè§‰å¯Ÿâ€çš„â€œå®¶åº­â€ä¸ç¯å¢ƒå½±å“ã€‚è¯·æè¿°ï¼š",
            "questions": ["1. ä½ çš„å®¶åº­æˆ–é‡è¦äº²å‹ï¼Œå¯¹ä½ çš„èŒä¸šæœ‰ä»€ä¹ˆæ ·çš„æœŸå¾…ï¼Ÿ",
                          "2. æœ‰æ²¡æœ‰å“ªä½æ¦œæ ·å¯¹ä½ çš„èŒä¸šé€‰æ‹©äº§ç”Ÿäº†å½±å“ï¼Ÿ",
                          "3. ä½ èº«è¾¹çš„â€œåœˆå­â€ï¼ˆä¾‹å¦‚æœ‹å‹ã€åŒå­¦ï¼‰ä¸»è¦ä»äº‹å“ªäº›å·¥ä½œï¼Ÿè¿™å¯¹ä½ æœ‰ä»€ä¹ˆæ½œåœ¨å½±å“ï¼Ÿ"],
            "button_text": "æäº¤å…³äºâ€œå®¶åº­â€çš„åˆ†æ"
        },
    }

    if stage in prompts:
        config = prompts[stage]
        st.markdown(config["title"])
        with st.form(f"stage{stage}_form"):
            responses = [st.text_area(q, height=100) for q in config["questions"]]
            if st.form_submit_button(config["button_text"], use_container_width=True):
                if all(responses):
                    input_text = f"### å…³äºé˜¶æ®µ {stage} çš„å›ç­”\n\n" + "\n\n".join(
                        [f"**{q}**\n{r}" for q, r in zip(config["questions"], responses)])
                    history.add_user_message(input_text);
                    st.session_state.exploration_stage += 1;
                    st.rerun()
                else:
                    st.warning("è¯·å®Œæ•´å¡«å†™æ‰€æœ‰é—®é¢˜çš„å›ç­”ã€‚")
    elif stage == 4:
        st.markdown("> **ç¬¬å››é˜¶æ®µï¼šAI æ™ºæ…§æ•´åˆä¸è¡ŒåŠ¨è®¡åˆ’**")
        with st.chat_message("ai", avatar="ğŸ¤–"):
            full_conversation = "\n\n".join([msg.content for msg in history.messages if isinstance(msg, HumanMessage)])
            stage4_prompt = ChatPromptTemplate.from_template(
                GLOBAL_PERSONA + "ä½œä¸ºä¸€åæ™ºæ…§ä¸”å¯Œæœ‰æ´å¯ŸåŠ›çš„èŒä¸šå‘å±•æ•™ç»ƒï¼Œè¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹ç”¨æˆ·åœ¨â€œæˆ‘â€ã€â€œç¤¾ä¼šâ€ã€â€œå®¶åº­â€ä¸‰ä¸ªé˜¶æ®µçš„å®Œæ•´å›ç­”ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆä¸€ä»½æ•´åˆåˆ†æä¸å»ºè®®æŠ¥å‘Šã€‚æŠ¥å‘Šå¿…é¡»åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†ï¼Œå¹¶ä½¿ç”¨æ¸…æ™°çš„Markdownæ ¼å¼ï¼š\n\n### 1. åˆæ­¥å†³ç­–æ–¹å‘å»ºè®®\n- ...\n\n### 2. é¢„æœŸâ€œæ”¶å…¥â€åˆ†æ\n- ...\n\n### 3. ç¬¬ä¸€ä¸ªâ€œè¡ŒåŠ¨â€å»ºè®®\n- ...\n\n---\nä»¥ä¸‹æ˜¯ç”¨æˆ·çš„å®Œæ•´å›ç­”: \n{conversation_history}\n---")
            stage4_chain = stage4_prompt | llm
            with st.spinner("AIæ•™ç»ƒæ­£åœ¨å…¨é¢åˆ†ææ‚¨çš„å›ç­”..."):
                response_content = st.write_stream(stage4_chain.stream({"conversation_history": full_conversation}))
            history.add_ai_message(response_content)
        st.session_state.exploration_stage = 5;
        st.rerun()
    elif stage == 5:
        st.markdown(
            "> AIæ•™ç»ƒå·²æ ¹æ®æ‚¨çš„å›ç­”ï¼Œä¸ºæ‚¨æä¾›äº†ä¸€ä»½æ•´åˆåˆ†æä¸å»ºè®®ã€‚è¿™ä»½æŠ¥å‘Šæ˜¯ä¸ºæ‚¨é‡èº«æ‰“é€ çš„èµ·ç‚¹ï¼Œè€Œéç»ˆç‚¹ã€‚\n>\n> è¯·ä»”ç»†é˜…è¯»æŠ¥å‘Šï¼Œç„¶åå›ç­”æœ€åä¸€ä¸ªã€ä¹Ÿæ˜¯æœ€é‡è¦çš„é—®é¢˜ï¼š\n> **æ‚¨è‡ªå·±å†³å®šè¦é‡‡å–çš„ã€ä¸‹å‘¨å¯ä»¥å®Œæˆçš„ç¬¬ä¸€ä¸ªå…·ä½“è¡ŒåŠ¨æ˜¯ä»€ä¹ˆï¼Ÿ**")
        if user_input := st.chat_input("è¯·åœ¨æ­¤è¾“å…¥æ‚¨çš„æœ€ç»ˆè¡ŒåŠ¨è®¡åˆ’..."):
            history.add_user_message(user_input);
            st.session_state.exploration_stage = 6;
            st.rerun()
    elif stage == 6:
        st.success("æ­å–œï¼æ‚¨å·²å®Œæˆæœ¬æ¬¡æ¢ç´¢çš„å…¨è¿‡ç¨‹ã€‚")


def render_decision_mode(llm):
    st.header("æ¨¡å¼äºŒ: Offer å†³ç­–åˆ†æ")
    with st.container(border=True):
        st.info("è¯·è¾“å…¥ä¸¤ä¸ªOfferçš„å…³é”®ä¿¡æ¯ï¼ŒAIå°†ä¸ºæ‚¨ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„å¯¹æ¯”åˆ†ææŠ¥å‘Šã€‚")
        chain = ChatPromptTemplate.from_template(
            GLOBAL_PERSONA + "You are an expert career advisor. Your task is to conduct a structured analysis of two job offers for a user. Offer A Details: {offer_a_details}. Offer B Details: {offer_b_details}. User Priorities: {user_priorities_sorted_list}. Please create a markdown report with: 1. Comparison Table. 2. Priority Matching Analysis. 3. Pros and Cons. 4. Risk Alert. 5. Recommendation.") | llm
        st.subheader("ç¬¬ä¸€æ­¥ï¼šè¯·å¡«å†™ Offer çš„æ ¸å¿ƒä¿¡æ¯")
        col1, col2 = st.columns(2, gap="large")
        with col1:
            offer_a = st.text_area("Offer A å…³é”®ä¿¡æ¯", height=200,
                                   placeholder="ä¾‹å¦‚: å…¬å¸åã€èŒä½ã€è–ªèµ„ã€åœ°ç‚¹ã€ä¼˜ç‚¹ã€é¡¾è™‘ç­‰")
        with col2:
            offer_b = st.text_area("Offer B å…³é”®ä¿¡æ¯", height=200,
                                   placeholder="åŒæ ·ï¼ŒåŒ…æ‹¬å…¬å¸åã€èŒä½ã€è–ªèµ„ã€åœ°ç‚¹ã€ä¼˜ç‚¹ã€é¡¾è™‘ç­‰")
        st.subheader("ç¬¬äºŒæ­¥ï¼š(å¯é€‰) æ·»åŠ ä½ çš„ä¸ªäººåå¥½")
        priorities_options = ["èŒä¸šæˆé•¿", "è–ªèµ„ç¦åˆ©", "å·¥ä½œç”Ÿæ´»å¹³è¡¡", "å›¢é˜Ÿæ°›å›´", "å…¬å¸ç¨³å®šæ€§"]
        user_priorities = st.multiselect("è¯·æŒ‰é‡è¦æ€§ä¾æ¬¡é€‰æ‹©ä½ çš„èŒä¸šåå¥½ï¼š", options=priorities_options,
                                         help="æ‚¨é€‰æ‹©çš„ç¬¬ä¸€ä¸ªé€‰é¡¹ä»£è¡¨æ‚¨æœ€çœ‹é‡çš„å› ç´ ã€‚")
        if st.button("ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š", use_container_width=True):
            if not offer_a or not offer_b:
                st.warning("è¯·è¾“å…¥ä¸¤ä¸ªOfferçš„ä¿¡æ¯ã€‚")
            else:
                with st.spinner("æ­£åœ¨ä¸ºæ‚¨ç”ŸæˆOfferåˆ†ææŠ¥å‘Š..."):
                    priorities_text = ", ".join(user_priorities) if user_priorities else "ç”¨æˆ·æœªæŒ‡å®š"
                    response_stream = chain.stream({"offer_a_details": offer_a, "offer_b_details": offer_b,
                                                    "user_priorities_sorted_list": priorities_text})
                    st.markdown("---");
                    st.subheader("ğŸ“‹ Offerå¯¹æ¯”åˆ†ææŠ¥å‘Š");
                    st.write_stream(response_stream)


def render_communication_mode(llm):
    st.header("æ¨¡å¼ä¸‰: å®¶åº­æ²Ÿé€šæ¨¡æ‹Ÿ")
    if not st.session_state.get('sim_started', False):
        with st.container(border=True):
            st.info("åœ¨è¿™é‡Œï¼ŒAIå¯ä»¥æ‰®æ¼”æ‚¨çš„å®¶äººï¼Œå¸®åŠ©æ‚¨ç»ƒä¹ å¦‚ä½•æ²Ÿé€šèŒä¸šè§„åˆ’ï¼Œå¹¶æä¾›å¤ç›˜å»ºè®®ã€‚")
            my_choice = st.text_input("é¦–å…ˆ, è¯·å‘Šè¯‰æˆ‘ä½ æƒ³è¦å’Œå®¶äººæ²Ÿé€šçš„èŒä¸šé€‰æ‹©æ˜¯ä»€ä¹ˆ?")
            family_concern = st.text_area("ä½ è®¤ä¸ºä»–ä»¬ä¸»è¦çš„æ‹…å¿§ä¼šæ˜¯ä»€ä¹ˆ?",
                                          placeholder="ä¾‹å¦‚: å·¥ä½œä¸ç¨³å®šã€ä¸æ˜¯é“é¥­ç¢—ã€ç¦»å®¶å¤ªè¿œç­‰")
            if st.button("å¼€å§‹æ¨¡æ‹Ÿ"):
                if not my_choice or not family_concern:
                    st.warning("è¯·è¾“å…¥æ‚¨çš„èŒä¸šé€‰æ‹©å’Œé¢„æƒ³çš„å®¶äººæ‹…å¿§ã€‚")
                else:
                    st.session_state.my_choice = my_choice;
                    st.session_state.family_concern = family_concern
                    st.session_state.sim_started = True;
                    st.session_state.debrief_requested = False
                    st.session_state.chat_history['communication_session'] = ChatMessageHistory()
                    initial_ai_prompt = f"å­©å­ï¼Œå…³äºä½ æƒ³åšâ€œ{my_choice}â€è¿™ä¸ªäº‹ï¼Œæˆ‘æœ‰äº›æ‹…å¿ƒã€‚æˆ‘ä¸»è¦æ˜¯è§‰å¾—å®ƒâ€œ{family_concern}â€ã€‚æˆ‘ä»¬èƒ½èŠèŠå—ï¼Ÿ"
                    get_session_history("communication_session").add_ai_message(initial_ai_prompt);
                    st.rerun()
    if st.session_state.get('sim_started', False):
        st.success(f"æ¨¡æ‹Ÿå¼€å§‹ï¼AIæ­£åœ¨æ‰®æ¼”æ‹…å¿§æ‚¨é€‰æ‹© â€œ{st.session_state.my_choice}â€ çš„å®¶äººã€‚")
        history = get_session_history("communication_session")
        with st.container():
            for msg in history.messages:
                avatar = "ğŸ§‘â€ğŸ’»" if isinstance(msg, HumanMessage) else "ğŸ§“";
                st.chat_message(msg.type, avatar=avatar).markdown(msg.content)
        if not st.session_state.get('debrief_requested', False):
            communication_prompt = ChatPromptTemplate.from_messages([("system",
                                                                      GLOBAL_PERSONA + f"""ç°åœ¨ï¼Œä½ å°†æ‰®æ¼”ä¸€ä¸ªå…³å¿ƒå­©å­ä½†æ€æƒ³ç•¥æ˜¾ä¼ ç»Ÿçš„å®¶äººã€‚- ä½ çš„æ ¸å¿ƒæ‹…å¿§æ˜¯: "{st.session_state.family_concern}"- ä½ çš„å¯¹è¯ç›®æ ‡æ˜¯ï¼šåå¤ç¡®è®¤å­©å­æ˜¯å¦è€ƒè™‘æ¸…æ¥šäº†è¿™äº›æ‹…å¿§ï¼Œè€Œä¸æ˜¯è½»æ˜“è¢«è¯´æœã€‚"""),
                                                                     MessagesPlaceholder(variable_name="history"),
                                                                     ("human", "{input}")])
            chain_with_history = RunnableWithMessageHistory(communication_prompt | llm,
                                                            lambda s: get_session_history(s),
                                                            input_messages_key="input", history_messages_key="history")
            if user_input := st.chat_input("ä½ çš„å›åº”:"):
                with st.spinner("..."): chain_with_history.invoke({"input": user_input}, config={
                    "configurable": {"session_id": "communication_session"}}); st.rerun()
            if len(history.messages) > 2:
                if st.button("ç»“æŸæ¨¡æ‹Ÿå¹¶è·å–å¤ç›˜å»ºè®®"): st.session_state.debrief_requested = True; st.rerun()
        else:
            with st.container(border=True):
                st.info("å¯¹è¯å·²ç»“æŸã€‚AIæ•™ç»ƒæ­£åœ¨ä¸ºæ‚¨å¤ç›˜åˆšæ‰çš„æ²Ÿé€šè¡¨ç°...")
                full_conversation = "\n".join(
                    [f"{'æˆ‘' if isinstance(msg, HumanMessage) else 'å®¶äºº'}: {msg.content}" for msg in history.messages])
                debrief_prompt = ChatPromptTemplate.from_template(
                    GLOBAL_PERSONA + "ä½ ç°åœ¨åˆ‡æ¢å›èŒä¸šå‘å±•æ•™ç»ƒçš„è§’è‰²ã€‚å¯¹ä»¥ä¸‹æ²Ÿé€šè®°å½•è¿›è¡Œå¤ç›˜ï¼Œå¿…é¡»åŒ…å«ï¼š\n\n### 1. æ²Ÿé€šäº®ç‚¹\n\n### 2. å¯ä¼˜åŒ–ç‚¹\n\n### 3. å…·ä½“è¯æœ¯å»ºè®®\n\n---\nå¯¹è¯è®°å½•:\n{conversation_history}\n---")
                debrief_chain = debrief_prompt | llm
                with st.spinner("æ­£åœ¨ç”Ÿæˆæ²Ÿé€šå¤ç›˜æŠ¥å‘Š..."):
                    response_stream = debrief_chain.stream(
                        {"my_choice": st.session_state.my_choice, "family_concern": st.session_state.family_concern,
                         "conversation_history": full_conversation})
                    st.subheader("ğŸ“‹ æ²Ÿé€šè¡¨ç°å¤ç›˜æŠ¥å‘Š");
                    st.write_stream(response_stream)


def render_company_info_mode(llm):
    st.header("æ¨¡å¼å››: ä¼ä¸šä¿¡æ¯é€Ÿè§ˆ")
    with st.container(border=True):
        st.info("è¯·è¾“å…¥å…¬å¸å…¨åï¼ŒAIå°†ä¸ºæ‚¨ç”Ÿæˆä¸€ä»½æ ¸å¿ƒä¿¡æ¯é€Ÿè§ˆæŠ¥å‘Šã€‚")
        chain = ChatPromptTemplate.from_template(
            GLOBAL_PERSONA + "You are a professional business analyst AI. Your task is to provide a concise and structured overview of a given company. Company Name: {company_name}. Please structure your report in markdown with these sections: ### 1. å…¬å¸ç®€ä»‹, ### 2. è¿‘æœŸåŠ¨æ€ä¸æ–°é—», ### 3. ä¼ä¸šæ–‡åŒ–ä¸ä»·å€¼è§‚, ### 4. çƒ­é—¨æ‹›è˜æ–¹å‘.") | llm
        company_name = st.text_input("è¯·è¾“å…¥å…¬å¸åç§°:", placeholder="ä¾‹å¦‚ï¼šé˜¿é‡Œå·´å·´ã€è…¾è®¯ã€å­—èŠ‚è·³åŠ¨")
        if st.button("ç”Ÿæˆé€Ÿè§ˆæŠ¥å‘Š", use_container_width=True):
            if not company_name:
                st.warning("è¯·è¾“å…¥å…¬å¸åç§°ã€‚")
            else:
                with st.spinner(f"æ­£åœ¨ç”Ÿæˆå…³äºâ€œ{company_name}â€çš„ä¿¡æ¯æŠ¥å‘Š..."):
                    response_stream = chain.stream({"company_name": company_name})
                    st.markdown("---");
                    st.subheader(f"ğŸ“„ {company_name} - æ ¸å¿ƒä¿¡æ¯é€Ÿè§ˆ");
                    st.write_stream(response_stream)


def render_panoramic_mode(llm):
    st.header("æ¨¡å¼äº”: èŒä¸šè·¯å¾„å…¨æ™¯è§„åˆ’")
    history = get_session_history("panoramic_session")
    stage = st.session_state.get('panoramic_stage', 1)
    for msg in history.messages:
        avatar = "ğŸ§‘â€ğŸ’»" if isinstance(msg, HumanMessage) else "ğŸ¤–"
        with st.chat_message(msg.type, avatar=avatar):
            if msg.type == 'ai' and "```mermaid" in msg.content:
                parts = msg.content.split("```mermaid");
                st.markdown(parts[0])
                mermaid_section = "```mermaid" + parts[1];
                mermaid_match = re.search("```mermaid\n(.*?)\n```", mermaid_section, re.DOTALL)
                if mermaid_match:
                    mermaid_code = mermaid_match.group(1);
                    st.subheader("äº§ä¸šé“¾å¯è§†åŒ–å›¾è¡¨")
                    with st.container(border=True): st_mermaid(mermaid_code.strip())
                after_diagram_content = mermaid_section.split("```")[-1]
                if after_diagram_content.strip(): st.markdown(after_diagram_content)
            else:
                st.markdown(msg.content)

    meta_prompt_template = GLOBAL_PERSONA + """
    You are an expert career strategist... (æŒ‡ä»¤åŒV9.3ç‰ˆæœ¬)
    """
    chain = ChatPromptTemplate.from_template(meta_prompt_template) | llm
    if stage == 1:
        st.markdown("> ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„èŒä¸šè·¯å¾„è§„åˆ’åŠ©æ‰‹ã€‚è®©æˆ‘ä»¬ä»è®¤è¯†ä½ è‡ªå·±å¼€å§‹ã€‚")
        with st.form("profile_form"):
            st.subheader("è¯·æ ¹æ®ä»¥ä¸‹äº”ä¸ªç»´åº¦ï¼Œæè¿°ä½ çš„â€œæ ¸å¿ƒèƒ½åŠ›â€ï¼š")
            edu = st.text_area("å­¦å†èƒŒæ™¯", placeholder="ä½ çš„ä¸“ä¸šã€å­¦ä½ã€ä»¥åŠç›¸å…³çš„æ ¸å¿ƒè¯¾ç¨‹")
            skills = st.text_area("æ ¸å¿ƒæŠ€èƒ½", placeholder="ä½ æœ€æ“…é•¿çš„3-5é¡¹ç¡¬æŠ€èƒ½æˆ–è½¯æŠ€èƒ½")
            exp = st.text_area("ç›¸å…³ç»éªŒ", placeholder="ç›¸å…³çš„å®ä¹ ã€å·¥ä½œé¡¹ç›®ã€æˆ–ä¸ªäººä½œå“é›†")
            char = st.text_area("å“è¡Œç‰¹è´¨", placeholder="ä½ è®¤ä¸ºè‡ªå·±æœ€é‡è¦çš„èŒä¸šå“è¡Œæˆ–å·¥ä½œé£æ ¼")
            motiv = st.text_area("å†…åœ¨åŠ¨æœº", placeholder="åœ¨å·¥ä½œä¸­ï¼Œä»€ä¹ˆæœ€èƒ½ç»™ä½ å¸¦æ¥æˆå°±æ„Ÿï¼Ÿ")
            if st.form_submit_button("æäº¤æˆ‘çš„èƒ½åŠ›ç”»åƒ", use_container_width=True):
                if all([edu, skills, exp, char, motiv]):
                    profile_text = f"å­¦å†èƒŒæ™¯: {edu}\næ ¸å¿ƒæŠ€èƒ½: {skills}\nç›¸å…³ç»éªŒ: {exp}\nå“è¡Œç‰¹è´¨: {char}\nå†…åœ¨åŠ¨æœº: {motiv}"
                    st.session_state.user_profile = profile_text;
                    history.add_user_message(f"è¿™æ˜¯æˆ‘çš„èƒ½åŠ›ç”»åƒï¼š\n{profile_text}")
                    st.session_state.panoramic_stage = 2;
                    st.rerun()
                else:
                    st.warning("è¯·å¡«å†™æ‰€æœ‰äº”ä¸ªç»´åº¦çš„ä¿¡æ¯ã€‚")
    elif stage in [2, 3]:
        if len(history.messages) % 2 != 0:
            with st.chat_message("ai", avatar="ğŸ¤–"):
                with st.spinner("AI æ­£åœ¨ä¸ºæ‚¨åˆ†æ..."):
                    response_stream = chain.stream(
                        {"current_stage": stage, "user_profile": st.session_state.user_profile,
                         "chosen_professions": st.session_state.get('chosen_professions', 'N/A'),
                         "chosen_region": st.session_state.get('chosen_region', 'N/A')})
                    response_content = st.write_stream(response_stream);
                    history.add_ai_message(response_content)
            st.rerun()
        st.info("ğŸ‘‡ è¯·åœ¨ä¸‹æ–¹çš„è¾“å…¥æ¡†ä¸­è¾“å…¥æ‚¨çš„é€‰æ‹©æˆ–æƒ³æ³•...", icon="ğŸ’¡")
        if user_input := st.chat_input("è¯·åœ¨æ­¤è¾“å…¥æ‚¨çš„é€‰æ‹©æˆ–æƒ³æ³•..."):
            history.add_user_message(user_input)
            if stage == 2:
                st.session_state.chosen_professions = user_input
            elif stage == 3:
                st.session_state.chosen_region = user_input
            st.session_state.panoramic_stage += 1;
            st.rerun()
    elif stage == 4:
        if len(history.messages) % 2 != 0:
            with st.chat_message("ai", avatar="ğŸ¤–"):
                st.markdown("å¥½çš„ï¼Œå·²æ”¶åˆ°æ‚¨çš„æ‰€æœ‰ä¿¡æ¯ã€‚ç°åœ¨ï¼Œæˆ‘å°†ä¸ºæ‚¨ç”Ÿæˆä¸€ä»½å®Œæ•´çš„ç»¼åˆåˆ†ææŠ¥å‘Š...")
                with st.spinner("AI æ­£åœ¨ä¸ºæ‚¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š..."):
                    response_stream = chain.stream({"current_stage": 4, "user_profile": st.session_state.user_profile,
                                                    "chosen_professions": st.session_state.get('chosen_professions',
                                                                                               'N/A'),
                                                    "chosen_region": st.session_state.get('chosen_region', 'N/A')})
                    response_content = st.write_stream(response_stream);
                    history.add_ai_message(response_content)
            st.session_state.panoramic_stage += 1;
            st.rerun()
    elif stage == 5:
        st.success("æ­å–œï¼æ‚¨å·²å®Œæˆæœ¬æ¬¡èŒä¸šè·¯å¾„å…¨æ™¯è§„åˆ’ã€‚")
        st.info("æ‚¨å¯ä»¥å‘ä¸Šæ»šåŠ¨æŸ¥çœ‹ä¸ºæ‚¨ç”Ÿæˆçš„å®Œæ•´æŠ¥å‘Šã€‚")
        if len(history.messages) > 0 and history.messages[-1].type == 'ai':
            report_content = history.messages[-1].content
            text_only_report = re.sub("```mermaid\n(.*?)\n```", "\n[æ­¤å¤„åŸä¸ºå¯è§†åŒ–å›¾è¡¨]\n", report_content,
                                      flags=re.DOTALL)
            st.download_button(label="ğŸ“¥ ä¸‹è½½å®Œæ•´æŠ¥å‘Š (.md)", data=text_only_report.encode('utf-8'),
                               file_name="æˆ‘çš„èŒä¸šè·¯å¾„è§„åˆ’æŠ¥å‘Š.md", mime="text/markdown")


def main():
    llm = get_llm_instance()
    if not llm: st.error("æ— æ³•åˆå§‹åŒ–è¯­è¨€æ¨¡å‹ï¼Œåº”ç”¨ç¨‹åºæ— æ³•å¯åŠ¨ã€‚è¯·æ£€æŸ¥æ‚¨çš„ API Key è®¾ç½®ã€‚"); st.stop()
    with st.sidebar:
        if st.session_state.current_mode != "menu":
            if st.button("â†©ï¸ è¿”å›ä¸»èœå•"):
                st.session_state.clear();
                st.session_state.current_mode = "menu";
                st.rerun()
        st.markdown("---");
        st.caption("Â© 2025 æ™ºæ…§èŒä¸šè¾…å¯¼ V10.1")
    modes = {"menu": render_menu, "exploration": render_exploration_mode, "decision": render_decision_mode,
             "communication": render_communication_mode, "company_info": render_company_info_mode,
             "panoramic": render_panoramic_mode}
    mode_func = modes.get(st.session_state.current_mode, render_menu)
    if st.session_state.current_mode == 'menu':
        mode_func()
    else:
        mode_func(llm)


if __name__ == "__main__":
    main()