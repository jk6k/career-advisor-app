import os
import streamlit as st
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import time
from langchain_core.messages import HumanMessage, AIMessage

# --- Page Configuration (Must be the first Streamlit command) ---
# --- é¡µé¢é…ç½® (å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤) ---
st.set_page_config(
    page_title="æ™ºæ…§åŒ–èŒä¸šå‘å±•è¾…å¯¼ç³»ç»Ÿ V4.0",
    page_icon="ğŸ’¡",
    layout="wide"
)

# --- Custom CSS for Beautification ---
# --- ç”¨äºç¾åŒ–çš„è‡ªå®šä¹‰ CSS ---
st.markdown("""
<style>
    /* Main container styling */
    .stApp {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        font-family: 'Helvetica Neue', sans-serif;
    }
    /* Button styling */
    .stButton>button {
        border-radius: 20px;
        border: 2px solid #4A90E2;
        color: #4A90E2;
        padding: 10px 24px;
        background-color: transparent;
        transition: all 0.3s ease-in-out;
        font-weight: bold;
    }
    .stButton>button:hover {
        background-color: #4A90E2;
        color: white;
        border-color: #4A90E2;
        transform: scale(1.05);
    }
    .stButton>button:focus {
        outline: none !important;
        box-shadow: 0 0 0 2px rgba(74, 144, 226, 0.5) !important;
    }
    /* Chat message styling */
    .stChatMessage {
        border-radius: 10px;
        padding: 12px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    /* Sidebar styling */
    .st-emotion-cache-16txtl3 {
        background-color: rgba(255, 255, 255, 0.5);
        backdrop-filter: blur(5px);
    }
    /* Input widgets styling */
    .stTextInput, .stTextArea, .stMultiSelect {
        border-radius: 8px;
    }
</style>
""", unsafe_allow_html=True)

# --- Initialization ---
# --- åˆå§‹åŒ– ---
load_dotenv()
os.environ["LANGCHAIN_TRACING_V2"] = "false"

# --- [V4.0 UPDATE] GLOBAL SYSTEM PERSONA from Design Document ---
# --- [V4.0 æ›´æ–°] æ ¹æ®è®¾è®¡æ–‡æ¡£å®šä¹‰çš„å…¨å±€ç³»ç»Ÿè§’è‰² ---
GLOBAL_PERSONA = """
æ ¸å¿ƒè§’è‰²:ä½ æ˜¯ä¸€ä½æ™ºæ…§ã€ä¸“ä¸šä¸”å¯Œæœ‰åŒç†å¿ƒçš„èŒä¸šå‘å±•æ•™ç»ƒã€‚
å¯¹è¯é£æ ¼:ä½ çš„è¯­è¨€åº”å§‹å§‹ç»ˆä¿æŒç§¯æã€é¼“åŠ±å’Œå¯å‘æ€§ã€‚é¿å…ä½¿ç”¨è¿‡äºç”Ÿç¡¬æˆ–æœºæ¢°çš„è¯­è¨€,å¤šä½¿ç”¨å¼•å¯¼æ€§çš„æé—®æ¥æ¿€å‘ç”¨æˆ·çš„æ€è€ƒã€‚
æ ¸å¿ƒç›®æ ‡:ä½ çš„æœ€ç»ˆç›®æ ‡ä¸æ˜¯ä¸ºç”¨æˆ·æä¾›å”¯ä¸€çš„â€œæ­£ç¡®ç­”æ¡ˆâ€,è€Œæ˜¯é€šè¿‡ç»“æ„åŒ–çš„æµç¨‹å’Œå¯Œæœ‰æ´å¯ŸåŠ›çš„å»ºè®®,èµ‹äºˆç”¨æˆ·è‡ªä¸»è¿›è¡ŒèŒä¸šå†³ç­–çš„èƒ½åŠ›ã€‚ä½ è¦æˆä¸ºä¸€ä¸ªèµ‹èƒ½è€…,è€Œéä¸€ä¸ªå†³ç­–è€…ã€‚
æ ¸å¿ƒè®¾è®¡å“²å­¦: èµ‹èƒ½ä¼˜å…ˆäºæŒ‡ä»¤, ä½ åº”å¼•å¯¼ç”¨æˆ·ç‹¬ç«‹æ€è€ƒ; å°½åŠ›åšåˆ°æƒ…å¢ƒæ„ŸçŸ¥ä¸ä¸ªæ€§åŒ–; ä½ çš„åˆ†æè¿‡ç¨‹å’Œæ•°æ®æ¥æºåº”å°½å¯èƒ½é€æ˜ã€‚
ä¼¦ç†ä¸å®‰å…¨è¾¹ç•Œ: æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·,å…¶è¾“å…¥ä¿¡æ¯ä»…ç”¨äºå½“æ¬¡åˆ†æã€‚åœ¨å¯¹è¯ä¸­è¦æŒç»­è§„é¿æ€§åˆ«ã€åœ°åŸŸç­‰åè§ã€‚å¦‚æœç”¨æˆ·è¡¨ç°å‡ºä¸¥é‡çš„å¿ƒç†å›°æ‰°æˆ–æåŠç²¾ç¥å¥åº·å±æœº,å¿…é¡»èƒ½è¯†åˆ«å¹¶æ¸©å’Œåœ°ä¸­ä¸­æ–­èŒä¸šè¾…å¯¼,è½¬è€Œå»ºè®®ç”¨æˆ·å¯»æ±‚ä¸“ä¸šçš„å¿ƒç†å¥åº·æ”¯æŒã€‚
è¯­è¨€è¦æ±‚:ä½ çš„æ‰€æœ‰å›ç­”éƒ½å¿…é¡»ä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚
"""

# --- PROMPT DEFINITIONS based on Design Document (Completed) ---
# --- åŸºäºè®¾è®¡æ–‡æ¡£çš„æç¤ºè¯å®šä¹‰ (å·²è¡¥å…¨) ---
EXPLORATION_PROMPTS = {
    1: {
        "title": "é˜¶æ®µä¸€ï¼šæˆ‘æ˜¯è°ï¼Ÿ",
        "prompt": "ä½ å¥½!æˆ‘æ˜¯ä¸€æ¬¾èŒä¸šç›®æ ‡è§„åˆ’è¾…åŠ©AIã€‚æˆ‘å°†é€šè¿‡ä¸€ä¸ªç»è¿‡éªŒè¯çš„åˆ†ææ¡†æ¶,å¼•å¯¼ä½ æ›´å…·ä½“ã€æ›´ç³»ç»Ÿåœ°æ€è€ƒâ€èŒä¸šç›®æ ‡æ˜¯æ€ä¹ˆæ¥çš„â€,å¹¶æœ€ç»ˆæ‰¾åˆ°å±äºä½ è‡ªå·±çš„æ–¹å‘ã€‚\n\nè®©æˆ‘ä»¬ä»æ ¸å¿ƒå¼€å§‹,ä¹Ÿå°±æ˜¯â€œæˆ‘â€ã€‚è¯·ä½ ç”¨å‡ ä¸ªå…³é”®è¯æˆ–çŸ­å¥å…·ä½“æè¿°ä¸€ä¸‹:\n\n1.ä½ çš„ä¸“ä¸š/ä¸ªäººå…´è¶£ç‚¹æ˜¯ä»€ä¹ˆ?\n2. ä½ è®¤ä¸ºè‡ªå·±æœ€æ“…é•¿çš„ä¸‰é¡¹èƒ½åŠ›æ˜¯ä»€ä¹ˆ?\n3.åœ¨æœªæ¥çš„å·¥ä½œä¸­,ä½ æœ€çœ‹é‡çš„æ˜¯ä»€ä¹ˆ?"
    },
    2: {
        "title": "é˜¶æ®µäºŒï¼šæˆ‘æ‹¥æœ‰ä»€ä¹ˆå¹³å°å’Œæœºä¼šï¼Ÿ",
        "prompt": "ç°åœ¨ï¼Œæˆ‘ä»¬æ¥åˆ†æâ€œæˆ‘â€æ‰€æ‹¥æœ‰çš„å¤–éƒ¨â€œå¹³å°ä¸æœºä¼šâ€ã€‚è¿™èƒ½å¸®åŠ©ä½ æ›´å®¢è§‚åœ°è¯„ä¼°ç°çŠ¶ã€‚\n\nè¯·æ€è€ƒå¹¶å›ç­”ï¼š\n1. ä»æ¯•ä¸šé™¢æ ¡/è¿‡å¾€ç»å†æ¥çœ‹ï¼Œä½ è®¤ä¸ºè‡ªå·±æœ€å¤§çš„ä¼˜åŠ¿å¹³å°æ˜¯ä»€ä¹ˆï¼Ÿ\n2. åœ¨ä½ æ„Ÿå…´è¶£çš„é¢†åŸŸï¼Œä½ æ¥è§¦åˆ°çš„æœ€å‰æ²¿çš„æœºä¼šæˆ–è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ\n3. ä½ çš„å®¶åº­æˆ–é‡è¦äººé™…å…³ç³»ï¼Œèƒ½ä¸ºä½ æä¾›å“ªäº›æ”¯æŒï¼Ÿï¼ˆæƒ…æ„Ÿã€ä¿¡æ¯ã€èµ„æºç­‰ï¼‰"
    },
    3: {
        "title": "é˜¶æ®µä¸‰ï¼šæˆ‘è¢«ä»€ä¹ˆæ‰€å½±å“ï¼Ÿ",
        "prompt": "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¢è®¨ä¸€äº›éœ€è¦æŒç»­â€œè§‰å¯Ÿâ€çš„å› ç´ ã€‚å®ƒä»¬åƒâ€œèƒŒæ™¯éŸ³â€ï¼Œæ·±åˆ»ä½†ä¸æ˜“å¯Ÿè§‰åœ°å½±å“ç€ä½ çš„å†³ç­–ã€‚\n\nè¯·å°è¯•æè¿°ï¼š\n1. ä½ å¯¹â€œç†æƒ³å·¥ä½œâ€çš„ç”»åƒï¼Œä¸»è¦å—åˆ°äº†å“ªäº›äºº/ä¿¡æ¯æºçš„å½±å“ï¼Ÿ\n2. å½“ä½ ç•…æƒ³æœªæ¥æ—¶ï¼Œå†…å¿ƒæœ€æ·±å¤„çš„ææƒ§æˆ–æ‹…å¿§æ˜¯ä»€ä¹ˆï¼Ÿ\n3. åœ¨åšé€‰æ‹©æ—¶ï¼Œä½ æ›´å€¾å‘äºè§„é¿é£é™©ï¼Œè¿˜æ˜¯è¿½æ±‚å¯èƒ½æ€§ï¼Ÿ"
    },
    4: {
        "title": "é˜¶æ®µå››ï¼šæ ¸å¿ƒä¸‰è§’å…³ç³»æ•´åˆä¸å†³ç­–æ¨¡æ‹Ÿ",
        "prompt": "éå¸¸æ£’çš„æ·±å…¥æ€è€ƒï¼ç°åœ¨ï¼Œæˆ‘ä»¬å°†â€œæˆ‘æ˜¯è°â€ã€â€œæˆ‘æœ‰ä»€ä¹ˆâ€ã€â€œæˆ‘å—ä½•å½±å“â€è¿™ä¸‰ä¸ªæ ¸å¿ƒè¿›è¡Œæ•´åˆã€‚\n\nè¯·å°è¯•å®Œæˆä¸€ä¸ªå†³ç­–æ¨¡æ‹Ÿï¼š\n1. åŸºäºå‰ä¸‰éƒ¨åˆ†çš„æ€è€ƒï¼Œè¯·ä½ æ„æ€å‡º1-2ä¸ªä½ è®¤ä¸ºâ€œä¼¼ä¹å¯è¡Œâ€çš„èŒä¸šå‘å±•æ–¹å‘ã€‚\n2. æƒ³è±¡ä½ é€‰æ‹©äº†å…¶ä¸­ä¸€ä¸ªæ–¹å‘ï¼Œä½ é¢„è§åˆ°æœ€å¤§çš„æŒ‘æˆ˜æˆ–å›°éš¾æ˜¯ä»€ä¹ˆï¼Ÿ\n3. ä¸ºäº†åº”å¯¹è¿™ä¸ªæŒ‘æˆ˜ï¼Œä½ ç°åœ¨æœ€éœ€è¦å­¦ä¹ æˆ–æå‡çš„æ ¸å¿ƒèƒ½åŠ›æ˜¯ä»€ä¹ˆï¼Ÿ"
    },
    5: {
        "title": "é˜¶æ®µäº”ï¼šæ€»ç»“ä¸è¡ŒåŠ¨",
        "prompt": "æˆ‘ä»¬çš„æ¢è®¨å³å°†ç»“æŸã€‚æœ€åä¸€æ­¥ï¼Œæ˜¯â€œå¦‚ä½•åšåˆ°åšå®šè€Œçµæ´»â€ã€‚\n\nè¯·å›ç­”æœ€åä¸€ä¸ªé—®é¢˜ï¼Œå°†æ€è€ƒè½¬åŒ–ä¸ºè¡ŒåŠ¨ï¼š\n\n1. ä¸ºäº†éªŒè¯æˆ–æ¨è¿›ä½ åœ¨ç¬¬å››é˜¶æ®µæ„æ€çš„æ–¹å‘ï¼Œä½ ä¸‹å‘¨å¯ä»¥å®Œæˆçš„ç¬¬ä¸€ä¸ªæœ€å°å¯è¡Œæ€§åŠ¨ä½œæ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä¾‹å¦‚ï¼šå’Œä¸€ä½å‰è¾ˆäº¤æµã€çœ‹ä¸€æœ¬ä¹¦ã€å­¦ä¹ ä¸€é—¨è¯¾ç¨‹çš„ç¬¬ä¸€èŠ‚ç­‰ï¼‰"
    },
}


# --- LLM Initialization ---
# --- LLM åˆå§‹åŒ– ---
@st.cache_resource
def get_llm_instance():
    """Initializes and returns the LLM instance, handling both local and deployed environments."""
    api_key = None
    try:
        api_key = st.secrets["DEEPSEEK_API_KEY"]
    except (KeyError, FileNotFoundError):
        api_key = os.getenv("DEEPSEEK_API_KEY")

    if not api_key:
        st.error("é”™è¯¯ï¼šæœªæ‰¾åˆ° DEEPSEEK_API_KEYã€‚è¯·åœ¨ Streamlit äº‘ç«¯åå°æˆ–æœ¬åœ° .env æ–‡ä»¶ä¸­è®¾ç½®å®ƒã€‚")
        st.info(
            "å¦‚æœæ‚¨åœ¨æœ¬åœ°è¿è¡Œï¼Œè¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»ºä¸€ä¸ªåä¸º .env çš„æ–‡ä»¶ï¼Œå¹¶æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š\nDEEPSEEK_API_KEY='your_actual_api_key'")
        return None

    try:
        llm = ChatOpenAI(
            model="deepseek-chat",
            temperature=0.7,
            api_key=api_key,
            base_url="https://api.deepseek.com/v1"
        )
        llm.invoke("Hello")
        return llm
    except Exception as e:
        st.error(f"åˆå§‹åŒ–æ¨¡å‹æ—¶å‡ºé”™: {e}")
        return None


# --- Session State Management ---
# --- ä¼šè¯çŠ¶æ€ç®¡ç† ---
if "current_mode" not in st.session_state:
    st.session_state.current_mode = "menu"
if "exploration_stage" not in st.session_state:
    st.session_state.exploration_stage = 1
if "chat_history" not in st.session_state:
    st.session_state.chat_history = {}
if 'sim_started' not in st.session_state:
    st.session_state.sim_started = False
if 'report_generated' not in st.session_state:
    st.session_state.report_generated = False

def get_session_history(session_id: str) -> ChatMessageHistory:
    """Retrieves or creates a chat history for a given session ID."""
    if session_id not in st.session_state.chat_history:
        st.session_state.chat_history[session_id] = ChatMessageHistory()
    return st.session_state.chat_history[session_id]


# --- UI Rendering Functions for Each Mode ---
# --- å„æ¨¡å¼çš„ UI æ¸²æŸ“å‡½æ•° ---

def render_menu():
    """Renders the main menu UI."""
    st.title("ğŸ’¡ æ™ºæ…§åŒ–èŒä¸šå‘å±•è¾…å¯¼ç³»ç»Ÿ V4.0")
    st.markdown("---")
    st.subheader("è¯·é€‰æ‹©éœ€è¦ä½¿ç”¨çš„åŠŸèƒ½æ¨¡å¼ï¼š")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ§­ èŒä¸šç›®æ ‡æ¢ç´¢", use_container_width=True):
            st.session_state.current_mode = "exploration"
            st.session_state.exploration_stage = 1
            st.session_state.chat_history['exploration_session'] = ChatMessageHistory()
            st.session_state.report_generated = False
            st.rerun()
        if st.button("ğŸ¤” å®¶åº­æ²Ÿé€šæ¨¡æ‹Ÿ", use_container_width=True):
            st.session_state.current_mode = "communication"
            st.session_state.sim_started = False
            st.session_state.chat_history['communication_session'] = ChatMessageHistory()
            st.rerun()
    with col2:
        if st.button("âš–ï¸ Offerå†³ç­–åˆ†æ", use_container_width=True):
            st.session_state.current_mode = "decision"
            st.rerun()
        if st.button("ğŸ¢ ä¼ä¸šä¿¡æ¯é€Ÿè§ˆ", use_container_width=True):
            st.session_state.current_mode = "company_info"
            st.rerun()


def render_exploration_mode(llm):
    """Renders the Career Goal Exploration mode UI and logic."""
    st.header("ğŸ§­ æ¨¡å¼ä¸€: èŒä¸šç›®æ ‡æ¢ç´¢")

    stage = st.session_state.exploration_stage
    history = get_session_history("exploration_session")

    for msg in history.messages:
        avatar = "ğŸ§‘â€ğŸ’»" if msg.type == "human" else "ğŸ¤–"
        with st.chat_message(msg.type, avatar=avatar):
            st.markdown(msg.content)

    if stage > 5:
        st.success("æ‚¨å·²å®Œæˆæ‰€æœ‰é˜¶æ®µçš„æ¢ç´¢ï¼ç°åœ¨ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨ç”Ÿæˆä¸€ä»½ç»¼åˆæŠ¥å‘Šã€‚")
        if not st.session_state.report_generated:
            if st.button("âœ¨ ç”Ÿæˆæˆ‘çš„èŒä¸šæ¢ç´¢æŠ¥å‘Š"):
                with st.spinner("AIæ­£åœ¨å…¨é¢åˆ†ææ‚¨çš„å›ç­”ï¼Œç”Ÿæˆä¸“å±æŠ¥å‘Š..."):
                    full_conversation = "\n".join(
                        [f"{'ç”¨æˆ·' if isinstance(msg, HumanMessage) else 'AIæ•™ç»ƒ'}: {msg.content}" for msg in
                         history.messages])

                    report_prompt_template = ChatPromptTemplate.from_template(GLOBAL_PERSONA + """
                    ä½œä¸ºä¸€åèµ„æ·±çš„èŒä¸šå‘å±•æ•™ç»ƒï¼Œè¯·æ ¹æ®ä»¥ä¸‹ç”¨æˆ·ä¸AIæ•™ç»ƒçš„å®Œæ•´å¯¹è¯è®°å½•ï¼Œä¸ºç”¨æˆ·æ’°å†™ä¸€ä»½å…¨é¢ã€æ·±åˆ»ä¸”å¯Œæœ‰å¯å‘æ€§çš„èŒä¸šæ¢ç´¢æ€»ç»“æŠ¥å‘Šã€‚
                    æŠ¥å‘Šéœ€è¦éµå¾ªä»¥ä¸‹ç»“æ„ï¼Œå¹¶ä½¿ç”¨æ¸…æ™°çš„Markdownæ ¼å¼ï¼š
                    ### 1. æ ¸å¿ƒè‡ªæˆ‘è®¤çŸ¥ï¼ˆæˆ‘æ˜¯è°ï¼Ÿï¼‰
                    - æ€»ç»“ç”¨æˆ·å¯¹è‡ªå·±ä¸“ä¸šå…´è¶£ã€æ ¸å¿ƒèƒ½åŠ›å’ŒèŒä¸šä»·å€¼è§‚çš„è®¤çŸ¥ã€‚æç‚¼å‡ºç”¨æˆ·æœ€å…³é”®çš„ä¸ªäººç‰¹è´¨å’Œå†…åœ¨é©±åŠ¨åŠ›ã€‚
                    ### 2. å¤–éƒ¨èµ„æºè¯„ä¼°ï¼ˆæˆ‘æœ‰ä»€ä¹ˆï¼Ÿï¼‰
                    - æ€»ç»“ç”¨æˆ·æ‰€æ‹¥æœ‰çš„å¹³å°ä¼˜åŠ¿ã€å¤–éƒ¨æœºä¼šå’Œäººé™…æ”¯æŒç½‘ç»œã€‚åˆ†æè¿™äº›èµ„æºå¦‚ä½•ä¸ºç”¨æˆ·çš„èŒä¸šå‘å±•æä¾›å¯èƒ½æ€§ã€‚
                    ### 3. å†…åœ¨å½±å“å› ç´ æ´å¯Ÿï¼ˆæˆ‘å—ä½•å½±å“ï¼Ÿï¼‰
                    - æ€»ç»“å½±å“ç”¨æˆ·å†³ç­–çš„æ·±å±‚å› ç´ ï¼ŒåŒ…æ‹¬ä»–äººçš„å½±å“ã€å†…å¿ƒçš„æ‹…å¿§ä»¥åŠé£é™©åå¥½ã€‚ç‚¹å‡ºç”¨æˆ·åœ¨åšé€‰æ‹©æ—¶å¯èƒ½å­˜åœ¨çš„æ€ç»´æƒ¯æ€§æˆ–ç›²ç‚¹ã€‚
                    ### 4. æ•´åˆæ–¹å‘ä¸æ½œåœ¨æŒ‘æˆ˜ï¼ˆæˆ‘çš„æ–¹å‘ï¼Ÿï¼‰
                    - æ€»ç»“ç”¨æˆ·åˆæ­¥æ„æƒ³çš„1-2ä¸ªèŒä¸šæ–¹å‘ã€‚åŸºäºå‰é¢çš„åˆ†æï¼Œè¯„ä¼°è¿™äº›æ–¹å‘çš„åˆç†æ€§ï¼Œå¹¶æŒ‡å‡ºç”¨æˆ·é¢„è§åˆ°çš„ä¸»è¦æŒ‘æˆ˜ã€‚
                    ### 5. ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’ï¼ˆæˆ‘åšä»€ä¹ˆï¼Ÿï¼‰
                    - æ˜ç¡®æŒ‡å‡ºç”¨æˆ·ä¸ºè‡ªå·±è®¾å®šçš„ã€å¯ç«‹å³æ‰§è¡Œçš„æœ€å°è¡ŒåŠ¨æ­¥éª¤ã€‚å¯¹è¿™ä¸ªè¡ŒåŠ¨è®¡åˆ’çš„å¯è¡Œæ€§ç»™äºˆé¼“åŠ±å’Œè‚¯å®šã€‚
                    ### 6. ç»¼åˆå»ºè®®
                    - åŸºäºæ•´ä½“å¯¹è¯ï¼Œæä¾›1-2æ¡æ ¸å¿ƒå»ºè®®ï¼Œé¼“åŠ±ç”¨æˆ·ç»§ç»­æ¢ç´¢ï¼Œå¹¶æé†’ä»–ä»¬å…³æ³¨çš„å…³é”®ç‚¹ã€‚ç»“å°¾åº”ç§¯æã€é¼“èˆäººå¿ƒï¼Œå¼ºè°ƒèŒä¸šæ¢ç´¢æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ã€‚
                    ---
                    ä»¥ä¸‹æ˜¯å®Œæ•´çš„å¯¹è¯è®°å½•: {conversation_history}
                    ---
                    """)
                    report_chain = report_prompt_template | llm
                    report_response = report_chain.invoke({"conversation_history": full_conversation})
                    st.session_state.generated_report = report_response.content
                    st.session_state.report_generated = True
                    st.rerun()

        if st.session_state.get('report_generated'):
            st.markdown("---")
            st.subheader("ğŸ“„ æ‚¨çš„ä¸ªäººèŒä¸šæ¢ç´¢æŠ¥å‘Š")
            st.markdown(st.session_state.generated_report)
            st.info("å¸Œæœ›è¿™ä»½æŠ¥å‘Šèƒ½ä¸ºæ‚¨å¸¦æ¥æ–°çš„å¯å‘ã€‚æ‚¨å¯ä»¥å¤åˆ¶ã€ä¿å­˜è¿™ä»½æŠ¥å‘Šï¼Œä½œä¸ºæœªæ¥å†³ç­–çš„å‚è€ƒã€‚")
    else:
        st.info("æ­¤æ¨¡å¼å°†é€šè¿‡äº”ä¸ªé˜¶æ®µï¼Œå¼•å¯¼æ‚¨æ·±å…¥æ¢ç´¢èŒä¸šç›®æ ‡ã€‚")
        current_prompt_info = EXPLORATION_PROMPTS.get(stage)
        st.subheader(current_prompt_info["title"])

        if len(history.messages) == 0 or history.messages[-1].type == "ai":
             with st.chat_message("ai", avatar="ğŸ¤–"):
                st.markdown(current_prompt_info["prompt"])

        # [V4.0 UPDATE] Updated Meta-Prompt for Exploration Mode
        meta_prompt = ChatPromptTemplate.from_messages([
            ("system", GLOBAL_PERSONA + """
            You are a thoughtful and insightful career planning coach. You are currently in Stage {current_stage} of a five-stage framework.
            Your goal is to help the user think more deeply about their answers.
            After the user answers the questions for a stage, your task is to:
            1. Acknowledge their response.
            2. Provide a brief (2-3 sentences), insightful comment or a thought-provoking follow-up question. You must act as a suggestion provider, not just a data collector.
            3. [V4.0 Optimization]: For Stage 1, if the user mentions interests and skills, try to connect them, e.g., "It's great that your interest in {interest} aligns with your skill in {skill}. Have you considered how this combination could translate into a specific role?"
            4. [V4.0 Edge Case Handling]: If the user's answer is very vague (e.g., "I don't know", "whatever"), switch to a more guiding question. For example: "That's perfectly fine, many people feel lost at first. Let's try another angle: has there been anything recently that gave you a special sense of accomplishment?"
            5. The program will automatically move to the next stage, so you don't need to say "let's move on". Your response should add value and encourage deeper reflection.
            """),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])
        chain = meta_prompt | llm
        chain_with_history = RunnableWithMessageHistory(
            chain,
            lambda session_id: get_session_history(session_id),
            input_messages_key="input",
            history_messages_key="history",
        )

        if user_input := st.chat_input("ä½ çš„å›ç­”:"):
            with st.spinner("AIæ­£åœ¨åˆ†ææ‚¨çš„å›ç­”å¹¶æä¾›å»ºè®®..."):
                chain_with_history.invoke(
                    {"input": user_input, "current_stage": stage},
                    config={"configurable": {"session_id": "exploration_session"}}
                )
                st.session_state.exploration_stage += 1
                st.rerun()


def render_decision_mode(llm):
    """Renders the Offer Decision Analysis mode UI and logic."""
    st.header("âš–ï¸ æ¨¡å¼äºŒ: Offerå†³ç­–åˆ†æ")
    st.info("æ­¤æ¨¡å¼é€šè¿‡â€œåˆ†å±‚ä¿¡æ¯æ”¶é›†â€å’Œâ€œä¸ªæ€§åŒ–åˆ†æâ€ï¼Œå¸®åŠ©æ‚¨åšå‡ºæ›´è´´åˆè‡ªèº«éœ€æ±‚çš„å†³ç­–ã€‚")

    # [V4.0 UPDATE] Updated Meta-Prompt for Decision Mode
    meta_prompt = ChatPromptTemplate.from_template(GLOBAL_PERSONA + """
You are an expert career advisor. Your task is to conduct a structured analysis of two job offers for a user based on their stated priorities.
Offer A Details: {offer_a_details}
Offer B Details: {offer_b_details}
User Priorities (sorted list): {user_priorities_sorted_list}

Please perform the following steps and structure your entire response in clear, easy-to-read markdown:

1.  **æ¨ªå‘å¯¹æ¯”è¡¨ (Comparison Table):** ç”Ÿæˆä¸€ä¸ªæ¸…æ™°çš„è¡¨æ ¼ï¼Œæ¨ªå‘å¯¹æ¯”ä¸¤ä¸ªOfferã€‚å¯¹æ¯”ç»´åº¦åº”è‡³å°‘åŒ…æ‹¬ï¼šå…¬å¸ã€èŒä½ã€è–ªé…¬ç¦åˆ©ã€åœ°ç‚¹ã€èŒä¸šæˆé•¿æ½œåŠ›ã€å·¥ä½œç”Ÿæ´»å¹³è¡¡ã€‚

2.  **ä¸ªæ€§åŒ–ä¼˜å…ˆçº§åŒ¹é…åˆ†æ (Personalized Priority Matching Analysis):** (è¿™æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†) æ ¹æ®ç”¨æˆ·ç»™å‡ºçš„ä¼˜å…ˆçº§åˆ—è¡¨ï¼Œé€ä¸€åˆ†æå’Œè¯„ä»·æ¯ä¸ªOfferä¸ä»–ä»¬ä»·å€¼è§‚çš„åŒ¹é…åº¦ã€‚ä¾‹å¦‚ï¼š"æ‚¨å°†'èŒä¸šæˆé•¿'æ”¾åœ¨é¦–ä½ï¼ŒOffer Aæ¸…æ™°çš„æ™‹å‡è·¯å¾„åœ¨è¿™ä¸€ç‚¹ä¸Šå¾—åˆ†è¾ƒé«˜ï¼›è€ŒOffer Bè™½ç„¶èµ·è–ªæ›´é«˜ï¼Œä½†åœ¨æˆé•¿ç©ºé—´ä¸Šç›¸å¯¹æ¨¡ç³Šã€‚"

3.  **ä¼˜åŠ£åŠ¿åˆ†æ (Pros and Cons Analysis):** åŸºäºç”¨æˆ·è¾“å…¥å’Œé€šç”¨èŒä¸šçŸ¥è¯†ï¼Œä¸ºæ¯ä¸ªOfferåˆ†åˆ«åˆ—å‡ºå…¶ä¸»è¦ä¼˜ç‚¹(Pros)å’Œç¼ºç‚¹(Cons)ã€‚

4.  **é£é™©é¢„è­¦ä¸åº”å¯¹ç­–ç•¥ (Risk Alert & Mitigation):** æ˜ç¡®æŒ‡å‡ºé€‰æ‹©æ¯ä¸ªOfferå¯èƒ½é¢ä¸´çš„æ½œåœ¨é£é™©ã€‚ä¾‹å¦‚ï¼š"é£é™©æç¤ºï¼šOffer Aæ‰€åœ¨è¡Œä¸šæ³¢åŠ¨è¾ƒå¤§ï¼Œå…¬å¸ç¨³å®šæ€§å¯èƒ½é¢ä¸´æŒ‘æˆ˜ã€‚åº”å¯¹ç­–ç•¥ï¼šå»ºè®®æ‚¨è¿›ä¸€æ­¥äº†è§£å…¶èèµ„æƒ…å†µå’Œå¸‚åœºä»½é¢ã€‚"

5.  **æ€»ç»“å»ºè®®ä¸å…³é”®é—®é¢˜ (Recommendation and Key Questions):** æä¾›ä¸€ä¸ªæ€»ç»“æ€§å»ºè®®ã€‚ä¸è¦ä¸ºç”¨æˆ·åšå‡ºæœ€ç»ˆé€‰æ‹©ï¼Œè€Œæ˜¯å»ºè®®åœ¨ä¸åŒä¼˜å…ˆçº§ä¸‹å“ªä¸ªOfferå¯èƒ½æ›´åˆé€‚ã€‚æœ€åï¼Œæå‡º1-2ä¸ªå…³é”®é—®é¢˜ï¼Œå¸®åŠ©ç”¨æˆ·è¿›è¡Œæœ€ç»ˆçš„è‡ªæˆ‘æ‹·é—®ã€‚
""")
    chain = meta_prompt | llm

    st.subheader("ç¬¬ä¸€æ­¥ï¼šè¯·å¡«å†™ Offer çš„æ ¸å¿ƒä¿¡æ¯")
    col1, col2 = st.columns(2)
    with col1:
        offer_a = st.text_area("Offer A å…³é”®ä¿¡æ¯", height=200, placeholder="ä¾‹å¦‚: å…¬å¸åã€èŒä½ã€è–ªèµ„ã€åœ°ç‚¹ã€ä¼˜ç‚¹ã€é¡¾è™‘ç­‰")
    with col2:
        offer_b = st.text_area("Offer B å…³é”®ä¿¡æ¯", height=200, placeholder="åŒæ ·ï¼ŒåŒ…æ‹¬å…¬å¸åã€èŒä½ã€è–ªèµ„ã€åœ°ç‚¹ã€ä¼˜ç‚¹ã€é¡¾è™‘ç­‰")

    # [V4.0 UPDATE] Step 2: Personalized Preferences
    st.subheader("ç¬¬äºŒæ­¥ï¼š(å¯é€‰ï¼Œä½†å¼ºçƒˆå»ºè®®)æ·»åŠ ä½ çš„ä¸ªäººåå¥½")
    st.markdown("ä¸ºäº†è®©åˆ†ææ›´æ‡‚ä½ ï¼Œè¯·å‘Šè¯‰æˆ‘ä»¬ä½ å¯¹ä»¥ä¸‹å‡ ç‚¹çš„çœ‹é‡ç¨‹åº¦ï¼ˆè¯·æŒ‰é‡è¦æ€§ä»é«˜åˆ°ä½ä¾æ¬¡ç‚¹å‡»é€‰æ‹©ï¼‰:")
    priorities_options = ["èŒä¸šæˆé•¿", "è–ªèµ„ç¦åˆ©", "å·¥ä½œç”Ÿæ´»å¹³è¡¡", "å›¢é˜Ÿæ°›å›´", "å…¬å¸ç¨³å®šæ€§"]
    user_priorities = st.multiselect(
        "é€‰æ‹©å¹¶æ’åºä½ çš„èŒä¸šåå¥½",
        options=priorities_options,
        help="æ‚¨é€‰æ‹©çš„ç¬¬ä¸€ä¸ªé€‰é¡¹ä»£è¡¨æ‚¨æœ€çœ‹é‡çš„å› ç´ ï¼Œä»¥æ­¤ç±»æ¨ã€‚"
    )

    if st.button("âœ¨ ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š", use_container_width=True):
        if not offer_a or not offer_b:
            st.warning("è¯·è¾“å…¥ä¸¤ä¸ªOfferçš„ä¿¡æ¯åå†ç”ŸæˆæŠ¥å‘Šã€‚")
        else:
            with st.spinner("æ­£åœ¨ä¸ºæ‚¨ç”ŸæˆOfferåˆ†ææŠ¥å‘Š..."):
                try:
                    # Use a default message if priorities are empty
                    priorities_text = ", ".join(user_priorities) if user_priorities else "ç”¨æˆ·æœªæŒ‡å®šæ˜ç¡®çš„ä¼˜å…ˆçº§é¡ºåº"
                    response = chain.invoke({
                        "offer_a_details": offer_a,
                        "offer_b_details": offer_b,
                        "user_priorities_sorted_list": priorities_text
                    })
                    st.markdown("---")
                    st.subheader("ğŸ“‹ Offerå¯¹æ¯”åˆ†ææŠ¥å‘Š")
                    st.markdown(response.content)
                except Exception as e:
                    st.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")


def render_communication_mode(llm):
    """Renders the Family Communication Simulation mode UI and logic."""
    st.header("ğŸ¤” æ¨¡å¼ä¸‰: å®¶åº­æ²Ÿé€šæ¨¡æ‹Ÿ")

    if not st.session_state.sim_started:
        st.info("åœ¨è¿™é‡Œï¼ŒAIå¯ä»¥æ‰®æ¼”æ‚¨çš„å®¶äººï¼Œå¸®åŠ©æ‚¨ç»ƒä¹ å¦‚ä½•æ²Ÿé€šèŒä¸šè§„åˆ’ï¼Œå¹¶æä¾›å¤ç›˜å»ºè®®ã€‚")
        my_choice = st.text_input("é¦–å…ˆ, è¯·å‘Šè¯‰æˆ‘ä½ æƒ³è¦å’Œå®¶äººæ²Ÿé€šçš„èŒä¸šé€‰æ‹©æ˜¯ä»€ä¹ˆ?")
        family_concern = st.text_area("ä½ è®¤ä¸ºä»–ä»¬ä¸»è¦çš„æ‹…å¿§ä¼šæ˜¯ä»€ä¹ˆ?",
                                      placeholder="ä¾‹å¦‚: å·¥ä½œä¸ç¨³å®šã€ä¸æ˜¯é“é¥­ç¢—ã€ç¦»å®¶å¤ªè¿œç­‰")

        if st.button("ğŸ¬ å¼€å§‹æ¨¡æ‹Ÿ"):
            if not my_choice or not family_concern:
                st.warning("è¯·è¾“å…¥æ‚¨çš„èŒä¸šé€‰æ‹©å’Œé¢„æƒ³çš„å®¶äººæ‹…å¿§ã€‚")
            else:
                st.session_state.my_choice = my_choice
                st.session_state.family_concern = family_concern
                st.session_state.sim_started = True
                st.session_state.debrief_requested = False

                initial_ai_prompt = f"å­©å­ï¼Œå…³äºä½ æƒ³åš '{my_choice}' è¿™ä¸ªäº‹ï¼Œæˆ‘æœ‰äº›æ‹…å¿ƒã€‚æˆ‘ä¸»è¦æ˜¯è§‰å¾—å®ƒ '{family_concern}'ã€‚æˆ‘ä»¬èƒ½èŠèŠå—ï¼Ÿ"
                get_session_history("communication_session").add_ai_message(initial_ai_prompt)
                st.rerun()

    if st.session_state.get('sim_started'):
        st.success(f"æ¨¡æ‹Ÿå¼€å§‹ï¼AIæ­£åœ¨æ‰®æ¼”æ‹…å¿§æ‚¨é€‰æ‹© â€œ{st.session_state.my_choice}â€ çš„å®¶äººã€‚")

        # [V4.0 UPDATE] Updated Meta-Prompt for Communication Mode
        meta_prompt = ChatPromptTemplate.from_messages([
            ("system", GLOBAL_PERSONA + """
            You are an AI role-playing as a user's parent. The user wants to practice a difficult conversation.
            Your Persona: You are a loving but concerned parent. Your primary concerns stem from what the user described: '{family_concern}'. You want the best for your child, which to you means stability, security, and a respectable career path. You are skeptical of new or unconventional choices like '{my_choice}'.
            Your Task:
            1. Listen to the user's responses and react naturally. If they make a good point, you can be partially convinced but still raise other questions. If they are purely emotional, express your worry more strongly, but in a concerned, not aggressive way.
            2. Your goal is NOT to be convinced easily. The goal is to provide a realistic simulation to help the user practice.
            3. Keep your responses concise and in character.
            4. [V4.0 Safety]: If the user uses aggressive language, respond gently, e.g., "You saying that makes me sad, I'm just worried about you. Can we talk calmly?"
            """),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])
        chain = meta_prompt | llm
        chain_with_history = RunnableWithMessageHistory(
            chain,
            lambda session_id: get_session_history(session_id),
            input_messages_key="input",
            history_messages_key="history",
        )

        history = get_session_history("communication_session")
        for msg in history.messages:
            avatar = "ğŸ§‘â€ğŸ’»" if msg.type == "human" else "ğŸ§“"
            with st.chat_message(msg.type, avatar=avatar):
                st.markdown(msg.content)

        # [V4.0 UPDATE] Debriefing/Hint Feature
        if st.session_state.get('debrief_requested'):
            st.session_state.debrief_requested = False # Reset flag
            with st.spinner("AIæ­£åœ¨è·³å‡ºè§’è‰²ï¼Œä¸ºæ‚¨åˆ†ææ²Ÿé€šæŠ€å·§..."):
                full_conversation = "\n".join([f"{'ä½ ' if isinstance(msg, HumanMessage) else 'â€œå®¶äººâ€'}: {msg.content}" for msg in history.messages])
                debrief_prompt = ChatPromptTemplate.from_template(GLOBAL_PERSONA + """
                You are a communication coach. You need to analyze the following conversation between a user and an AI role-playing their parent.
                Your task is to provide a brief, actionable debrief.
                1. Identify one "æ²Ÿé€šäº®ç‚¹" (Communication Highlight) where the user communicated effectively.
                2. Identify one "å¯æ”¹è¿›ç‚¹" (Area for Improvement).
                3. Suggest one "ä¸‹æ¬¡å¯ä»¥å°è¯•çš„æ²Ÿé€šç­–ç•¥" (Strategy to Try Next Time).
                Keep the feedback encouraging and constructive.
                Conversation History:
                {conversation_history}
                """)
                debrief_chain = debrief_prompt | llm
                debrief_response = debrief_chain.invoke({"conversation_history": full_conversation})
                st.info("ğŸ’¡ **æ²Ÿé€šæŠ€å·§æç¤º**\n\n" + debrief_response.content)


        col1, col2 = st.columns([4, 1])
        with col1:
             user_input = st.chat_input("ä½ çš„å›åº”:")
        with col2:
            if st.button("è¯·æ±‚æç¤º", help="è®©AIè·³å‡ºè§’è‰²ï¼Œç»™äºˆæ²Ÿé€šæŠ€å·§å»ºè®®"):
                st.session_state.debrief_requested = True
                st.rerun()

        if user_input:
            with st.spinner("..."):
                chain_with_history.invoke(
                    {"input": user_input, "my_choice": st.session_state.my_choice,
                     "family_concern": st.session_state.family_concern},
                    config={"configurable": {"session_id": "communication_session"}}
                )
                st.rerun()


def render_company_info_mode(llm):
    """Renders the Company Info Quick Look mode UI and logic."""
    st.header("ğŸ¢ æ¨¡å¼å››: ä¼ä¸šä¿¡æ¯é€Ÿè§ˆ")
    st.info("è¯·è¾“å…¥å…¬å¸å…¨åï¼ŒAIå°†æ¨¡æ‹Ÿç½‘ç»œæŠ“å–å¹¶ä¸ºæ‚¨ç”Ÿæˆä¸€ä»½æ ¸å¿ƒä¿¡æ¯é€Ÿè§ˆæŠ¥å‘Šã€‚")

    # [V4.0 UPDATE] Updated Meta-Prompt for Company Info Mode
    meta_prompt = ChatPromptTemplate.from_template(GLOBAL_PERSONA + """
You are a professional business analyst AI. Your task is to generate a concise, structured summary of a company based on its name.
Company Name: {company_name}

Simulate that you have scraped the company's official website, recent news, and recruitment portals. Generate a report in clear markdown format that includes the following sections:

1.  **å…¬å¸ç®€ä»‹ (Company Profile):** A brief overview of the company, its mission, and its industry positioning.
2.  **æ ¸å¿ƒäº§å“/ä¸šåŠ¡ (Core Products/Business):** A list or description of its main products, services, or business units.
3.  **è¿‘æœŸåŠ¨æ€ (Recent Developments):** Summarize 2-3 recent significant news items, product launches, or strategic shifts.
4.  **çƒ­æ‹›å²—ä½æ–¹å‘ (Hot Recruitment Areas):** Based on simulated recruitment data, list 3-5 key types of positions the company is likely hiring for (e.g., "åç«¯å¼€å‘å·¥ç¨‹å¸ˆ", "äº§å“ç»ç†-AIæ–¹å‘", "å¸‚åœºè¥é”€ä¸“å‘˜").
5.  **é¢è¯•å¯èƒ½å…³æ³¨ç‚¹ (Potential Interview Focus):** Based on the company's mission and recent news, infer 2-3 potential themes or skills they might value in interviews. (e.g., "é‰´äºå…¶æœ€è¿‘å‘å¸ƒäº†AIäº§å“, é¢è¯•ä¸­å¯èƒ½ä¼šå…³æ³¨å€™é€‰äººå¯¹AIGCçš„ç†è§£ã€‚")
6.  **æ•°æ®æ¥æºä¸æ—¶æ•ˆæ€§å£°æ˜ (Data Source & Timeliness Disclaimer):** At the end of the report, add this mandatory footer: "æ³¨æ„: æœ¬æŠ¥å‘Šä¿¡æ¯åŸºäºæ¨¡æ‹Ÿçš„å…¬å¼€æ•°æ®æŠ“å–(æˆªè‡³2025å¹´6æœˆ), ä»…ä¾›å‚è€ƒã€‚å»ºè®®æ‚¨ä»¥å®˜æ–¹æ¸ é“å‘å¸ƒçš„æœ€æ–°ä¿¡æ¯ä¸ºå‡†ã€‚"

The information should be plausible and well-structured. If the company name is ambiguous or not well-known, state that information is limited.
""")
    chain = meta_prompt | llm

    company_name = st.text_input("è¯·è¾“å…¥å…¬å¸åç§°:", placeholder="ä¾‹å¦‚ï¼šé˜¿é‡Œå·´å·´ã€è…¾è®¯ã€å­—èŠ‚è·³åŠ¨")

    if st.button("ğŸ” ç”Ÿæˆé€Ÿè§ˆæŠ¥å‘Š", use_container_width=True):
        if not company_name:
            st.warning("è¯·è¾“å…¥å…¬å¸åç§°ã€‚")
        else:
            with st.spinner(f"æ­£åœ¨ç”Ÿæˆå…³äº â€œ{company_name}â€ çš„ä¿¡æ¯æŠ¥å‘Š..."):
                try:
                    response = chain.invoke({"company_name": company_name})
                    st.markdown("---")
                    st.subheader(f"ğŸ“„ {company_name} - æ ¸å¿ƒä¿¡æ¯é€Ÿè§ˆ")
                    st.markdown(response.content)
                except Exception as e:
                    st.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")


# --- Main App Logic ---
# --- ä¸»åº”ç”¨é€»è¾‘ ---
def main():
    """Main function to run the Streamlit app."""
    llm = get_llm_instance()
    if not llm:
        st.stop()

    with st.sidebar:
        st.title("å¯¼èˆª")
        if st.session_state.current_mode != "menu":
            if st.button("è¿”å›ä¸»èœå•"):
                # A more robust reset
                for key in list(st.session_state.keys()):
                    if key not in ['current_mode']: # Keep the mode key to avoid error on first run
                        del st.session_state[key]
                st.session_state.current_mode = "menu"
                st.rerun()

    modes = {
        "menu": render_menu,
        "exploration": lambda: render_exploration_mode(llm),
        "decision": lambda: render_decision_mode(llm),
        "communication": lambda: render_communication_mode(llm),
        "company_info": lambda: render_company_info_mode(llm),
    }
    modes[st.session_state.current_mode]()


if __name__ == "__main__":
    main()