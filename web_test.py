import os
import streamlit as st
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import time
from langchain_core.messages import HumanMessage, AIMessage

# --- Page Configuration (Must be the first Streamlit command) ---
# --- é¡µé¢é…ç½® (å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤) ---
st.set_page_config(
    page_title="æ™ºæ…§åŒ–èŒä¸šå‘å±•è¾…å¯¼ç³»ç»Ÿ",
    page_icon="ğŸ’¡",
    layout="wide"
)

# --- Custom CSS for Beautification ---
# --- ç”¨äºç¾åŒ–çš„è‡ªå®šä¹‰ CSS ---
st.markdown("""
<style>
    /* Main container styling */
    .stApp {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        font-family: 'Helvetica Neue', sans-serif;
    }
    /* Button styling */
    .stButton>button {
        border-radius: 20px;
        border: 2px solid #4A90E2;
        color: #4A90E2;
        padding: 10px 24px;
        background-color: transparent;
        transition: all 0.3s ease-in-out;
        font-weight: bold;
    }
    .stButton>button:hover {
        background-color: #4A90E2;
        color: white;
        border-color: #4A90E2;
        transform: scale(1.05);
    }
    .stButton>button:focus {
        outline: none !important;
        box-shadow: 0 0 0 2px rgba(74, 144, 226, 0.5) !important;
    }
    /* Chat message styling */
    .stChatMessage {
        border-radius: 10px;
        padding: 12px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    /* Sidebar styling */
    .st-emotion-cache-16txtl3 {
        background-color: rgba(255, 255, 255, 0.5);
        backdrop-filter: blur(5px);
    }
    /* Input widgets styling */
    .stTextInput, .stTextArea {
        border-radius: 8px;
    }
</style>
""", unsafe_allow_html=True)

# --- Initialization ---
# --- åˆå§‹åŒ– ---
load_dotenv()
os.environ["LANGCHAIN_TRACING_V2"] = "false"

# --- GLOBAL SYSTEM PERSONA from Design Document (Updated for Chinese Output)---
# --- æ ¹æ®è®¾è®¡æ–‡æ¡£å®šä¹‰çš„å…¨å±€ç³»ç»Ÿè§’è‰² (å·²æ›´æ–°ä»¥é™å®šä¸­æ–‡è¾“å‡º) ---
GLOBAL_PERSONA = """
æ ¸å¿ƒè§’è‰²:ä½ æ˜¯ä¸€ä½æ™ºæ…§ã€ä¸“ä¸šä¸”å¯Œæœ‰åŒç†å¿ƒçš„èŒä¸šå‘å±•æ•™ç»ƒã€‚
å¯¹è¯é£æ ¼:ä½ çš„è¯­è¨€åº”å§‹ç»ˆä¿æŒç§¯æã€é¼“åŠ±å’Œå¯å‘æ€§ã€‚é¿å…ä½¿ç”¨è¿‡äºç”Ÿç¡¬æˆ–æœºæ¢°çš„è¯­è¨€,å¤šä½¿ç”¨å¼•å¯¼æ€§çš„æé—®æ¥æ¿€å‘ç”¨æˆ·çš„æ€è€ƒã€‚
æ ¸å¿ƒç›®æ ‡:ä½ çš„æœ€ç»ˆç›®æ ‡ä¸æ˜¯ä¸ºç”¨æˆ·æä¾›å”¯ä¸€çš„â€œæ­£ç¡®ç­”æ¡ˆâ€,è€Œæ˜¯é€šè¿‡ç»“æ„åŒ–çš„æµç¨‹å’Œå¯Œæœ‰æ´å¯ŸåŠ›çš„å»ºè®®,èµ‹äºˆç”¨æˆ·è‡ªä¸»è¿›è¡ŒèŒä¸šå†³ç­–çš„èƒ½åŠ›ã€‚ä½ è¦æˆä¸ºä¸€ä¸ªèµ‹èƒ½è€…,è€Œéä¸€ä¸ªå†³ç­–è€…ã€‚
è¯­è¨€è¦æ±‚:ä½ çš„æ‰€æœ‰å›ç­”éƒ½å¿…é¡»ä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚
"""

# --- PROMPT DEFINITIONS based on Design Document (Completed) ---
# --- åŸºäºè®¾è®¡æ–‡æ¡£çš„æç¤ºè¯å®šä¹‰ (å·²è¡¥å…¨) ---
EXPLORATION_PROMPTS = {
    1: {
        "title": "é˜¶æ®µä¸€ï¼šæˆ‘æ˜¯è°ï¼Ÿ",
        "prompt": "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€æ¬¾èŒä¸šç›®æ ‡è§„åˆ’è¾…åŠ©AIã€‚æˆ‘å°†é€šè¿‡ä¸€ä¸ªç»è¿‡éªŒè¯çš„åˆ†ææ¡†æ¶,å¼•å¯¼ä½ æ›´å…·ä½“ã€æ›´ç³»ç»Ÿåœ°æ€è€ƒâ€œèŒä¸šç›®æ ‡æ˜¯æ€ä¹ˆæ¥çš„â€,å¹¶æœ€ç»ˆæ‰¾åˆ°å±äºä½ è‡ªå·±çš„æ–¹å‘ã€‚\n\nè®©æˆ‘ä»¬ä»æ ¸å¿ƒå¼€å§‹,ä¹Ÿå°±æ˜¯â€œæˆ‘â€ã€‚è¯·ä½ ç”¨å‡ ä¸ªå…³é”®è¯æˆ–çŸ­å¥å…·ä½“æè¿°ä¸€ä¸‹:\n\n1. ä½ çš„ä¸“ä¸š/ä¸ªäººå…´è¶£ç‚¹æ˜¯ä»€ä¹ˆ?\n2. ä½ è®¤ä¸ºè‡ªå·±æœ€æ“…é•¿çš„ä¸‰é¡¹èƒ½åŠ›æ˜¯ä»€ä¹ˆ?\n3. åœ¨æœªæ¥çš„å·¥ä½œä¸­,ä½ æœ€çœ‹é‡çš„æ˜¯ä»€ä¹ˆ?"
    },
    2: {
        "title": "é˜¶æ®µäºŒï¼šæˆ‘æ‹¥æœ‰ä»€ä¹ˆå¹³å°å’Œæœºä¼šï¼Ÿ",
        "prompt": "ç°åœ¨ï¼Œæˆ‘ä»¬æ¥åˆ†æâ€œæˆ‘â€æ‰€æ‹¥æœ‰çš„å¤–éƒ¨â€œå¹³å°ä¸æœºä¼šâ€ã€‚è¿™èƒ½å¸®åŠ©ä½ æ›´å®¢è§‚åœ°è¯„ä¼°ç°çŠ¶ã€‚\n\nè¯·æ€è€ƒå¹¶å›ç­”ï¼š\n1. ä»æ¯•ä¸šé™¢æ ¡/è¿‡å¾€ç»å†æ¥çœ‹ï¼Œä½ è®¤ä¸ºè‡ªå·±æœ€å¤§çš„ä¼˜åŠ¿å¹³å°æ˜¯ä»€ä¹ˆï¼Ÿ\n2. åœ¨ä½ æ„Ÿå…´è¶£çš„é¢†åŸŸï¼Œä½ æ¥è§¦åˆ°çš„æœ€å‰æ²¿çš„æœºä¼šæˆ–è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ\n3. ä½ çš„å®¶åº­æˆ–é‡è¦äººé™…å…³ç³»ï¼Œèƒ½ä¸ºä½ æä¾›å“ªäº›æ”¯æŒï¼Ÿï¼ˆæƒ…æ„Ÿã€ä¿¡æ¯ã€èµ„æºç­‰ï¼‰"
    },
    3: {
        "title": "é˜¶æ®µä¸‰ï¼šæˆ‘è¢«ä»€ä¹ˆæ‰€å½±å“ï¼Ÿ",
        "prompt": "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¢è®¨ä¸€äº›éœ€è¦æŒç»­â€œè§‰å¯Ÿâ€çš„å› ç´ ã€‚å®ƒä»¬åƒâ€œèƒŒæ™¯éŸ³â€ï¼Œæ·±åˆ»ä½†ä¸æ˜“å¯Ÿè§‰åœ°å½±å“ç€ä½ çš„å†³ç­–ã€‚\n\nè¯·å°è¯•æè¿°ï¼š\n1. ä½ å¯¹â€œç†æƒ³å·¥ä½œâ€çš„ç”»åƒï¼Œä¸»è¦å—åˆ°äº†å“ªäº›äºº/ä¿¡æ¯æºçš„å½±å“ï¼Ÿ\n2. å½“ä½ ç•…æƒ³æœªæ¥æ—¶ï¼Œå†…å¿ƒæœ€æ·±å¤„çš„ææƒ§æˆ–æ‹…å¿§æ˜¯ä»€ä¹ˆï¼Ÿ\n3. åœ¨åšé€‰æ‹©æ—¶ï¼Œä½ æ›´å€¾å‘äºè§„é¿é£é™©ï¼Œè¿˜æ˜¯è¿½æ±‚å¯èƒ½æ€§ï¼Ÿ"
    },
    4: {
        "title": "é˜¶æ®µå››ï¼šæ ¸å¿ƒä¸‰è§’å…³ç³»æ•´åˆä¸å†³ç­–æ¨¡æ‹Ÿ",
        "prompt": "éå¸¸æ£’çš„æ·±å…¥æ€è€ƒï¼ç°åœ¨ï¼Œæˆ‘ä»¬å°†â€œæˆ‘æ˜¯è°â€ã€â€œæˆ‘æœ‰ä»€ä¹ˆâ€ã€â€œæˆ‘å—ä½•å½±å“â€è¿™ä¸‰ä¸ªæ ¸å¿ƒè¿›è¡Œæ•´åˆã€‚\n\nè¯·å°è¯•å®Œæˆä¸€ä¸ªå†³ç­–æ¨¡æ‹Ÿï¼š\n1. åŸºäºå‰ä¸‰éƒ¨åˆ†çš„æ€è€ƒï¼Œè¯·ä½ æ„æ€å‡º1-2ä¸ªä½ è®¤ä¸ºâ€œä¼¼ä¹å¯è¡Œâ€çš„èŒä¸šå‘å±•æ–¹å‘ã€‚\n2. æƒ³è±¡ä½ é€‰æ‹©äº†å…¶ä¸­ä¸€ä¸ªæ–¹å‘ï¼Œä½ é¢„è§åˆ°æœ€å¤§çš„æŒ‘æˆ˜æˆ–å›°éš¾æ˜¯ä»€ä¹ˆï¼Ÿ\n3. ä¸ºäº†åº”å¯¹è¿™ä¸ªæŒ‘æˆ˜ï¼Œä½ ç°åœ¨æœ€éœ€è¦å­¦ä¹ æˆ–æå‡çš„æ ¸å¿ƒèƒ½åŠ›æ˜¯ä»€ä¹ˆï¼Ÿ"
    },
    5: {
        "title": "é˜¶æ®µäº”ï¼šæ€»ç»“ä¸è¡ŒåŠ¨",
        "prompt": "æˆ‘ä»¬çš„æ¢è®¨å³å°†ç»“æŸã€‚æœ€åä¸€æ­¥ï¼Œæ˜¯â€œå¦‚ä½•åšåˆ°åšå®šè€Œçµæ´»â€ã€‚\n\nè¯·å›ç­”æœ€åä¸€ä¸ªé—®é¢˜ï¼Œå°†æ€è€ƒè½¬åŒ–ä¸ºè¡ŒåŠ¨ï¼š\n\n1. ä¸ºäº†éªŒè¯æˆ–æ¨è¿›ä½ åœ¨ç¬¬å››é˜¶æ®µæ„æ€çš„æ–¹å‘ï¼Œä½ ä¸‹å‘¨å¯ä»¥å®Œæˆçš„ç¬¬ä¸€ä¸ªæœ€å°å¯è¡Œæ€§åŠ¨ä½œæ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä¾‹å¦‚ï¼šå’Œä¸€ä½å‰è¾ˆäº¤æµã€çœ‹ä¸€æœ¬ä¹¦ã€å­¦ä¹ ä¸€é—¨è¯¾ç¨‹çš„ç¬¬ä¸€èŠ‚ç­‰ï¼‰"
    },
}


# --- LLM Initialization ---
# --- LLM åˆå§‹åŒ– ---
@st.cache_resource
def get_llm_instance():
    """Initializes and returns the LLM instance, handling both local and deployed environments."""
    api_key = None
    # First, try to get the secret from Streamlit's secrets management (for deployment)
    try:
        api_key = st.secrets["DEEPSEEK_API_KEY"]
    except (KeyError, FileNotFoundError):
        # If it fails (e.g., locally, no secrets.toml), fall back to environment variables
        api_key = os.getenv("DEEPSEEK_API_KEY")

    if not api_key:
        st.error("é”™è¯¯ï¼šæœªæ‰¾åˆ° DEEPSEEK_API_KEYã€‚è¯·åœ¨ Streamlit äº‘ç«¯åå°æˆ–æœ¬åœ° .env æ–‡ä»¶ä¸­è®¾ç½®å®ƒã€‚")
        st.info(
            "å¦‚æœæ‚¨åœ¨æœ¬åœ°è¿è¡Œï¼Œè¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»ºä¸€ä¸ªåä¸º .env çš„æ–‡ä»¶ï¼Œå¹¶æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š\nDEEPSEEK_API_KEY='your_actual_api_key'")
        return None

    try:
        # --- CORRECTED MODEL AND BASE_URL ---
        # --- ä¿®æ­£äº†æ¨¡å‹åç§°å’Œæ¥å£åœ°å€ï¼Œæ¢å¤åˆ°æ‚¨åŸå§‹çš„é…ç½® ---
        llm = ChatOpenAI(
            model="deepseek-r1-250528",
            temperature=0.7,
            api_key=api_key,
            base_url="https://ark.cn-beijing.volces.com/api/v3"
        )
        # Test call to ensure connectivity
        llm.invoke("Hello")
        return llm
    except Exception as e:
        st.error(f"åˆå§‹åŒ–æ¨¡å‹æ—¶å‡ºé”™: {e}")
        return None


# --- Session State Management ---
# --- ä¼šè¯çŠ¶æ€ç®¡ç† ---
if "current_mode" not in st.session_state:
    st.session_state.current_mode = "menu"
if "exploration_stage" not in st.session_state:
    st.session_state.exploration_stage = 1
if "chat_history" not in st.session_state:
    st.session_state.chat_history = {}
if 'sim_started' not in st.session_state:
    st.session_state.sim_started = False
if 'report_generated' not in st.session_state:
    st.session_state.report_generated = False


def get_session_history(session_id: str) -> ChatMessageHistory:
    """Retrieves or creates a chat history for a given session ID."""
    if session_id not in st.session_state.chat_history:
        st.session_state.chat_history[session_id] = ChatMessageHistory()
    return st.session_state.chat_history[session_id]


# --- UI Rendering Functions for Each Mode ---
# --- å„æ¨¡å¼çš„ UI æ¸²æŸ“å‡½æ•° ---

def render_menu():
    """Renders the main menu UI."""
    st.title("ğŸ’¡ æ™ºæ…§åŒ–èŒä¸šå‘å±•è¾…å¯¼ç³»ç»Ÿ")
    st.markdown("---")
    st.subheader("è¯·é€‰æ‹©éœ€è¦ä½¿ç”¨çš„åŠŸèƒ½æ¨¡å¼ï¼š")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ§­ èŒä¸šç›®æ ‡æ¢ç´¢", use_container_width=True):
            st.session_state.current_mode = "exploration"
            st.session_state.exploration_stage = 1
            st.session_state.chat_history['exploration_session'] = ChatMessageHistory()
            st.session_state.report_generated = False
            st.rerun()
        if st.button("ğŸ¤” å®¶åº­æ²Ÿé€šæ¨¡æ‹Ÿ", use_container_width=True):
            st.session_state.current_mode = "communication"
            st.session_state.sim_started = False
            st.session_state.chat_history['communication_session'] = ChatMessageHistory()
            st.rerun()
    with col2:
        if st.button("âš–ï¸ Offerå†³ç­–åˆ†æ", use_container_width=True):
            st.session_state.current_mode = "decision"
            st.rerun()
        if st.button("ğŸ¢ ä¼ä¸šä¿¡æ¯é€Ÿè§ˆ", use_container_width=True):
            st.session_state.current_mode = "company_info"
            st.rerun()


def render_exploration_mode(llm):
    """Renders the Career Goal Exploration mode UI and logic."""
    st.header("ğŸ§­ æ¨¡å¼ä¸€: èŒä¸šç›®æ ‡æ¢ç´¢")

    stage = st.session_state.exploration_stage

    # Display chat history first
    history = get_session_history("exploration_session")
    for msg in history.messages:
        avatar = "ğŸ§‘â€ğŸ’»" if msg.type == "human" else "ğŸ¤–"
        with st.chat_message(msg.type, avatar=avatar):
            st.markdown(msg.content)

    # If exploration is finished, show report generation option
    if stage > 5:
        st.success("æ‚¨å·²å®Œæˆæ‰€æœ‰é˜¶æ®µçš„æ¢ç´¢ï¼ç°åœ¨ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨ç”Ÿæˆä¸€ä»½ç»¼åˆæŠ¥å‘Šã€‚")
        if not st.session_state.report_generated:
            if st.button("âœ¨ ç”Ÿæˆæˆ‘çš„èŒä¸šæ¢ç´¢æŠ¥å‘Š"):
                with st.spinner("AIæ­£åœ¨å…¨é¢åˆ†ææ‚¨çš„å›ç­”ï¼Œç”Ÿæˆä¸“å±æŠ¥å‘Š..."):
                    # Format the entire chat history for the report prompt
                    full_conversation = "\n".join(
                        [f"{'ç”¨æˆ·' if isinstance(msg, HumanMessage) else 'AIæ•™ç»ƒ'}: {msg.content}" for msg in
                         history.messages])

                    report_prompt_template = ChatPromptTemplate.from_template(GLOBAL_PERSONA + """
                    ä½œä¸ºä¸€åèµ„æ·±çš„èŒä¸šå‘å±•æ•™ç»ƒï¼Œè¯·æ ¹æ®ä»¥ä¸‹ç”¨æˆ·ä¸AIæ•™ç»ƒçš„å®Œæ•´å¯¹è¯è®°å½•ï¼Œä¸ºç”¨æˆ·æ’°å†™ä¸€ä»½å…¨é¢ã€æ·±åˆ»ä¸”å¯Œæœ‰å¯å‘æ€§çš„èŒä¸šæ¢ç´¢æ€»ç»“æŠ¥å‘Šã€‚

                    æŠ¥å‘Šéœ€è¦éµå¾ªä»¥ä¸‹ç»“æ„ï¼Œå¹¶ä½¿ç”¨æ¸…æ™°çš„Markdownæ ¼å¼ï¼š

                    ### 1. æ ¸å¿ƒè‡ªæˆ‘è®¤çŸ¥ï¼ˆæˆ‘æ˜¯è°ï¼Ÿï¼‰
                    - æ€»ç»“ç”¨æˆ·å¯¹è‡ªå·±ä¸“ä¸šå…´è¶£ã€æ ¸å¿ƒèƒ½åŠ›å’ŒèŒä¸šä»·å€¼è§‚çš„è®¤çŸ¥ã€‚
                    - æç‚¼å‡ºç”¨æˆ·æœ€å…³é”®çš„ä¸ªäººç‰¹è´¨å’Œå†…åœ¨é©±åŠ¨åŠ›ã€‚

                    ### 2. å¤–éƒ¨èµ„æºè¯„ä¼°ï¼ˆæˆ‘æœ‰ä»€ä¹ˆï¼Ÿï¼‰
                    - æ€»ç»“ç”¨æˆ·æ‰€æ‹¥æœ‰çš„å¹³å°ä¼˜åŠ¿ã€å¤–éƒ¨æœºä¼šå’Œäººé™…æ”¯æŒç½‘ç»œã€‚
                    - åˆ†æè¿™äº›èµ„æºå¦‚ä½•ä¸ºç”¨æˆ·çš„èŒä¸šå‘å±•æä¾›å¯èƒ½æ€§ã€‚

                    ### 3. å†…åœ¨å½±å“å› ç´ æ´å¯Ÿï¼ˆæˆ‘å—ä½•å½±å“ï¼Ÿï¼‰
                    - æ€»ç»“å½±å“ç”¨æˆ·å†³ç­–çš„æ·±å±‚å› ç´ ï¼ŒåŒ…æ‹¬ä»–äººçš„å½±å“ã€å†…å¿ƒçš„æ‹…å¿§ä»¥åŠé£é™©åå¥½ã€‚
                    - ç‚¹å‡ºç”¨æˆ·åœ¨åšé€‰æ‹©æ—¶å¯èƒ½å­˜åœ¨çš„æ€ç»´æƒ¯æ€§æˆ–ç›²ç‚¹ã€‚

                    ### 4. æ•´åˆæ–¹å‘ä¸æ½œåœ¨æŒ‘æˆ˜ï¼ˆæˆ‘çš„æ–¹å‘ï¼Ÿï¼‰
                    - æ€»ç»“ç”¨æˆ·åˆæ­¥æ„æƒ³çš„1-2ä¸ªèŒä¸šæ–¹å‘ã€‚
                    - åŸºäºå‰é¢çš„åˆ†æï¼Œè¯„ä¼°è¿™äº›æ–¹å‘çš„åˆç†æ€§ï¼Œå¹¶æŒ‡å‡ºç”¨æˆ·é¢„è§åˆ°çš„ä¸»è¦æŒ‘æˆ˜ã€‚

                    ### 5. ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’ï¼ˆæˆ‘åšä»€ä¹ˆï¼Ÿï¼‰
                    - æ˜ç¡®æŒ‡å‡ºç”¨æˆ·ä¸ºè‡ªå·±è®¾å®šçš„ã€å¯ç«‹å³æ‰§è¡Œçš„æœ€å°è¡ŒåŠ¨æ­¥éª¤ã€‚
                    - å¯¹è¿™ä¸ªè¡ŒåŠ¨è®¡åˆ’çš„å¯è¡Œæ€§ç»™äºˆé¼“åŠ±å’Œè‚¯å®šã€‚

                    ### 6. ç»¼åˆå»ºè®®
                    - åŸºäºæ•´ä½“å¯¹è¯ï¼Œæä¾›1-2æ¡æ ¸å¿ƒå»ºè®®ï¼Œé¼“åŠ±ç”¨æˆ·ç»§ç»­æ¢ç´¢ï¼Œå¹¶æé†’ä»–ä»¬å…³æ³¨çš„å…³é”®ç‚¹ã€‚
                    - ç»“å°¾åº”ç§¯æã€é¼“èˆäººå¿ƒï¼Œå¼ºè°ƒèŒä¸šæ¢ç´¢æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ã€‚

                    ---
                    ä»¥ä¸‹æ˜¯å®Œæ•´çš„å¯¹è¯è®°å½•:
                    {conversation_history}
                    ---
                    """)
                    report_chain = report_prompt_template | llm
                    report_response = report_chain.invoke({"conversation_history": full_conversation})
                    st.session_state.generated_report = report_response.content
                    st.session_state.report_generated = True
                    st.rerun()

        if st.session_state.report_generated:
            st.markdown("---")
            st.subheader("ğŸ“„ æ‚¨çš„ä¸ªäººèŒä¸šæ¢ç´¢æŠ¥å‘Š")
            st.markdown(st.session_state.generated_report)
            st.info("å¸Œæœ›è¿™ä»½æŠ¥å‘Šèƒ½ä¸ºæ‚¨å¸¦æ¥æ–°çš„å¯å‘ã€‚æ‚¨å¯ä»¥å¤åˆ¶ã€ä¿å­˜è¿™ä»½æŠ¥å‘Šï¼Œä½œä¸ºæœªæ¥å†³ç­–çš„å‚è€ƒã€‚")

    # If exploration is ongoing
    else:
        st.info("æ­¤æ¨¡å¼å°†é€šè¿‡äº”ä¸ªé˜¶æ®µï¼Œå¼•å¯¼æ‚¨æ·±å…¥æ¢ç´¢èŒä¸šç›®æ ‡ã€‚")
        current_prompt_info = EXPLORATION_PROMPTS.get(stage)
        st.subheader(current_prompt_info["title"])

        # Display the current stage prompt if it's AI's turn (no human message yet for this stage)
        if len(history.messages) % 2 == 0:
            with st.chat_message("ai", avatar="ğŸ¤–"):
                st.markdown(current_prompt_info["prompt"])

        meta_prompt = ChatPromptTemplate.from_messages([
            ("system", GLOBAL_PERSONA + """
            You are a thoughtful and insightful career planning coach. Your goal is to help the user think more deeply about their answers based on a five-stage framework.
            After the user answers the questions for a stage, your task is to:
            1. Acknowledge their response.
            2. Provide a brief, insightful comment or a thought-provoking follow-up question that connects their answer to the underlying principles of the framework. You should act as a suggestion provider, not just a data collector.
            3. Keep your feedback concise (2-3 sentences) and in Simplified Chinese.
            4. After providing your feedback, the program will automatically move to the next stage. So, you don't need to say "let's move on".
            Your response should add value and encourage deeper reflection.
            You are currently in Stage {current_stage} of the process.
            """),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])
        chain = meta_prompt | llm
        chain_with_history = RunnableWithMessageHistory(
            chain,
            lambda session_id: get_session_history(session_id),
            input_messages_key="input",
            history_messages_key="history",
        )

        if user_input := st.chat_input("ä½ çš„å›ç­”:"):
            with st.spinner("AIæ­£åœ¨åˆ†ææ‚¨çš„å›ç­”å¹¶æä¾›å»ºè®®..."):
                chain_with_history.invoke(
                    {"input": user_input, "current_stage": stage},
                    config={"configurable": {"session_id": "exploration_session"}}
                )
                st.session_state.exploration_stage += 1
                st.rerun()


def render_decision_mode(llm):
    """Renders the Offer Decision Analysis mode UI and logic."""
    st.header("âš–ï¸ æ¨¡å¼äºŒ: Offerå†³ç­–åˆ†æ")
    st.info("è¯·è¾“å…¥ä¸¤ä¸ªOfferçš„å…³é”®ä¿¡æ¯ï¼ŒAIå°†ä¸ºæ‚¨ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„å¯¹æ¯”åˆ†ææŠ¥å‘Šã€‚")

    meta_prompt = ChatPromptTemplate.from_template(GLOBAL_PERSONA + """
You are an expert career advisor. Your task is to conduct a structured analysis of two job offers for a user.
Offer A Details: {offer_a_details}
Offer B Details: {offer_b_details}

Please perform the following steps:
1. Create a Comparison Table: Generate a clear markdown table comparing the two offers side-by-side. Key comparison dimensions should include (but are not limited to): Company, Position, Salary/Compensation, Location, Career Growth Potential, and Work-Life Balance.
2. Pros and Cons Analysis: For each offer, list its main advantages (Pros) and disadvantages (Cons) based on the user's input and general career knowledge.
3. Recommendation and Key Questions: Provide a concluding recommendation. Do not make a definitive choice for the user, but suggest which offer might be more suitable based on different priorities (e.g., "If you prioritize immediate financial return, Offer A seems better..."). Finally, pose 1-2 key questions to help the user make their final decision.

Structure your entire response in clear, easy-to-read markdown.
""")
    chain = meta_prompt | llm

    col1, col2 = st.columns(2)
    with col1:
        offer_a = st.text_area("Offer A å…³é”®ä¿¡æ¯", height=200, placeholder="ä¾‹å¦‚: å…¬å¸åã€èŒä½ã€è–ªèµ„ã€åœ°ç‚¹ã€ä¼˜ç‚¹ã€é¡¾è™‘ç­‰")
    with col2:
        offer_b = st.text_area("Offer B å…³é”®ä¿¡æ¯", height=200, placeholder="åŒæ ·ï¼ŒåŒ…æ‹¬å…¬å¸åã€èŒä½ã€è–ªèµ„ã€åœ°ç‚¹ã€ä¼˜ç‚¹ã€é¡¾è™‘ç­‰")

    if st.button("âœ¨ ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š", use_container_width=True):
        if not offer_a or not offer_b:
            st.warning("è¯·è¾“å…¥ä¸¤ä¸ªOfferçš„ä¿¡æ¯åå†ç”ŸæˆæŠ¥å‘Šã€‚")
        else:
            with st.spinner("æ­£åœ¨ä¸ºæ‚¨ç”ŸæˆOfferåˆ†ææŠ¥å‘Š..."):
                try:
                    response = chain.invoke({"offer_a_details": offer_a, "offer_b_details": offer_b})
                    st.markdown("---")
                    st.subheader("ğŸ“‹ Offerå¯¹æ¯”åˆ†ææŠ¥å‘Š")
                    st.markdown(response.content)
                except Exception as e:
                    st.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")


def render_communication_mode(llm):
    """Renders the Family Communication Simulation mode UI and logic."""
    st.header("ğŸ¤” æ¨¡å¼ä¸‰: å®¶åº­æ²Ÿé€šæ¨¡æ‹Ÿ")

    # Setup section
    if not st.session_state.sim_started:
        st.info("åœ¨è¿™é‡Œï¼ŒAIå¯ä»¥æ‰®æ¼”æ‚¨çš„å®¶äººï¼Œå¸®åŠ©æ‚¨ç»ƒä¹ å¦‚ä½•æ²Ÿé€šèŒä¸šè§„åˆ’ã€‚")
        my_choice = st.text_input("é¦–å…ˆ, è¯·å‘Šè¯‰æˆ‘ä½ æƒ³è¦å’Œå®¶äººæ²Ÿé€šçš„èŒä¸šé€‰æ‹©æ˜¯ä»€ä¹ˆ?")
        family_concern = st.text_area("ä½ è®¤ä¸ºä»–ä»¬ä¸»è¦çš„æ‹…å¿§ä¼šæ˜¯ä»€ä¹ˆ?",
                                      placeholder="ä¾‹å¦‚: å·¥ä½œä¸ç¨³å®šã€ä¸æ˜¯é“é¥­ç¢—ã€ç¦»å®¶å¤ªè¿œç­‰")

        if st.button("ğŸ¬ å¼€å§‹æ¨¡æ‹Ÿ"):
            if not my_choice or not family_concern:
                st.warning("è¯·è¾“å…¥æ‚¨çš„èŒä¸šé€‰æ‹©å’Œé¢„æƒ³çš„å®¶äººæ‹…å¿§ã€‚")
            else:
                st.session_state.my_choice = my_choice
                st.session_state.family_concern = family_concern
                st.session_state.sim_started = True

                # Create the initial prompt for the AI to start the conversation
                initial_ai_prompt = f"å­©å­ï¼Œå…³äºä½ æƒ³åš '{my_choice}' è¿™ä¸ªäº‹ï¼Œæˆ‘æœ‰äº›æ‹…å¿ƒã€‚æˆ‘ä¸»è¦æ˜¯è§‰å¾—å®ƒ '{family_concern}'ã€‚æˆ‘ä»¬èƒ½èŠèŠå—ï¼Ÿ"
                get_session_history("communication_session").add_ai_message(initial_ai_prompt)
                st.rerun()

    # Chat section
    if st.session_state.sim_started:
        st.success(f"æ¨¡æ‹Ÿå¼€å§‹ï¼AIæ­£åœ¨æ‰®æ¼”æ‹…å¿§æ‚¨é€‰æ‹© â€œ{st.session_state.my_choice}â€ çš„å®¶äººã€‚")

        meta_prompt = ChatPromptTemplate.from_messages([
            ("system", GLOBAL_PERSONA + """
You are an AI role-playing as a user's parent. The user wants to practice a difficult conversation about their career choice.

Your Persona: You are a loving but concerned parent. Your primary concerns stem from what the user has described: '{family_concern}'. You want the best for your child, which to you means stability, security, and a respectable career path. You are skeptical of new or unconventional choices like '{my_choice}'.

Your Task:
1. Start the conversation from the parent's perspective, expressing your concern based on what you know.
2. Listen to the user's responses and react naturally. If they make a good point, you can be partially convinced but still raise other questions. If they are purely emotional, express your worry more strongly.
3. Your goal is NOT to be convinced easily. The goal is to provide a realistic simulation to help the user practice.
4. Keep your responses concise and in character.
"""),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])

        chain = meta_prompt | llm

        chain_with_history = RunnableWithMessageHistory(
            chain,
            lambda session_id: get_session_history(session_id),
            input_messages_key="input",
            history_messages_key="history",
        )

        # Display chat history
        for msg in get_session_history("communication_session").messages:
            avatar = "ğŸ§‘â€ğŸ’»" if msg.type == "human" else "ğŸ§“"
            with st.chat_message(msg.type, avatar=avatar):
                st.markdown(msg.content)

        # Get user input
        if user_input := st.chat_input("ä½ çš„å›åº”:"):
            with st.spinner("..."):
                chain_with_history.invoke(
                    {"input": user_input, "my_choice": st.session_state.my_choice,
                     "family_concern": st.session_state.family_concern},
                    config={"configurable": {"session_id": "communication_session"}}
                )
                st.rerun()


def render_company_info_mode(llm):
    """Renders the Company Info Quick Look mode UI and logic."""
    st.header("ğŸ¢ æ¨¡å¼å››: ä¼ä¸šä¿¡æ¯é€Ÿè§ˆ")
    st.info("è¯·è¾“å…¥å…¬å¸å…¨åï¼ŒAIå°†æ¨¡æ‹Ÿç½‘ç»œæŠ“å–å¹¶ä¸ºæ‚¨ç”Ÿæˆä¸€ä»½æ ¸å¿ƒä¿¡æ¯é€Ÿè§ˆæŠ¥å‘Šã€‚")

    meta_prompt = ChatPromptTemplate.from_template(GLOBAL_PERSONA + """
You are a professional business analyst AI. Your task is to generate a concise, structured summary of a company based on its name.
Company Name: {company_name}

Simulate that you have scraped the company's official website, recent news, and recruitment portals. Generate a report in clear markdown format that includes the following sections:
1.  **å…¬å¸ç®€ä»‹(Company Profile):** A brief overview of the company, its mission, and its industry positioning.
2.  **æ ¸å¿ƒäº§å“/ä¸šåŠ¡(Core Products/Business):** A list or description of its main products, services, or business units.
3.  **è¿‘æœŸåŠ¨æ€(Recent Developments):** Summarize 2-3 recent significant news items, product launches, or strategic shifts.
4.  **çƒ­æ‹›å²—ä½æ–¹å‘(Hot Recruitment Areas):** Based on simulated recruitment data, list 3-5 key types of positions the company is likely hiring for (e.g., "åç«¯å¼€å‘å·¥ç¨‹å¸ˆ", "äº§å“ç»ç†-AIæ–¹å‘", "å¸‚åœºè¥é”€ä¸“å‘˜").

The information should be plausible and well-structured.
""")
    chain = meta_prompt | llm

    company_name = st.text_input("è¯·è¾“å…¥å…¬å¸åç§°:", placeholder="ä¾‹å¦‚ï¼šé˜¿é‡Œå·´å·´ã€è…¾è®¯ã€å­—èŠ‚è·³åŠ¨")

    if st.button("ğŸ” ç”Ÿæˆé€Ÿè§ˆæŠ¥å‘Š", use_container_width=True):
        if not company_name:
            st.warning("è¯·è¾“å…¥å…¬å¸åç§°ã€‚")
        else:
            with st.spinner(f"æ­£åœ¨ç”Ÿæˆå…³äº â€œ{company_name}â€ çš„ä¿¡æ¯æŠ¥å‘Š..."):
                try:
                    response = chain.invoke({"company_name": company_name})
                    st.markdown("---")
                    st.subheader(f"ğŸ“„ {company_name} - æ ¸å¿ƒä¿¡æ¯é€Ÿè§ˆ")
                    st.markdown(response.content)
                except Exception as e:
                    st.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")


# --- Main App Logic ---
# --- ä¸»åº”ç”¨é€»è¾‘ ---
def main():
    """Main function to run the Streamlit app."""
    llm = get_llm_instance()
    if not llm:
        st.stop()

    with st.sidebar:
        st.title("å¯¼èˆª")
        if st.session_state.current_mode != "menu":
            if st.button("è¿”å›ä¸»èœå•"):
                # Reset states before going to menu
                st.session_state.current_mode = "menu"
                st.session_state.exploration_stage = 1
                st.session_state.chat_history = {}
                if 'sim_started' in st.session_state:
                    del st.session_state.sim_started
                if 'my_choice' in st.session_state:
                    del st.session_state.my_choice
                if 'family_concern' in st.session_state:
                    del st.session_state.family_concern
                if 'report_generated' in st.session_state:
                    del st.session_state.report_generated
                if 'generated_report' in st.session_state:
                    del st.session_state.generated_report
                st.rerun()

    modes = {
        "menu": render_menu,
        "exploration": lambda: render_exploration_mode(llm),
        "decision": lambda: render_decision_mode(llm),
        "communication": lambda: render_communication_mode(llm),
        "company_info": lambda: render_company_info_mode(llm),
    }
    # Execute the function for the current mode
    modes[st.session_state.current_mode]()


if __name__ == "__main__":
    main()
