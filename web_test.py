import os
import streamlit as st
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import time
from langchain_core.messages import HumanMessage, AIMessage

# --- é é¢é…ç½® (å¿…é ˆæ˜¯ç¬¬ä¸€å€‹ Streamlit å‘½ä»¤) ---
st.set_page_config(
    page_title="æ™ºæ…§åŒ–è·æ¥­ç™¼å±•è¼”å°ç³»çµ± V4.0",
    page_icon="ğŸ’¡",
    layout="wide"
)

# --- ç”¨æ–¼ç¾åŒ–çš„è‡ªå®šç¾© CSS ---
st.markdown("""
<style>
    /* Main container styling */
    .stApp {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        font-family: 'Helvetica Neue', sans-serif;
    }
    /* Button styling */
    .stButton>button {
        border-radius: 20px;
        border: 2px solid #4A90E2;
        color: #4A90E2;
        padding: 10px 24px;
        background-color: transparent;
        transition: all 0.3s ease-in-out;
        font-weight: bold;
    }
    .stButton>button:hover {
        background-color: #4A90E2;
        color: white;
        border-color: #4A90E2;
        transform: scale(1.05);
    }
    .stButton>button:focus {
        outline: none !important;
        box-shadow: 0 0 0 2px rgba(74, 144, 226, 0.5) !important;
    }
    /* Chat message styling */
    .stChatMessage {
        border-radius: 10px;
        padding: 12px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    /* Sidebar styling */
    .st-emotion-cache-16txtl3 {
        background-color: rgba(255, 255, 255, 0.5);
        backdrop-filter: blur(5px);
    }
    /* Input widgets styling */
    .stTextInput, .stTextArea, .stMultiSelect {
        border-radius: 8px;
    }
</style>
""", unsafe_allow_html=True)

# --- åˆå§‹åŒ– ---
load_dotenv()
os.environ["LANGCHAIN_TRACING_V2"] = "false"

# --- [V4.0 æ›´æ–°] æ ¹æ“šè¨­è¨ˆæ–‡æª”å®šç¾©çš„å…¨åŸŸç³»çµ±è§’è‰² ---
GLOBAL_PERSONA = """
æ ¸å¿ƒè§’è‰²:ä½ æ˜¯ä¸€ä½æ™ºæ…§ã€å°ˆæ¥­ä¸”å¯Œæœ‰åŒç†å¿ƒçš„è·æ¥­ç™¼å±•æ•™ç·´ã€‚
å°è©±é¢¨æ ¼:ä½ çš„èªè¨€æ‡‰å§‹çµ‚ä¿æŒç©æ¥µã€é¼“å‹µå’Œå•Ÿç™¼æ€§ã€‚é¿å…ä½¿ç”¨éäºç”Ÿç¡¬æˆ–æ©Ÿæ¢°çš„èªè¨€,å¤šä½¿ç”¨å¼•å°æ€§çš„æå•ä¾†æ¿€ç™¼ç”¨æˆ¶çš„æ€è€ƒã€‚
æ ¸å¿ƒç›®æ¨™:ä½ çš„æœ€çµ‚ç›®æ¨™ä¸æ˜¯ç‚ºç”¨æˆ¶æä¾›å”¯ä¸€çš„â€œæ­£ç¢ºç­”æ¡ˆâ€,è€Œæ˜¯é€šè¿‡çµæ§‹åŒ–çš„æµç¨‹å’Œå¯Œæœ‰æ´å¯ŸåŠ›çš„å»ºè­°,è³¦äºˆç”¨æˆ¶è‡ªä¸»é€²è¡Œè·æ¥­æ±ºç­–çš„èƒ½åŠ›ã€‚ä½ è¦æˆç‚ºä¸€å€‹è³¦èƒ½è€…,è€Œéä¸€å€‹æ±ºç­–è€…ã€‚
æ ¸å¿ƒè¨­è¨ˆå“²å­¸: è³¦èƒ½å„ªå…ˆäºæŒ‡ä»¤, ä½ æ‡‰å¼•å°ç”¨æˆ¶ç¨ç«‹æ€è€ƒ; ç›¡åŠ›åšåˆ°æƒ…å¢ƒæ„ŸçŸ¥ä¸å€‹æ€§åŒ–; ä½ çš„åˆ†æéç¨‹å’Œæ•¸æ“šä¾†æºæ‡‰ç›¡å¯èƒ½é€æ˜ã€‚
å€«ç†ä¸å®‰å…¨é‚Šç•Œ: æ˜ç¢ºå‘ŠçŸ¥ç”¨æˆ¶,å…¶è¼¸å…¥ä¿¡æ¯åƒ…ç”¨äºç•¶æ¬¡åˆ†æã€‚åœ¨å°è©±ä¸­è¦æŒçºŒè¦é¿æ€§åˆ¥ã€åœ°åŸŸç­‰åè¦‹ã€‚å¦‚æœç”¨æˆ¶è¡¨ç¾å‡ºåš´é‡çš„å¿ƒç†å›°æ“¾æˆ–æåŠç²¾ç¥å¥åº·å±æ©Ÿ,å¿…é ˆèƒ½è­˜åˆ¥ä¸¦æº«å’Œåœ°ä¸­æ–·è·æ¥­è¼”å°,è½‰è€Œå»ºè­°ç”¨æˆ¶å°‹æ±‚å°ˆæ¥­çš„å¿ƒç†å¥åº·æ”¯æŒã€‚
èªè¨€è¦æ±‚:ä½ çš„æ‰€æœ‰å›ç­”éƒ½å¿…é ˆä½¿ç”¨ç°¡é«”ä¸­æ–‡ã€‚
"""

# --- åŸºæ–¼è¨­è¨ˆæ–‡æª”çš„æç¤ºè©å®šç¾© (å·²è£œå…¨) ---
EXPLORATION_PROMPTS = {
    1: {
        "title": "éšæ®µä¸€ï¼šæˆ‘æ˜¯èª°ï¼Ÿ",
        "prompt": "ä½ å¥½!æˆ‘æ˜¯ä¸€æ¬¾è·æ¥­ç›®æ¨™è¦åŠƒè¼”åŠ©AIã€‚æˆ‘å°‡é€šè¿‡ä¸€å€‹ç¶“éé©—è­‰çš„åˆ†ææ¡†æ¶,å¼•å°ä½ æ›´å…·é«”ã€æ›´ç³»çµ±åœ°æ€è€ƒâ€è·æ¥­ç›®æ¨™æ˜¯æ€éº¼ä¾†çš„â€,ä¸¦æœ€çµ‚æ‰¾åˆ°å±¬äºä½ è‡ªå·±çš„æ–¹å‘ã€‚\n\nè®“æˆ‘å€‘å¾æ ¸å¿ƒé–‹å§‹,ä¹Ÿå°±æ˜¯â€œæˆ‘â€ã€‚è«‹ä½ ç”¨å¹¾å€‹é—œéµè©æˆ–çŸ­å¥å…·é«”æè¿°ä¸€ä¸‹:\n\n1.ä½ çš„å°ˆæ¥­/å€‹äººèˆˆè¶£é»æ˜¯ä»€éº¼?\n2. ä½ èªç‚ºè‡ªå·±æœ€æ“…é•·çš„ä¸‰é …èƒ½åŠ›æ˜¯ä»€éº¼?\n3.åœ¨æœªä¾†çš„å·¥ä½œä¸­,ä½ æœ€çœ‹é‡çš„æ˜¯ä»€éº¼?"
    },
    2: {
        "title": "éšæ®µäºŒï¼šæˆ‘æ“æœ‰ä»€éº¼å¹³å°å’Œæ©Ÿæœƒï¼Ÿ",
        "prompt": "ç¾åœ¨ï¼Œæˆ‘å€‘ä¾†åˆ†æâ€œæˆ‘â€æ‰€æ“æœ‰çš„å¤–éƒ¨â€œå¹³å°èˆ‡æ©Ÿæœƒâ€ã€‚é€™èƒ½å¹«åŠ©ä½ æ›´å®¢è§€åœ°è©•ä¼°ç¾ç‹€ã€‚\n\nè«‹æ€è€ƒä¸¦å›ç­”ï¼š\n1. å¾ç•¢æ¥­é™¢æ ¡/éå¾€ç¶“æ­·ä¾†çœ‹ï¼Œä½ èªç‚ºè‡ªå·±æœ€å¤§çš„å„ªå‹¢å¹³å°æ˜¯ä»€éº¼ï¼Ÿ\n2. åœ¨ä½ æ„Ÿèˆˆè¶£çš„é ˜åŸŸï¼Œä½ æ¥è§¸åˆ°çš„æœ€å‰æ²¿çš„æ©Ÿæœƒæˆ–è¶¨å‹¢æ˜¯ä»€éº¼ï¼Ÿ\n3. ä½ çš„å®¶åº­æˆ–é‡è¦äººéš›é—œä¿‚ï¼Œèƒ½ç‚ºä½ æä¾›å“ªäº›æ”¯æŒï¼Ÿï¼ˆæƒ…æ„Ÿã€ä¿¡æ¯ã€è³‡æºç­‰ï¼‰"
    },
    3: {
        "title": "éšæ®µä¸‰ï¼šæˆ‘è¢«ä»€éº¼æ‰€å½±éŸ¿ï¼Ÿ",
        "prompt": "æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘æ¢è¨ä¸€äº›éœ€è¦æŒçºŒâ€œè¦ºå¯Ÿâ€çš„å› ç´ ã€‚å®ƒå€‘åƒâ€œèƒŒæ™¯éŸ³â€ï¼Œæ·±åˆ»ä½†ä¸æ˜“å¯Ÿè¦ºåœ°å½±éŸ¿ç€ä½ çš„æ±ºç­–ã€‚\n\nè«‹å˜—è©¦æè¿°ï¼š\n1. ä½ å°â€œç†æƒ³å·¥ä½œâ€çš„ç•«åƒï¼Œä¸»è¦å—åˆ°äº†å“ªäº›äºº/ä¿¡æ¯æºçš„å½±éŸ¿ï¼Ÿ\n2. ç•¶ä½ æš¢æƒ³æœªä¾†æ™‚ï¼Œå…§å¿ƒæœ€æ·±è™•çš„ææ‡¼æˆ–æ“”æ†‚æ˜¯ä»€éº¼ï¼Ÿ\n3. åœ¨åšé¸æ“‡æ™‚ï¼Œä½ æ›´å‚¾å‘äºè¦é¿é¢¨éšªï¼Œé‚„æ˜¯è¿½æ±‚å¯èƒ½æ€§ï¼Ÿ"
    },
    4: {
        "title": "éšæ®µå››ï¼šæ ¸å¿ƒä¸‰è§’é—œä¿‚æ•´åˆèˆ‡æ±ºç­–æ¨¡æ“¬",
        "prompt": "éå¸¸æ£’çš„æ·±å…¥æ€è€ƒï¼ç¾åœ¨ï¼Œæˆ‘å€‘å°‡â€œæˆ‘æ˜¯èª°â€ã€â€œæˆ‘æœ‰ä»€ä¹ˆâ€ã€â€œæˆ‘å—ä½•å½±éŸ¿â€é€™ä¸‰å€‹æ ¸å¿ƒé€²è¡Œæ•´åˆã€‚\n\nè«‹å˜—è©¦å®Œæˆä¸€å€‹æ±ºç­–æ¨¡æ“¬ï¼š\n1. åŸºæ–¼å‰ä¸‰éƒ¨åˆ†çš„æ€è€ƒï¼Œè«‹ä½ æ§‹æ€å‡º1-2å€‹ä½ èªç‚ºâ€œä¼¼ä¹å¯è¡Œâ€çš„è·æ¥­ç™¼å±•æ–¹å‘ã€‚\n2. æƒ³åƒä½ é¸æ“‡äº†å…¶ä¸­ä¸€å€‹æ–¹å‘ï¼Œä½ é è¦‹åˆ°æœ€å¤§çš„æŒ‘æˆ°æˆ–å›°é›£æ˜¯ä»€éº¼ï¼Ÿ\n3. ç‚ºäº†æ‡‰å°é€™å€‹æŒ‘æˆ°ï¼Œä½ ç¾åœ¨æœ€éœ€è¦å­¸ç¿’æˆ–æå‡çš„æ ¸å¿ƒèƒ½åŠ›æ˜¯ä»€éº¼ï¼Ÿ"
    },
    5: {
        "title": "éšæ®µäº”ï¼šç¸½çµèˆ‡è¡Œå‹•",
        "prompt": "æˆ‘å€‘çš„æ¢è¨å³å°‡çµæŸã€‚æœ€å¾Œä¸€æ­¥ï¼Œæ˜¯â€œå¦‚ä½•åšåˆ°å …å®šè€Œéˆæ´»â€ã€‚\n\nè«‹å›ç­”æœ€å¾Œä¸€å€‹å•é¡Œï¼Œå°‡æ€è€ƒè½‰åŒ–ç‚ºè¡Œå‹•ï¼š\n\n1. ç‚ºäº†é©—è­‰æˆ–æ¨é€²ä½ åœ¨ç¬¬å››éšæ®µæ§‹æ€çš„æ–¹å‘ï¼Œä½ ä¸‹å‘¨å¯ä»¥å®Œæˆçš„ç¬¬ä¸€å€‹æœ€å°å¯è¡Œæ€§å‹•ä½œæ˜¯ä»€éº¼ï¼Ÿï¼ˆä¾‹å¦‚ï¼šå’Œä¸€ä½å‰è¼©äº¤æµã€çœ‹ä¸€æœ¬æ›¸ã€å­¸ç¿’ä¸€é–€èª²ç¨‹çš„ç¬¬ä¸€ç¯€ç­‰ï¼‰"
    },
}

# --- LLM åˆå§‹åŒ– ---
@st.cache_resource
def get_llm_instance():
    """åˆå§‹åŒ–ä¸¦è¿”å› LLM å¯¦ä¾‹ï¼Œè™•ç†æœ¬åœ°å’Œéƒ¨ç½²ç’°å¢ƒã€‚"""
    api_key = None
    key_name = "DEEPSEEK_API_KEY"
    try:
        api_key = st.secrets[key_name]
    except (KeyError, FileNotFoundError):
        api_key = os.getenv(key_name)

    if not api_key:
        st.error(f"éŒ¯èª¤ï¼šæœªæ‰¾åˆ° {key_name}ã€‚è«‹åœ¨ Streamlit Cloud Secrets æˆ–æœ¬åœ° .env æª”æ¡ˆä¸­è¨­å®šå®ƒã€‚")
        st.info(
            f"è«‹æ³¨æ„ï¼šæ‚¨ä½¿ç”¨çš„æ˜¯ç«å±±å¼•æ“æ–¹èˆŸå¹³å°ï¼Œå› æ­¤é€™è£¡éœ€è¦å¡«å…¥çš„æ˜¯æ‚¨åœ¨ç«å±±å¼•æ“å¹³å°ç²å–çš„ API Keyã€‚")
        return None

    try:
        # ä½¿ç”¨è€…æŒ‡å®šçš„ç«å±±å¼•æ“ç«¯é»å’Œæ¨¡å‹
        llm = ChatOpenAI(
            model="deepseek-r1-250528",
            temperature=0.7,
            api_key=api_key,
            base_url="https://ark.cn-beijing.volces.com/api/v3"
        )
        llm.invoke("Hello")
        return llm
    except Exception as e:
        st.error(f"åˆå§‹åŒ–æ¨¡å‹æ™‚å‡ºéŒ¯: {e}")
        return None

# --- æœƒè©±ç‹€æ…‹ç®¡ç† ---
if "current_mode" not in st.session_state:
    st.session_state.current_mode = "menu"
if "exploration_stage" not in st.session_state:
    st.session_state.exploration_stage = 1
if "chat_history" not in st.session_state:
    st.session_state.chat_history = {}
if 'sim_started' not in st.session_state:
    st.session_state.sim_started = False
if 'report_generated' not in st.session_state:
    st.session_state.report_generated = False

def get_session_history(session_id: str) -> ChatMessageHistory:
    """ç‚ºçµ¦å®šçš„ session ID æª¢ç´¢æˆ–å‰µå»ºèŠå¤©æ­·å²ã€‚"""
    if session_id not in st.session_state.chat_history:
        st.session_state.chat_history[session_id] = ChatMessageHistory()
    return st.session_state.chat_history[session_id]

# --- å„æ¨¡å¼çš„ UI æ¸²æŸ“å‡½æ•¸ ---
def render_menu():
    """æ¸²æŸ“ä¸»èœå–® UIã€‚"""
    st.title("ğŸ’¡ æ™ºæ…§åŒ–è·æ¥­ç™¼å±•è¼”å°ç³»çµ± V4.0")
    st.markdown("---")
    st.subheader("è«‹é¸æ“‡éœ€è¦ä½¿ç”¨çš„åŠŸèƒ½æ¨¡å¼ï¼š")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ§­ è·æ¥­ç›®æ¨™æ¢ç´¢", use_container_width=True):
            st.session_state.current_mode = "exploration"
            st.session_state.exploration_stage = 1
            st.session_state.chat_history['exploration_session'] = ChatMessageHistory()
            st.session_state.report_generated = False
            st.rerun()
        if st.button("ğŸ¤” å®¶åº­æºé€šæ¨¡æ“¬", use_container_width=True):
            st.session_state.current_mode = "communication"
            st.session_state.sim_started = False
            st.session_state.chat_history['communication_session'] = ChatMessageHistory()
            st.rerun()
    with col2:
        if st.button("âš–ï¸ Offeræ±ºç­–åˆ†æ", use_container_width=True):
            st.session_state.current_mode = "decision"
            st.rerun()
        if st.button("ğŸ¢ ä¼æ¥­è³‡è¨Šé€Ÿè¦½", use_container_width=True):
            st.session_state.current_mode = "company_info"
            st.rerun()

def render_exploration_mode(llm):
    """æ¸²æŸ“è·æ¥­ç›®æ¨™æ¢ç´¢æ¨¡å¼çš„ UI å’Œé‚è¼¯ã€‚"""
    st.header("ğŸ§­ æ¨¡å¼ä¸€: è·æ¥­ç›®æ¨™æ¢ç´¢")

    stage = st.session_state.exploration_stage
    history = get_session_history("exploration_session")

    for msg in history.messages:
        avatar = "ğŸ§‘â€ğŸ’»" if msg.type == "human" else "ğŸ¤–"
        with st.chat_message(msg.type, avatar=avatar):
            st.markdown(msg.content)

    if stage > 5:
        st.success("æ‚¨å·²å®Œæˆæ‰€æœ‰éšæ®µçš„æ¢ç´¢ï¼ç¾åœ¨ï¼Œæˆ‘å¯ä»¥ç‚ºæ‚¨ç”Ÿæˆä¸€ä»½ç¶œåˆå ±å‘Šã€‚")
        if not st.session_state.report_generated:
            if st.button("âœ¨ ç”Ÿæˆæˆ‘çš„è·æ¥­æ¢ç´¢å ±å‘Š"):
                with st.spinner("AIæ­£åœ¨å…¨é¢åˆ†ææ‚¨çš„å›ç­”ï¼Œç”Ÿæˆå°ˆå±¬å ±å‘Š..."):
                    full_conversation = "\n".join(
                        [f"{'ç”¨æˆ¶' if isinstance(msg, HumanMessage) else 'AIæ•™ç·´'}: {msg.content}" for msg in
                         history.messages])

                    report_prompt_template = ChatPromptTemplate.from_template(GLOBAL_PERSONA + """
                    ä½œç‚ºä¸€åè³‡æ·±çš„è·æ¥­ç™¼å±•æ•™ç·´ï¼Œè«‹æ ¹æ“šä»¥ä¸‹ç”¨æˆ¶èˆ‡AIæ•™ç·´çš„å®Œæ•´å°è©±è¨˜éŒ„ï¼Œç‚ºç”¨æˆ¶æ’°å¯«ä¸€ä»½å…¨é¢ã€æ·±åˆ»ä¸”å¯Œæœ‰å•Ÿç™¼æ€§çš„è·æ¥­æ¢ç´¢ç¸½çµå ±å‘Šã€‚
                    å ±å‘Šéœ€è¦éµå¾ªä»¥ä¸‹çµæ§‹ï¼Œä¸¦ä½¿ç”¨æ¸…æ™°çš„Markdownæ ¼å¼ï¼š
                    ### 1. æ ¸å¿ƒè‡ªæˆ‘èªçŸ¥ï¼ˆæˆ‘æ˜¯èª°ï¼Ÿï¼‰
                    - ç¸½çµç”¨æˆ¶å°è‡ªå·±å°ˆæ¥­èˆˆè¶£ã€æ ¸å¿ƒèƒ½åŠ›å’Œè·æ¥­åƒ¹å€¼è§€çš„èªçŸ¥ã€‚æç…‰å‡ºç”¨æˆ¶æœ€é—œéµçš„å€‹äººç‰¹è³ªå’Œå…§åœ¨é©…å‹•åŠ›ã€‚
                    ### 2. å¤–éƒ¨è³‡æºè©•ä¼°ï¼ˆæˆ‘æœ‰ä»€ä¹ˆï¼Ÿï¼‰
                    - ç¸½çµç”¨æˆ¶æ‰€æ“æœ‰çš„å¹³å°å„ªå‹¢ã€å¤–éƒ¨æ©Ÿæœƒå’Œäººéš›æ”¯æŒç¶²çµ¡ã€‚åˆ†æé€™äº›è³‡æºå¦‚ä½•ç‚ºç”¨æˆ¶çš„è·æ¥­ç™¼å±•æä¾›å¯èƒ½æ€§ã€‚
                    ### 3. å…§åœ¨å½±éŸ¿å› ç´ æ´å¯Ÿï¼ˆæˆ‘å—ä½•å½±éŸ¿ï¼Ÿï¼‰
                    - ç¸½çµå½±éŸ¿ç”¨æˆ¶æ±ºç­–çš„æ·±å±¤å› ç´ ï¼ŒåŒ…æ‹¬ä»–äººçš„å½±éŸ¿ã€å…§å¿ƒçš„æ“”æ†‚ä»¥åŠé¢¨éšªåå¥½ã€‚é»å‡ºç”¨æˆ¶åœ¨åšé¸æ“‡æ™‚å¯èƒ½å­˜åœ¨çš„æ€ç¶­æ…£æ€§æˆ–ç›²é»ã€‚
                    ### 4. æ•´åˆæ–¹å‘èˆ‡æ½›åœ¨æŒ‘æˆ°ï¼ˆæˆ‘çš„æ–¹å‘ï¼Ÿï¼‰
                    - ç¸½çµç”¨æˆ¶åˆæ­¥æ§‹æƒ³çš„1-2å€‹è·æ¥­æ–¹å‘ã€‚åŸºæ–¼å‰é¢çš„åˆ†æï¼Œè©•ä¼°é€™äº›æ–¹å‘çš„åˆç†æ€§ï¼Œä¸¦æŒ‡å‡ºç”¨æˆ¶é è¦‹åˆ°çš„ä¸»è¦æŒ‘æˆ°ã€‚
                    ### 5. ä¸‹ä¸€æ­¥è¡Œå‹•è¨ˆåŠƒï¼ˆæˆ‘åšä»€ä¹ˆï¼Ÿï¼‰
                    - æ˜ç¢ºæŒ‡å‡ºç”¨æˆ¶ç‚ºè‡ªå·±è¨­å®šçš„ã€å¯ç«‹å³åŸ·è¡Œçš„æœ€å°è¡Œå‹•æ­¥é©Ÿã€‚å°é€™å€‹è¡Œå‹•è¨ˆåŠƒçš„å¯è¡Œæ€§çµ¦äºˆé¼“å‹µå’Œè‚¯å®šã€‚
                    ### 6. ç¶œåˆå»ºè­°
                    - åŸºæ–¼æ•´é«”å°è©±ï¼Œæä¾›1-2æ¢æ ¸å¿ƒå»ºè­°ï¼Œé¼“å‹µç”¨æˆ¶ç¹¼çºŒæ¢ç´¢ï¼Œä¸¦æé†’ä»–å€‘é—œæ³¨çš„é—œéµé»ã€‚çµå°¾æ‡‰ç©æ¥µã€é¼“èˆäººå¿ƒï¼Œå¼·èª¿è·æ¥­æ¢ç´¢æ˜¯ä¸€å€‹æŒçºŒçš„éç¨‹ã€‚
                    ---
                    ä»¥ä¸‹æ˜¯å®Œæ•´çš„å°è©±è¨˜éŒ„: {conversation_history}
                    ---
                    """)
                    report_chain = report_prompt_template | llm
                    report_response = report_chain.invoke({"conversation_history": full_conversation})
                    st.session_state.generated_report = report_response.content
                    st.session_state.report_generated = True
                    st.rerun()

        if st.session_state.get('report_generated'):
            st.markdown("---")
            st.subheader("ğŸ“„ æ‚¨çš„å€‹äººè·æ¥­æ¢ç´¢å ±å‘Š")
            st.markdown(st.session_state.generated_report)
            st.info("å¸Œæœ›é€™ä»½å ±å‘Šèƒ½ç‚ºæ‚¨å¸¶ä¾†æ–°çš„å•Ÿç™¼ã€‚æ‚¨å¯ä»¥è¤‡è£½ã€ä¿å­˜é€™ä»½å ±å‘Šï¼Œä½œç‚ºæœªä¾†æ±ºç­–çš„åƒè€ƒã€‚")
    else:
        st.info("æ­¤æ¨¡å¼å°‡é€šéäº”å€‹éšæ®µï¼Œå¼•å°æ‚¨æ·±å…¥æ¢ç´¢è·æ¥­ç›®æ¨™ã€‚")
        current_prompt_info = EXPLORATION_PROMPTS.get(stage)
        st.subheader(current_prompt_info["title"])

        if len(history.messages) == 0 or history.messages[-1].type == "ai":
             with st.chat_message("ai", avatar="ğŸ¤–"):
                st.markdown(current_prompt_info["prompt"])

        # [KeyError ä¿®æ­£] ç§»é™¤ system prompt ä¸­ç„¡æ•ˆçš„ {interest} å’Œ {skill} é ç•™ä½ç½®
        meta_prompt = ChatPromptTemplate.from_messages([
            ("system", GLOBAL_PERSONA + """
            You are a thoughtful and insightful career planning coach. You are currently in Stage {current_stage} of a five-stage framework.
            Your goal is to help the user think more deeply about their answers.
            After the user answers the questions for a stage, your task is to:
            1. Acknowledge their response.
            2. Provide a brief (2-3 sentences), insightful comment or a thought-provoking follow-up question. You must act as a suggestion provider, not just a data collector.
            3. [V4.0 Optimization]: For Stage 1, if the user mentions specific interests and skills, try to connect them. For example, you could say something like: "It's great that your interest in [user's interest] aligns with your skill in [user's skill]. Have you considered how this combination could translate into a specific role?"
            4. [V4.0 Edge Case Handling]: If the user's answer is very vague (e.g., "I don't know", "whatever"), switch to a more guiding question. For example: "That's perfectly fine, many people feel lost at first. Let's try another angle: has there been anything recently that gave you a special sense of accomplishment?"
            5. The program will automatically move to the next stage, so you don't need to say "let's move on". Your response should add value and encourage deeper reflection.
            """),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])
        chain = meta_prompt | llm
        chain_with_history = RunnableWithMessageHistory(
            chain,
            lambda session_id: get_session_history(session_id),
            input_messages_key="input",
            history_messages_key="history",
        )

        if user_input := st.chat_input("ä½ çš„å›ç­”:"):
            with st.spinner("AIæ­£åœ¨åˆ†ææ‚¨çš„å›ç­”ä¸¦æä¾›å»ºè­°..."):
                chain_with_history.invoke(
                    {"input": user_input, "current_stage": stage},
                    config={"configurable": {"session_id": "exploration_session"}}
                )
                st.session_state.exploration_stage += 1
                st.rerun()

def render_decision_mode(llm):
    """æ¸²æŸ“ Offer æ±ºç­–åˆ†ææ¨¡å¼çš„ UI å’Œé‚è¼¯ã€‚"""
    st.header("âš–ï¸ æ¨¡å¼äºŒ: Offeræ±ºç­–åˆ†æ")
    st.info("æ­¤æ¨¡å¼é€šéâ€œåˆ†å±¤ä¿¡æ¯æ”¶é›†â€å’Œâ€œå€‹æ€§åŒ–åˆ†æâ€ï¼Œå¹«åŠ©æ‚¨åšå‡ºæ›´è²¼åˆè‡ªèº«éœ€æ±‚çš„æ±ºç­–ã€‚")

    meta_prompt = ChatPromptTemplate.from_template(GLOBAL_PERSONA + """
You are an expert career advisor. Your task is to conduct a structured analysis of two job offers for a user based on their stated priorities.
Offer A Details: {offer_a_details}
Offer B Details: {offer_b_details}
User Priorities (sorted list): {user_priorities_sorted_list}

Please perform the following steps and structure your entire response in clear, easy-to-read markdown:

1.  **æ©«å‘å°æ¯”è¡¨ (Comparison Table):** ç”Ÿæˆä¸€å€‹æ¸…æ™°çš„è¡¨æ ¼ï¼Œæ©«å‘å°æ¯”å…©å€‹Offerã€‚å°æ¯”ç¶­åº¦æ‡‰è‡³å°‘åŒ…æ‹¬ï¼šå…¬å¸ã€è·ä½ã€è–ªé…¬ç¦åˆ©ã€åœ°é»ã€è·æ¥­æˆé•·æ½›åŠ›ã€å·¥ä½œç”Ÿæ´»å¹³è¡¡ã€‚

2.  **å€‹æ€§åŒ–å„ªå…ˆç´šåŒ¹é…åˆ†æ (Personalized Priority Matching Analysis):** (é€™æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†) æ ¹æ“šç”¨æˆ¶çµ¦å‡ºçš„å„ªå…ˆç´šåˆ—è¡¨ï¼Œé€ä¸€åˆ†æå’Œè©•åƒ¹æ¯å€‹Offerèˆ‡ä»–å€‘åƒ¹å€¼è§€çš„åŒ¹é…åº¦ã€‚ä¾‹å¦‚ï¼š"æ‚¨å°‡'è·æ¥­æˆé•·'æ”¾åœ¨é¦–ä½ï¼ŒOffer Aæ¸…æ™°çš„æ™‰å‡è·¯å¾‘åœ¨é€™ä¸€é»ä¸Šå¾—åˆ†è¼ƒé«˜ï¼›è€ŒOffer Bé›–ç„¶èµ·è–ªæ›´é«˜ï¼Œä½†åœ¨æˆé•·ç©ºé–“ä¸Šç›¸å°æ¨¡ç³Šã€‚"

3.  **å„ªåŠ£å‹¢åˆ†æ (Pros and Cons Analysis):** åŸºæ–¼ç”¨æˆ¶è¼¸å…¥å’Œé€šç”¨è·æ¥­çŸ¥è­˜ï¼Œç‚ºæ¯å€‹Offeråˆ†åˆ¥åˆ—å‡ºå…¶ä¸»è¦å„ªé»(Pros)å’Œç¼ºé»(Cons)ã€‚

4.  **é¢¨éšªé è­¦èˆ‡æ‡‰å°ç­–ç•¥ (Risk Alert & Mitigation):** æ˜ç¢ºæŒ‡å‡ºé¸æ“‡æ¯å€‹Offerå¯èƒ½é¢è‡¨çš„æ½›åœ¨é¢¨éšªã€‚ä¾‹å¦‚ï¼š"é¢¨éšªæç¤ºï¼šOffer Aæ‰€åœ¨è¡Œæ¥­æ³¢å‹•è¼ƒå¤§ï¼Œå…¬å¸ç©©å®šæ€§å¯èƒ½é¢è‡¨æŒ‘æˆ°ã€‚æ‡‰å°ç­–ç•¥ï¼šå»ºè­°æ‚¨é€²ä¸€æ­¥äº†è§£å…¶èè³‡æƒ…æ³å’Œå¸‚å ´ä»½é¡ã€‚"

5.  **ç¸½çµå»ºè­°èˆ‡é—œéµå•é¡Œ (Recommendation and Key Questions):** æä¾›ä¸€å€‹ç¸½çµæ€§å»ºè­°ã€‚ä¸è¦ç‚ºç”¨æˆ¶åšå‡ºæœ€çµ‚é¸æ“‡ï¼Œè€Œæ˜¯å»ºè­°åœ¨ä¸åŒå„ªå…ˆç´šä¸‹å“ªå€‹Offerå¯èƒ½æ›´åˆé©ã€‚æœ€å¾Œï¼Œæå‡º1-2å€‹é—œéµå•é¡Œï¼Œå¹«åŠ©ç”¨æˆ¶é€²è¡Œæœ€çµ‚çš„è‡ªæˆ‘æ‹·å•ã€‚
""")
    chain = meta_prompt | llm

    st.subheader("ç¬¬ä¸€æ­¥ï¼šè«‹å¡«å¯« Offer çš„æ ¸å¿ƒè³‡è¨Š")
    col1, col2 = st.columns(2)
    with col1:
        offer_a = st.text_area("Offer A é—œéµè³‡è¨Š", height=200, placeholder="ä¾‹å¦‚: å…¬å¸åã€è·ä½ã€è–ªè³‡ã€åœ°é»ã€å„ªé»ã€é¡§æ…®ç­‰")
    with col2:
        offer_b = st.text_area("Offer B é—œéµè³‡è¨Š", height=200, placeholder="åŒæ¨£ï¼ŒåŒ…æ‹¬å…¬å¸åã€è·ä½ã€è–ªè³‡ã€åœ°é»ã€å„ªé»ã€é¡§æ…®ç­‰")

    st.subheader("ç¬¬äºŒæ­¥ï¼š(å¯é¸ï¼Œä½†å¼·çƒˆå»ºè­°)æ·»åŠ ä½ çš„å€‹äººåå¥½")
    st.markdown("ç‚ºäº†è®“åˆ†ææ›´æ‡‚ä½ ï¼Œè«‹å‘Šè¨´æˆ‘å€‘ä½ å°ä»¥ä¸‹å¹¾é»çš„çœ‹é‡ç¨‹åº¦ï¼ˆè«‹æŒ‰é‡è¦æ€§å¾é«˜åˆ°ä½ä¾æ¬¡é»æ“Šé¸æ“‡ï¼‰:")
    priorities_options = ["è·æ¥­æˆé•·", "è–ªè³‡ç¦åˆ©", "å·¥ä½œç”Ÿæ´»å¹³è¡¡", "åœ˜éšŠæ°›åœ", "å…¬å¸ç©©å®šæ€§"]
    user_priorities = st.multiselect(
        "é¸æ“‡ä¸¦æ’åºä½ çš„è·æ¥­åå¥½",
        options=priorities_options,
        help="æ‚¨é¸æ“‡çš„ç¬¬ä¸€å€‹é¸é …ä»£è¡¨æ‚¨æœ€çœ‹é‡çš„å› ç´ ï¼Œä»¥æ­¤é¡æ¨ã€‚"
    )

    if st.button("âœ¨ ç”Ÿæˆå°æ¯”åˆ†æå ±å‘Š", use_container_width=True):
        if not offer_a or not offer_b:
            st.warning("è«‹è¼¸å…¥å…©å€‹Offerçš„è³‡è¨Šå¾Œå†ç”Ÿæˆå ±å‘Šã€‚")
        else:
            with st.spinner("æ­£åœ¨ç‚ºæ‚¨ç”ŸæˆOfferåˆ†æå ±å‘Š..."):
                try:
                    priorities_text = ", ".join(user_priorities) if user_priorities else "ç”¨æˆ¶æœªæŒ‡å®šæ˜ç¢ºçš„å„ªå…ˆç´šé †åº"
                    response = chain.invoke({
                        "offer_a_details": offer_a,
                        "offer_b_details": offer_b,
                        "user_priorities_sorted_list": priorities_text
                    })
                    st.markdown("---")
                    st.subheader("ğŸ“‹ Offerå°æ¯”åˆ†æå ±å‘Š")
                    st.markdown(response.content)
                except Exception as e:
                    st.error(f"ç”Ÿæˆå ±å‘Šæ™‚å‡ºéŒ¯: {e}")

def render_communication_mode(llm):
    """æ¸²æŸ“å®¶åº­æºé€šæ¨¡æ“¬æ¨¡å¼çš„ UI å’Œé‚è¼¯ã€‚"""
    st.header("ğŸ¤” æ¨¡å¼ä¸‰: å®¶åº­æºé€šæ¨¡æ“¬")

    if not st.session_state.sim_started:
        st.info("åœ¨é€™è£¡ï¼ŒAIå¯ä»¥æ‰®æ¼”æ‚¨çš„å®¶äººï¼Œå¹«åŠ©æ‚¨ç·´ç¿’å¦‚ä½•æºé€šè·æ¥­è¦åŠƒï¼Œä¸¦æä¾›è¤‡ç›¤å»ºè­°ã€‚")
        my_choice = st.text_input("é¦–å…ˆ, è«‹å‘Šè¨´æˆ‘ä½ æƒ³è¦å’Œå®¶äººæºé€šçš„è·æ¥­é¸æ“‡æ˜¯ä»€éº¼?")
        family_concern = st.text_area("ä½ èªç‚ºä»–å€‘ä¸»è¦çš„æ“”æ†‚æœƒæ˜¯ä»€éº¼?",
                                      placeholder="ä¾‹å¦‚: å·¥ä½œä¸ç©©å®šã€ä¸æ˜¯éµé£¯ç¢—ã€é›¢å®¶å¤ªé ç­‰")

        if st.button("ğŸ¬ é–‹å§‹æ¨¡æ“¬"):
            if not my_choice or not family_concern:
                st.warning("è«‹è¼¸å…¥æ‚¨çš„è·æ¥­é¸æ“‡å’Œé æƒ³çš„å®¶äººæ“”æ†‚ã€‚")
            else:
                st.session_state.my_choice = my_choice
                st.session_state.family_concern = family_concern
                st.session_state.sim_started = True
                st.session_state.debrief_requested = False

                initial_ai_prompt = f"å­©å­ï¼Œé—œæ–¼ä½ æƒ³åš '{my_choice}' é€™å€‹äº‹ï¼Œæˆ‘æœ‰äº›æ“”å¿ƒã€‚æˆ‘ä¸»è¦æ˜¯è¦ºå¾—å®ƒ '{family_concern}'ã€‚æˆ‘å€‘èƒ½èŠèŠå—ï¼Ÿ"
                get_session_history("communication_session").add_ai_message(initial_ai_prompt)
                st.rerun()

    if st.session_state.get('sim_started'):
        st.success(f"æ¨¡æ“¬é–‹å§‹ï¼AIæ­£åœ¨æ‰®æ¼”æ“”æ†‚æ‚¨é¸æ“‡ â€œ{st.session_state.my_choice}â€ çš„å®¶äººã€‚")

        meta_prompt = ChatPromptTemplate.from_messages([
            ("system", GLOBAL_PERSONA + """
            You are an AI role-playing as a user's parent. The user wants to practice a difficult conversation.
            Your Persona: You are a loving but concerned parent. Your primary concerns stem from what the user described: '{family_concern}'. You want the best for your child, which to you means stability, security, and a respectable career path. You are skeptical of new or unconventional choices like '{my_choice}'.
            Your Task:
            1. Listen to the user's responses and react naturally. If they make a good point, you can be partially convinced but still raise other questions. If they are purely emotional, express your worry more strongly, but in a concerned, not aggressive way.
            2. Your goal is NOT to be convinced easily. The goal is to provide a realistic simulation to help the user practice.
            3. Keep your responses concise and in character.
            4. [V4.0 Safety]: If the user uses aggressive language, respond gently, e.g., "You saying that makes me sad, I'm just worried about you. Can we talk calmly?"
            """),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])
        chain = meta_prompt | llm
        chain_with_history = RunnableWithMessageHistory(
            chain,
            lambda session_id: get_session_history(session_id),
            input_messages_key="input",
            history_messages_key="history",
        )

        history = get_session_history("communication_session")
        for msg in history.messages:
            avatar = "ğŸ§‘â€ğŸ’»" if msg.type == "human" else "ğŸ§“"
            with st.chat_message(msg.type, avatar=avatar):
                st.markdown(msg.content)

        if st.session_state.get('debrief_requested'):
            st.session_state.debrief_requested = False
            with st.spinner("AIæ­£åœ¨è·³å‡ºè§’è‰²ï¼Œç‚ºæ‚¨åˆ†ææºé€šæŠ€å·§..."):
                full_conversation = "\n".join([f"{'ä½ ' if isinstance(msg, HumanMessage) else 'â€œå®¶äººâ€'}: {msg.content}" for msg in history.messages])
                debrief_prompt = ChatPromptTemplate.from_template(GLOBAL_PERSONA + """
                You are a communication coach. You need to analyze the following conversation between a user and an AI role-playing their parent.
                Your task is to provide a brief, actionable debrief.
                1. Identify one "æºé€šäº®é»" (Communication Highlight) where the user communicated effectively.
                2. Identify one "å¯æ”¹é€²é»" (Area for Improvement).
                3. Suggest one "ä¸‹æ¬¡å¯ä»¥å˜—è©¦çš„æºé€šç­–ç•¥" (Strategy to Try Next Time).
                Keep the feedback encouraging and constructive.
                Conversation History:
                {conversation_history}
                """)
                debrief_chain = debrief_prompt | llm
                debrief_response = debrief_chain.invoke({"conversation_history": full_conversation})
                st.info("ğŸ’¡ **æºé€šæŠ€å·§æç¤º**\n\n" + debrief_response.content)

        col1, col2 = st.columns([4, 1])
        with col1:
             user_input = st.chat_input("ä½ çš„å›æ‡‰:")
        with col2:
            if st.button("è«‹æ±‚æç¤º", help="è®“AIè·³å‡ºè§’è‰²ï¼Œçµ¦äºˆæºé€šæŠ€å·§å»ºè­°"):
                st.session_state.debrief_requested = True
                st.rerun()

        if user_input:
            with st.spinner("..."):
                chain_with_history.invoke(
                    {"input": user_input, "my_choice": st.session_state.my_choice,
                     "family_concern": st.session_state.family_concern},
                    config={"configurable": {"session_id": "communication_session"}}
                )
                st.rerun()

def render_company_info_mode(llm):
    """æ¸²æŸ“ä¼æ¥­è³‡è¨Šé€Ÿè¦½æ¨¡å¼çš„ UI å’Œé‚è¼¯ã€‚"""
    st.header("ğŸ¢ æ¨¡å¼å››: ä¼æ¥­è³‡è¨Šé€Ÿè¦½")
    st.info("è«‹è¼¸å…¥å…¬å¸å…¨åï¼ŒAIå°‡æ¨¡æ“¬ç¶²è·¯æŠ“å–ä¸¦ç‚ºæ‚¨ç”Ÿæˆä¸€ä»½æ ¸å¿ƒè³‡è¨Šé€Ÿè¦½å ±å‘Šã€‚")

    meta_prompt = ChatPromptTemplate.from_template(GLOBAL_PERSONA + """
You are a professional business analyst AI. Your task is to generate a concise, structured summary of a company based on its name.
Company Name: {company_name}

Simulate that you have scraped the company's official website, recent news, and recruitment portals. Generate a report in clear markdown format that includes the following sections:

1.  **å…¬å¸ç°¡ä»‹ (Company Profile):** A brief overview of the company, its mission, and its industry positioning.
2.  **æ ¸å¿ƒç”¢å“/æ¥­å‹™ (Core Products/Business):** A list or description of its main products, services, or business units.
3.  **è¿‘æœŸå‹•æ…‹ (Recent Developments):** Summarize 2-3 recent significant news items, product launches, or strategic shifts.
4.  **ç†±æ‹›å´—ä½æ–¹å‘ (Hot Recruitment Areas):** Based on simulated recruitment data, list 3-5 key types of positions the company is likely hiring for (e.g., "å¾Œç«¯é–‹ç™¼å·¥ç¨‹å¸«", "ç”¢å“ç¶“ç†-AIæ–¹å‘", "å¸‚å ´è¡ŒéŠ·å°ˆå“¡").
5.  **é¢è©¦å¯èƒ½é—œæ³¨é» (Potential Interview Focus):** Based on the company's mission and recent news, infer 2-3 potential themes or skills they might value in interviews. (e.g., "é‘‘äºå…¶æœ€è¿‘ç™¼å¸ƒäº†AIç”¢å“, é¢è©¦ä¸­å¯èƒ½æœƒé—œæ³¨å€™é¸äººå°AIGCçš„ç†è§£ã€‚")
6.  **æ•¸æ“šä¾†æºèˆ‡æ™‚æ•ˆæ€§è²æ˜ (Data Source & Timeliness Disclaimer):** At the end of the report, add this mandatory footer: "æ³¨æ„: æœ¬å ±å‘Šè³‡è¨ŠåŸºæ–¼æ¨¡æ“¬çš„å…¬é–‹æ•¸æ“šæŠ“å–(æˆªè‡³2025å¹´6æœˆ), åƒ…ä¾›åƒè€ƒã€‚å»ºè­°æ‚¨ä»¥å®˜æ–¹æ¸ é“ç™¼å¸ƒçš„æœ€æ–°è³‡è¨Šç‚ºæº–ã€‚"

The information should be plausible and well-structured. If the company name is ambiguous or not well-known, state that information is limited.
""")
    chain = meta_prompt | llm

    company_name = st.text_input("è«‹è¼¸å…¥å…¬å¸åç¨±:", placeholder="ä¾‹å¦‚ï¼šé˜¿é‡Œå·´å·´ã€é¨°è¨Šã€å­—ç¯€è·³å‹•")

    if st.button("ğŸ” ç”Ÿæˆé€Ÿè¦½å ±å‘Š", use_container_width=True):
        if not company_name:
            st.warning("è«‹è¼¸å…¥å…¬å¸åç¨±ã€‚")
        else:
            with st.spinner(f"æ­£åœ¨ç”Ÿæˆé—œæ–¼ â€œ{company_name}â€ çš„è³‡è¨Šå ±å‘Š..."):
                try:
                    response = chain.invoke({"company_name": company_name})
                    st.markdown("---")
                    st.subheader(f"ğŸ“„ {company_name} - æ ¸å¿ƒè³‡è¨Šé€Ÿè¦½")
                    st.markdown(response.content)
                except Exception as e:
                    st.error(f"ç”Ÿæˆå ±å‘Šæ™‚å‡ºéŒ¯: {e}")

# --- ä¸»æ‡‰ç”¨é‚è¼¯ ---
def main():
    """ä¸»å‡½æ•¸ï¼Œé‹è¡Œ Streamlit æ‡‰ç”¨ã€‚"""
    llm = get_llm_instance()
    if not llm:
        st.stop()

    with st.sidebar:
        st.title("å°èˆª")
        if st.session_state.current_mode != "menu":
            if st.button("è¿”å›ä¸»èœå–®"):
                # ä¸€å€‹æ›´ç©©å¥çš„é‡ç½®æ–¹æ³•
                for key in list(st.session_state.keys()):
                    if key not in ['current_mode']:
                        del st.session_state[key]
                st.session_state.current_mode = "menu"
                st.rerun()

    modes = {
        "menu": render_menu,
        "exploration": lambda: render_exploration_mode(llm),
        "decision": lambda: render_decision_mode(llm),
        "communication": lambda: render_communication_mode(llm),
        "company_info": lambda: render_company_info_mode(llm),
    }
    modes[st.session_state.current_mode]()

if __name__ == "__main__":
    main()