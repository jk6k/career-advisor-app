import os
import re
import streamlit as st
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, AIMessage
from streamlit_mermaid import st_mermaid
from PIL import Image
import platform

# --- é é¢è¨­å®š (å¿…é ˆæ˜¯ç¬¬ä¸€å€‹ Streamlit å‘½ä»¤) ---
st.set_page_config(
    page_title="æ™ºæ…§åŒ–èŒä¸šå‘å±•è¾…å¯¼ç³»ç»Ÿ",
    page_icon="âœ¨",
    layout="wide"
)

# --- UI ç¾åŒ– CSS æ¨£å¼ ---
st.markdown("""
<style>
    /* æ˜ç¢ºåŒ¯å…¥æ‰€éœ€çš„æ–‡å­—å­—å‹å’Œåœ–ç¤ºå­—å‹ */
    @import url('https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200');
    @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;500;700&display=swap');

    html, body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Noto Sans SC", "Helvetica Neue", "PingFang SC", "Microsoft YaHei", sans-serif;
        line-height: 1.65;
        background-color: #F8F9FA;
        color: #495057;
    }

    h1, h2, h3, h4, h5, h6 { color: #212529; font-weight: 700; }
    h1 { font-size: 32px; }
    h2 { font-size: 28px; border-bottom: 2px solid #E9ECEF; padding-bottom: 0.4em; }
    h3 { font-size: 22px; }

    .st-emotion-cache-z5fcl4 {
        display: flex;
        flex-direction: column;
        height: 100%;
    }

    .st-emotion-cache-z5fcl4 .stButton {
        margin-top: auto;
    }

    .st-emotion-cache-z5fcl4 {
        background-color: #FFFFFF;
        border-radius: 12px;
        padding: 28px;
        box-shadow: 0 4px 12px rgba(0,0,0,0.05);
        border: 1px solid #E9ECEF;
        transition: transform 0.3s ease, box-shadow 0.3s ease;
    }
    .st-emotion-cache-z5fcl4:hover {
        transform: translateY(-5px);
        box-shadow: 0 12px 24px rgba(0,0,0,0.08);
    }

    .stButton>button {
        border-radius: 8px; border: none; color: white; font-weight: 500;
        padding: 12px 24px; background-image: linear-gradient(135deg, #5D9CEC 0%, #4A90E2 100%);
        transition: all 0.3s ease; box-shadow: 0 4px 15px rgba(74, 144, 226, 0.2);
    }
    .stButton>button:hover { transform: translateY(-2px); box-shadow: 0 6px 20px rgba(74, 144, 226, 0.3); }

    .stChatMessage { border-radius: 12px; border: 1px solid #E9ECEF; background-color: #FFFFFF; padding: 16px; margin-bottom: 1rem; }
    .st-emotion-cache-T21nqy { background-color: #e3eeff; border-color: #a4c7ff; }

    .stChatInputContainer {
        position: sticky; bottom: 0; background-color: #FFFFFF;
        padding: 12px 0px; border-top: 1px solid #E9ECEF;
        box-shadow: 0 -4px 12px rgba(0,0,0,0.05);
    }
</style>
""", unsafe_allow_html=True)

# --- OCRè·¯å¾‘è¨­å®š (æ ¹æ“šæ‚¨çš„å¯¦éš›å®‰è£è·¯å¾‘ä¿®æ”¹) ---
try:
    import pytesseract

    if platform.system() == "Windows":
        pytesseract.pytesseract.tesseract_cmd = r'E:\Tesseract at UB Mannheim\tesseract.exe'
except ImportError:
    pass

# --- åˆå§‹åŒ– ---
load_dotenv()
os.environ["LANGCHAIN_TRACING_V2"] = "false"

# --- å…¨åŸŸç³»çµ±è§’è‰² (ç°¡é«”ä¸­æ–‡) ---
GLOBAL_PERSONA = "æ ¸å¿ƒè§’è‰²: ä½ æ˜¯ä¸€ä½æ™ºæ…§ã€ä¸“ä¸šä¸”å¯Œæœ‰åŒç†å¿ƒçš„èŒä¸šå‘å±•æ•™ç»ƒä¸æˆ˜ç•¥è§„åˆ’å¸ˆã€‚\nè¯­è¨€è¦æ±‚: ä½ çš„æ‰€æœ‰å›ç­”éƒ½å¿…é¡»ä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚"


# --- LLM åˆå§‹åŒ– ---
@st.cache_resource
def get_llm_instance():
    api_key = None;
    key_name = "VOLCENGINE_API_KEY"
    try:
        api_key = st.secrets[key_name]
    except (KeyError, FileNotFoundError):
        api_key = os.getenv(key_name)
    if not api_key:
        st.error(f"é”™è¯¯ï¼šæœªæ‰¾åˆ° {key_name}ã€‚è¯·åœ¨ Streamlit Cloud Secrets æˆ–æœ¬åœ° .env æ–‡ä»¶ä¸­è®¾ç½®å®ƒã€‚");
        return None
    try:
        llm = ChatOpenAI(model="deepseek-r1-250528", temperature=0.7, api_key=api_key,
                         base_url="https://ark.cn-beijing.volces.com/api/v3")
        llm.invoke("Hello");
        return llm
    except Exception as e:
        st.error(f"åˆå§‹åŒ–æ¨¡å‹æ—¶å‡ºé”™: {e}");
        return None


# --- æœƒè©±ç‹€æ…‹ç®¡ç† ---
def init_session_state():
    defaults = {"current_mode": "menu", "chat_history": {}, "exploration_stage": 1, "sim_started": False,
                "debrief_requested": False, "panoramic_stage": 1, "user_profile": None, "chosen_professions": None,
                "chosen_region": None, "curriculum_stage": 1, "curriculum_content": None, "chosen_career": None,
                "key_courses_identified": None}
    for key, value in defaults.items():
        if key not in st.session_state: st.session_state[key] = value


init_session_state()


def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in st.session_state.chat_history: st.session_state.chat_history[session_id] = ChatMessageHistory()
    return st.session_state.chat_history[session_id]


# --- UI æ¸²æŸ“å‡½å¼ ---
def render_menu():
    st.title("âœ¨ æ™ºæ…§åŒ–èŒä¸šå‘å±•è¾…å¯¼ç³»ç»Ÿ")
    st.markdown("---")
    st.subheader("æ¬¢è¿ä½¿ç”¨ï¼è¯·é€‰æ‹©ä¸€é¡¹åŠŸèƒ½å¼€å§‹æ¢ç´¢ï¼š")
    st.write("")
    modes_config = [
        ("exploration", ":compass: èŒä¸šç›®æ ‡æ¢ç´¢", "é€šè¿‡â€œæˆ‘-ç¤¾ä¼š-å®¶åº­â€æ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°æ¢ç´¢å†…åœ¨åŠ¨æœºä¸å¤–åœ¨æœºä¼šã€‚"),
        ("panoramic", ":globe_with_meridians: èŒä¸šè·¯å¾„å…¨æ™¯è§„åˆ’",
         "ä»æ‚¨çš„æ ¸å¿ƒèƒ½åŠ›å‡ºå‘ï¼Œè¿æ¥èŒä¸šã€ä¼ä¸šã€åœ°åŒºä¸äº§ä¸šé“¾ï¼Œç”Ÿæˆæ‚¨çš„ä¸ªäººå‘å±•è“å›¾ã€‚"),
        ("decision", ":balance_scale: Offer å†³ç­–åˆ†æ", "ç»“æ„åŒ–å¯¹æ¯”å¤šä¸ªOfferï¼Œè·å¾—æ¸…æ™°çš„å†³ç­–å»ºè®®ã€‚"),
        ("company_info", ":office: ä¼ä¸šä¿¡æ¯é€Ÿè§ˆ", "å¿«é€Ÿäº†è§£ç›®æ ‡å…¬å¸çš„æ ¸å¿ƒä¸šåŠ¡ã€è¿‘æœŸåŠ¨æ€ä¸çƒ­æ‹›æ–¹å‘ã€‚"),
        ("communication", ":family: å®¶åº­æ²Ÿé€šæ¨¡æ‹Ÿ", "ä¸AIæ‰®æ¼”çš„å®¶äººè¿›è¡Œå¯¹è¯ï¼Œå®‰å…¨åœ°ç»ƒä¹ å¦‚ä½•è¡¨è¾¾æ‚¨çš„èŒä¸šé€‰æ‹©ã€‚"),
        ("curriculum_analysis", ":school: ä¸“ä¸šåŸ¹å…»æ–¹æ¡ˆè§£æ",
         "ä¸Šä¼ æ‚¨çš„ä¸“ä¸šåŸ¹å…»æ–¹æ¡ˆï¼ŒAIå°†ä¸ºæ‚¨è§£æè¯¾ç¨‹ä½“ç³»ï¼Œå¹¶è§„åˆ’é‡ç‚¹å­¦ä¹ è·¯å¾„ã€‚")
    ]
    cols = st.columns(3)
    for i, (mode_key, title, caption) in enumerate(modes_config):
        with cols[i % 3]:
            with st.container(border=True, height=230):
                st.subheader(title)
                st.caption(caption)
                button_label = f"å¼€å§‹{title.split(' ')[1][:2]}"
                if st.button(button_label, use_container_width=True, key=f"menu_{mode_key}"):
                    st.session_state.current_mode = mode_key
                    init_session_state()
                    st.session_state.current_mode = mode_key
                    st.rerun()
            st.write("")


# ----------------------------------------------------------------
# --- æ¨¡å¼ä¸€è‡³äº” (å®Œæ•´ç¨‹å¼ç¢¼) ---
# ----------------------------------------------------------------
def render_exploration_mode(llm):
    st.header("æ¨¡å¼ä¸€: èŒä¸šç›®æ ‡æ¢ç´¢")
    history = get_session_history("exploration_session")
    stage = st.session_state.get('exploration_stage', 1)

    def generate_interim_response(user_input, prompt_template):
        with st.chat_message("ai", avatar="ğŸ¤–"):
            chain = ChatPromptTemplate.from_template(prompt_template) | llm
            with st.spinner("AIæ•™ç»ƒæ­£åœ¨æ€è€ƒ..."):
                response_content = st.write_stream(chain.stream({"user_input": user_input}))
            history.add_ai_message(response_content)
        st.session_state.exploration_stage += 1
        st.rerun()

    for msg in history.messages:
        avatar = "ğŸ§‘â€ğŸ’»" if isinstance(msg, HumanMessage) else "ğŸ¤–"
        st.chat_message(msg.type, avatar=avatar).markdown(msg.content, unsafe_allow_html=True)
    if stage == 1:
        with st.chat_message("ai", avatar="ğŸ¤–"):
            if len(history.messages) == 0:
                welcome_msg = "ä½ å¥½ï¼æˆ‘å°†å¼•å¯¼ä½ ä½¿ç”¨â€œèŒä¸šç›®æ ‡ç¼˜èµ·åˆ†ææ¡†æ¶â€ï¼Œä»â€œæˆ‘â€ã€â€œç¤¾ä¼šâ€ã€â€œå®¶åº­â€ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ï¼Œç³»ç»Ÿæ€§åœ°æ¢ç´¢ä½ çš„èŒä¸šæ–¹å‘ã€‚\n\né¦–å…ˆï¼Œæˆ‘ä»¬æ¥åˆ†æâ€œæˆ‘â€è¿™ä¸ªæ ¸å¿ƒã€‚è¯·åœ¨ä¸‹æ–¹å›ç­”ï¼š"
                st.markdown(welcome_msg)
                history.add_ai_message(welcome_msg)
        questions = ["1. ä½ çš„ä¸“ä¸šæ˜¯ä»€ä¹ˆï¼Ÿä½ å¯¹å®ƒçš„çœ‹æ³•å¦‚ä½•ï¼Ÿ", "2. ä½ çš„å­¦æ ¡æˆ–è¿‡å¾€ç»å†ï¼Œä¸ºä½ æä¾›äº†æ€æ ·çš„å¹³å°ä¸åŸºç¡€ï¼Ÿ"]
        with st.form("stage1_form"):
            st.markdown("> **ç¬¬ä¸€é˜¶æ®µï¼šåˆ†æâ€œæˆ‘â€(å¯æ§å› ç´ )**")
            responses = [st.text_area(q, height=100, key=f"s1_q{i}") for i, q in enumerate(questions)]
            if st.form_submit_button("æäº¤å…³äºâ€œæˆ‘â€çš„åˆ†æ", use_container_width=True):
                if all(responses):
                    input_text = "### å…³äºâ€œæˆ‘â€çš„å›ç­”\n\n" + "\n\n".join(
                        [f"**{q}**\n{r}" for q, r in zip(questions, responses)])
                    history.add_user_message(input_text)
                    st.session_state.exploration_stage += 1
                    st.rerun()
                else:
                    st.warning("è¯·å®Œæ•´å¡«å†™æ‰€æœ‰é—®é¢˜çš„å›ç­”ã€‚")
    elif stage in [2, 4, 6]:
        last_user_message = history.messages[-1].content
        prompts = {
            2: GLOBAL_PERSONA + "ä»»åŠ¡ï¼šä½œä¸ºèŒä¸šæ•™ç»ƒï¼Œå¯¹ç”¨æˆ·åˆšæ‰æä¾›çš„å…³äºâ€œæˆ‘â€çš„ä¿¡æ¯ï¼Œç»™äºˆä¸€æ®µç®€çŸ­ã€ç§¯æçš„æ€»ç»“å’Œè‚¯å®šã€‚ç„¶åï¼Œè‡ªç„¶åœ°å¼•å‡ºæˆ‘ä»¬ä¸‹ä¸€ä¸ªè¦æ¢è®¨çš„â€œç¤¾ä¼šâ€ç»´åº¦ã€‚\nè¦æ±‚ï¼šè¯­è¨€è¦å¯Œæœ‰åŒç†å¿ƒï¼Œå……æ»¡é¼“åŠ±ï¼Œä¸è¦è¶…è¿‡100å­—ã€‚ç»“å°¾å¿…é¡»æ˜¯å¼•å‡ºä¸‹ä¸€é˜¶æ®µçš„æé—®ã€‚\nç”¨æˆ·çš„è¾“å…¥ï¼š{user_input}\nä½ çš„å›åº”ï¼š",
            4: GLOBAL_PERSONA + "ä»»åŠ¡ï¼šä½œä¸ºèŒä¸šæ•™ç»ƒï¼Œå¯¹ç”¨æˆ·åˆšæ‰æä¾›çš„å…³äºâ€œç¤¾ä¼šâ€è¶‹åŠ¿çš„è§‚å¯Ÿï¼Œç»™äºˆä¸€æ®µç®€çŸ­ã€å¯Œæœ‰æ´å¯ŸåŠ›çš„æ€»ç»“ã€‚ç„¶åï¼Œè‡ªç„¶åœ°å¼•å‡ºæˆ‘ä»¬éœ€è¦æ¢è®¨çš„æœ€åä¸€ä¸ªç»´åº¦â€œå®¶åº­â€ã€‚\nè¦æ±‚ï¼šè‚¯å®šç”¨æˆ·è§‚å¯Ÿçš„ä»·å€¼ï¼Œè¯­è¨€ç²¾ç‚¼ï¼Œä¸è¦è¶…è¿‡100å­—ã€‚ç»“å°¾å¿…é¡»æ˜¯å¼•å‡ºä¸‹ä¸€é˜¶æ®µçš„æé—®ã€‚\nç”¨æˆ·çš„è¾“å…¥ï¼š{user_input}\nä½ çš„å›åº”ï¼š",
            6: GLOBAL_PERSONA + "ä»»åŠ¡ï¼šä½œä¸ºèŒä¸šæ•™ç»ƒï¼Œå¯¹ç”¨æˆ·åˆšæ‰æä¾›çš„å…³äºâ€œå®¶åº­â€ä¸ç¯å¢ƒå½±å“çš„æè¿°ï¼Œç»™äºˆä¸€æ®µå¯Œæœ‰åŒç†å¿ƒå’Œç†è§£çš„å›åº”ã€‚ç„¶åå‘Šè¯‰ç”¨æˆ·ï¼Œç°åœ¨ä¿¡æ¯å·²ç»æ”¶é›†å®Œæ¯•ï¼Œä½ å°†ä¸ºä»–æ•´åˆæ‰€æœ‰ä¿¡æ¯å¹¶ç”Ÿæˆæœ€ç»ˆçš„åˆ†ææŠ¥å‘Šã€‚\nè¦æ±‚ï¼šè¡¨è¾¾ç†è§£å’Œå…±æƒ…ï¼Œè¯­è¨€æ¸©æš–ï¼Œä¸è¦è¶…è¿‡100å­—ã€‚æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ä¸‹ä¸€æ­¥æ˜¯ç”Ÿæˆæ€»æŠ¥å‘Šã€‚\nç”¨æˆ·çš„è¾“å…¥ï¼š{user_input}\nä½ çš„å›åº”ï¼š"
        }
        generate_interim_response(last_user_message, prompts[stage])
    elif stage in [3, 5]:
        forms = {
            3: ("stage3_form", "> **ç¬¬äºŒé˜¶æ®µï¼šåˆ†æâ€œç¤¾ä¼šâ€(å¤–éƒ¨æœºä¼š)**", "æäº¤å…³äºâ€œç¤¾ä¼šâ€çš„åˆ†æ",
                ["1. ä½ è§‚å¯Ÿåˆ°å½“ä¸‹æœ‰å“ªäº›ä½ æ„Ÿå…´è¶£çš„ç¤¾ä¼šæˆ–ç§‘æŠ€è¶‹åŠ¿ï¼Ÿï¼ˆä¾‹å¦‚ï¼šAIã€å¤§å¥åº·ã€å¯æŒç»­å‘å±•ç­‰ï¼‰",
                 "2. æ ¹æ®ä½ çš„è§‚å¯Ÿï¼Œè¿™äº›è¶‹åŠ¿å¯èƒ½å¸¦æ¥å“ªäº›æ–°çš„è¡Œä¸šæˆ–èŒä½æœºä¼šï¼Ÿ",
                 "3. åœ¨ä½ è¿‡å¾€çš„ç»å†ä¸­ï¼Œæœ‰æ²¡æœ‰ä¸€äº›å¶ç„¶çš„æœºç¼˜æˆ–æ‰“å·¥ç»éªŒï¼Œè®©ä½ å¯¹æŸä¸ªé¢†åŸŸäº§ç”Ÿäº†ç‰¹åˆ«çš„äº†è§£ï¼Ÿ"],
                "### å…³äºâ€œç¤¾ä¼šâ€çš„å›ç­”"),
            5: ("stage5_form", "> **ç¬¬ä¸‰é˜¶æ®µï¼šè§‰å¯Ÿâ€œå®¶åº­â€(ç¯å¢ƒå½±å“)**", "æäº¤å…³äºâ€œå®¶åº­â€çš„åˆ†æ",
                ["1. ä½ çš„å®¶åº­æˆ–é‡è¦äº²å‹ï¼Œå¯¹ä½ çš„èŒä¸šæœ‰ä»€ä¹ˆæ ·çš„æœŸå¾…ï¼Ÿ", "2. æœ‰æ²¡æœ‰å“ªä½æ¦œæ ·å¯¹ä½ çš„èŒä¸šé€‰æ‹©äº§ç”Ÿäº†å½±å“ï¼Ÿ",
                 "3. ä½ èº«è¾¹çš„â€œåœˆå­â€ï¼ˆä¾‹å¦‚æœ‹å‹ã€åŒå­¦ï¼‰ä¸»è¦ä»äº‹å“ªäº›å·¥ä½œï¼Ÿè¿™å¯¹ä½ æœ‰ä»€ä¹ˆæ½œåœ¨å½±å“ï¼Ÿ"],
                "### å…³äºâ€œå®¶åº­â€çš„å›ç­”")
        }
        form_key, title, button_text, questions, header = forms[stage]
        with st.form(form_key):
            st.markdown(title)
            responses = [st.text_area(q, height=100, key=f"s{stage}_q{i}") for i, q in enumerate(questions)]
            if st.form_submit_button(button_text, use_container_width=True):
                if all(responses):
                    input_text = f"{header}\n\n" + "\n\n".join([f"**{q}**\n{r}" for q, r in zip(questions, responses)])
                    history.add_user_message(input_text)
                    st.session_state.exploration_stage += 1
                    st.rerun()
                else:
                    st.warning("è¯·å®Œæ•´å¡«å†™æ‰€æœ‰é—®é¢˜çš„å›ç­”ã€‚")
    elif stage == 7:
        st.markdown("> **ç¬¬å››é˜¶æ®µï¼šAI æ™ºæ…§æ•´åˆä¸è¡ŒåŠ¨è®¡åˆ’**")
        with st.chat_message("ai", avatar="ğŸ¤–"):
            full_conversation = "\n\n".join([msg.content for msg in history.messages if isinstance(msg, HumanMessage)])
            stage4_prompt = ChatPromptTemplate.from_template(
                GLOBAL_PERSONA + "ä½œä¸ºä¸€åæ™ºæ…§ä¸”å¯Œæœ‰æ´å¯ŸåŠ›çš„èŒä¸šå‘å±•æ•™ç»ƒï¼Œè¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹ç”¨æˆ·åœ¨â€œæˆ‘â€ã€â€œç¤¾ä¼šâ€ã€â€œå®¶åº­â€ä¸‰ä¸ªé˜¶æ®µçš„å®Œæ•´å›ç­”ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆä¸€ä»½ç»“æ„æ¸…æ™°ã€å¯Œæœ‰æ´è§çš„æ•´åˆåˆ†æä¸å»ºè®®æŠ¥å‘Šã€‚æŠ¥å‘Šå¿…é¡»åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªæ ¸å¿ƒéƒ¨åˆ†ï¼š\n\n**1. æ ¸å¿ƒæ´å¯Ÿæ€»ç»“ï¼š**\n   - **ä¼˜åŠ¿ä¸æœºé‡ (S&O):** ç»“åˆç”¨æˆ·çš„â€œæˆ‘â€å’Œâ€œç¤¾ä¼šâ€ï¼Œæç‚¼å‡º 2-3 ä¸ªæœ€å…³é”®çš„ä¼˜åŠ¿ä¸å¤–éƒ¨æœºé‡çš„ç»“åˆç‚¹ã€‚\n   - **æŒ‘æˆ˜ä¸å…³æ³¨ (C&A):** ç»“åˆç”¨æˆ·çš„â€œæˆ‘â€çš„æ½œåœ¨å±€é™å’Œâ€œå®¶åº­/ç¯å¢ƒâ€çš„å½±å“ï¼ŒæŒ‡å‡º 1-2 ä¸ªéœ€è¦ç‰¹åˆ«å…³æ³¨å’Œåº”å¯¹çš„æŒ‘æˆ˜ã€‚\n\n**2. èŒä¸šæ–¹å‘å»ºè®® (æ¢ç´¢è±¡é™):**\n   - åŸºäºä»¥ä¸Šåˆ†æï¼Œæå‡º 2-3 ä¸ªå…·ä½“çš„ã€å¯æ¢ç´¢çš„èŒä¸šæ–¹å‘å»ºè®®ã€‚\n   - å¯¹æ¯ä¸ªæ–¹å‘ï¼Œç”¨ä¸€å¥è¯ç‚¹æ˜å®ƒä¸ºä»€ä¹ˆä¸ç”¨æˆ·çš„â€œæˆ‘-ç¤¾ä¼š-å®¶åº­â€åˆ†æç›¸åŒ¹é…ã€‚\n\n**3. ä¸‹ä¸€æ­¥è¡ŒåŠ¨æ¸…å• (Action Plan):**\n   - æä¾›ä¸€ä¸ªåŒ…å« 3-5 ä¸ªå…·ä½“ã€å¯æ‰§è¡Œçš„â€œè½»é‡çº§â€è¡ŒåŠ¨å»ºè®®ã€‚\n\n**æŠ¥å‘Šé£æ ¼è¦æ±‚ï¼š**\n- è¯­è¨€ä¸“ä¸šã€ç§¯æã€å¯Œæœ‰å¯å‘æ€§ï¼Œä½†ä¹Ÿè¦å®äº‹æ±‚æ˜¯ã€‚\n- ä½¿ç”¨ Markdown æ ¼å¼ï¼Œæ¡ç†æ¸…æ™°ï¼Œé‡ç‚¹çªå‡ºã€‚\n- ç›´æ¥è¾“å‡ºæŠ¥å‘Šå†…å®¹ï¼Œæ— éœ€é‡å¤ç”¨æˆ·çš„å›ç­”ã€‚\n\n---\nä»¥ä¸‹æ˜¯ç”¨æˆ·çš„å®Œæ•´å›ç­”:\n{conversation_history}\n---")
            stage4_chain = stage4_prompt | llm
            with st.spinner("AIæ•™ç»ƒæ­£åœ¨å…¨é¢åˆ†ææ‚¨çš„å›ç­”ï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š..."):
                response_content = st.write_stream(stage4_chain.stream({"conversation_history": full_conversation}))
            history.add_ai_message(response_content)
        st.session_state.exploration_stage += 1
        st.rerun()
    elif stage == 8:
        st.markdown(
            "> AIæ•™ç»ƒå·²æ ¹æ®æ‚¨çš„å›ç­”ï¼Œä¸ºæ‚¨æä¾›äº†ä¸€ä»½æ•´åˆåˆ†æä¸å»ºè®®ã€‚è¿™ä»½æŠ¥å‘Šæ˜¯ä¸ºæ‚¨é‡èº«æ‰“é€ çš„èµ·ç‚¹ï¼Œè€Œéç»ˆç‚¹ã€‚\n>\n> è¯·ä»”ç»†é˜…è¯»æŠ¥å‘Šï¼Œç„¶åå›ç­”æœ€åä¸€ä¸ªã€ä¹Ÿæ˜¯æœ€é‡è¦çš„é—®é¢˜ï¼š\n> **æ‚¨è‡ªå·±å†³å®šè¦é‡‡å–çš„ã€ä¸‹å‘¨å¯ä»¥å®Œæˆçš„ç¬¬ä¸€ä¸ªå…·ä½“è¡ŒåŠ¨æ˜¯ä»€ä¹ˆï¼Ÿ**")
        if user_input := st.chat_input("è¯·åœ¨æ­¤è¾“å…¥æ‚¨çš„æœ€ç»ˆè¡ŒåŠ¨è®¡åˆ’..."):
            history.add_user_message(f"æˆ‘çš„æœ€ç»ˆè¡ŒåŠ¨è®¡åˆ’æ˜¯ï¼š{user_input}")
            st.session_state.exploration_stage += 1
            st.rerun()
    elif stage == 9:
        with st.chat_message("ai", avatar="ğŸ¤–"):
            final_msg = "å¤ªæ£’äº†ï¼æ˜ç¡®çš„è¡ŒåŠ¨æ˜¯æ¨åŠ¨ä¸€åˆ‡æ”¹å˜çš„å¼€å§‹ã€‚é¢„ç¥ä½ è¡ŒåŠ¨é¡ºåˆ©ï¼Œåœ¨èŒä¸šæ¢ç´¢çš„é“è·¯ä¸Šä¸æ–­æœ‰æ–°çš„å‘ç°å’Œæ”¶è·ï¼"
            st.markdown(final_msg)
            history.add_ai_message(final_msg)
        st.success("æ­å–œï¼æ‚¨å·²å®Œæˆæœ¬æ¬¡æ¢ç´¢çš„å…¨è¿‡ç¨‹ã€‚")
        st.session_state.exploration_stage += 1


def render_decision_mode(llm):
    st.header("æ¨¡å¼äºŒ: Offer å†³ç­–åˆ†æ")
    with st.container(border=True):
        st.info("è¯·è¾“å…¥ä¸¤ä¸ªOfferçš„å…³é”®ä¿¡æ¯ï¼ŒAIå°†ä¸ºæ‚¨ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„å¯¹æ¯”åˆ†ææŠ¥å‘Šã€‚")
        chain = ChatPromptTemplate.from_template(
            GLOBAL_PERSONA + "ä½œä¸ºä¸€åä¸“ä¸šçš„èŒä¸šé¡¾é—®ï¼Œä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·å¯¹æ¯”ä¸¤ä¸ªOfferï¼Œå¹¶æ ¹æ®ä»–ä»¬æä¾›çš„ä¸ªäººåå¥½ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–ã€é€»è¾‘æ¸…æ™°çš„åˆ†ææŠ¥å‘Šã€‚\n\n**è¾“å…¥ä¿¡æ¯:**\n- **Offer A è¯¦æƒ…:** {offer_a_details}\n- **Offer B è¯¦æƒ…:** {offer_b_details}\n- **ç”¨æˆ·çš„ä¸ªäººåå¥½ (æŒ‰é‡è¦æ€§æ’åº):** {user_priorities_sorted_list}\n\n**è¾“å‡ºæŠ¥å‘Šè¦æ±‚:**\n1.  **å¼€ç¯‡æ€»ç»“:** é¦–å…ˆï¼Œå¯¹ä¸¤ä¸ªOfferçš„æ ¸å¿ƒäº®ç‚¹è¿›è¡Œä¸€å¥è¯æ€»ç»“ã€‚\n2.  **å¤šç»´åº¦å¯¹æ¯”åˆ†æ:**\n    -   æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„åå¥½ç»´åº¦è¿›è¡Œé€ä¸€å¯¹æ¯”ã€‚\n    -   å¦‚æœç”¨æˆ·æœªæä¾›åå¥½ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„é€šç”¨ç»´åº¦ï¼ˆå¦‚ï¼šè–ªé…¬ã€å‘å±•ã€ç¨³å®šæ€§ã€é€šå‹¤ã€æ–‡åŒ–ï¼‰è¿›è¡Œåˆ†æã€‚\n    -   åœ¨æ¯ä¸ªç»´åº¦ä¸‹ï¼Œæ¸…æ™°åœ°åˆ—å‡ºOffer Aå’ŒOffer Bå„è‡ªçš„è¡¨ç°ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªç®€çŸ­çš„å°ç»“ã€‚\n    -   ä½¿ç”¨Markdownçš„è¡¨æ ¼æˆ–é¡¹ç›®ç¬¦å·ï¼Œè®©å¯¹æ¯”ä¸€ç›®äº†ç„¶ã€‚\n3.  **ç»¼åˆå»ºè®®:**\n    -   åŸºäºå‰é¢çš„å¤šç»´åº¦åˆ†æï¼Œç»™å‡ºä¸€ä¸ªç»¼åˆæ€§çš„å†³ç­–å»ºè®®ã€‚\n    -   æ˜ç¡®æŒ‡å‡ºå“ªä¸ªOfferä¸ç”¨æˆ·çš„åå¥½æ›´åŒ¹é…ï¼Œå¹¶è§£é‡ŠåŸå› ã€‚\n4.  **é£æ ¼è¦æ±‚:** è¯­è¨€å®¢è§‚ã€ä¸­ç«‹ã€å¯Œæœ‰é€»è¾‘ï¼Œé¿å…ä½¿ç”¨ç»å¯¹åŒ–çš„è¯è¯­ã€‚") | llm
        st.subheader("ç¬¬ä¸€æ­¥ï¼šè¯·å¡«å†™ Offer çš„æ ¸å¿ƒä¿¡æ¯")
        col1, col2 = st.columns(2, gap="large");
        with col1:
            offer_a = st.text_area("Offer A å…³é”®ä¿¡æ¯", height=200,
                                   placeholder="ä¾‹å¦‚ï¼š\nå…¬å¸: Aç§‘æŠ€\nèŒä½: åˆçº§äº§å“ç»ç†\nè–ªèµ„: 15k * 14è–ª\nåœ°ç‚¹: ä¸Šæµ·å¼ æ±Ÿ...")
        with col2:
            offer_b = st.text_area("Offer B å…³é”®ä¿¡æ¯", height=200,
                                   placeholder="ä¾‹å¦‚ï¼š\nå…¬å¸: Bé›†å›¢\nèŒä½: ç®¡åŸ¹ç”Ÿ\nè–ªèµ„: 13k * 16è–ª + 2wç­¾å­—è´¹\nåœ°ç‚¹: åŒ—äº¬æµ·æ·€...")
        st.subheader("ç¬¬äºŒæ­¥ï¼š(å¯é€‰) æ·»åŠ ä½ çš„ä¸ªäººåå¥½")
        priorities_options = ["èŒä¸šæˆé•¿", "è–ªèµ„ç¦åˆ©", "å·¥ä½œç”Ÿæ´»å¹³è¡¡", "å›¢é˜Ÿæ°›å›´", "å…¬å¸ç¨³å®šæ€§"]
        user_priorities = st.multiselect("è¯·æŒ‰é‡è¦æ€§ä¾æ¬¡é€‰æ‹©ä½ çš„èŒä¸šåå¥½ï¼š", options=priorities_options)
        if st.button("ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š", use_container_width=True):
            if not offer_a or not offer_b:
                st.warning("è¯·è¾“å…¥ä¸¤ä¸ªOfferçš„ä¿¡æ¯ã€‚")
            else:
                with st.spinner("æ­£åœ¨ä¸ºæ‚¨ç”ŸæˆOfferåˆ†ææŠ¥å‘Š..."):
                    priorities_text = ", ".join(user_priorities) if user_priorities else "ç”¨æˆ·æœªæŒ‡å®š"
                    response_stream = chain.stream({"offer_a_details": offer_a, "offer_b_details": offer_b,
                                                    "user_priorities_sorted_list": priorities_text})
                    st.markdown("---");
                    st.subheader("ğŸ“‹ Offerå¯¹æ¯”åˆ†ææŠ¥å‘Š");
                    st.write_stream(response_stream)


def render_communication_mode(llm):
    st.header("æ¨¡å¼ä¸‰: å®¶åº­æ²Ÿé€šæ¨¡æ‹Ÿ")
    if not st.session_state.get('sim_started', False):
        with st.container(border=True):
            st.info("åœ¨è¿™é‡Œï¼ŒAIå¯ä»¥æ‰®æ¼”æ‚¨çš„å®¶äººï¼Œå¸®åŠ©æ‚¨ç»ƒä¹ å¦‚ä½•æ²Ÿé€šèŒä¸šè§„åˆ’ï¼Œå¹¶æä¾›å¤ç›˜å»ºè®®ã€‚")
            my_choice = st.text_input("ä½ æƒ³å’Œå®¶äººæ²Ÿé€šçš„èŒä¸šé€‰æ‹©æ˜¯ï¼Ÿ")
            family_concern = st.text_area("ä½ è®¤ä¸ºä»–ä»¬ä¸»è¦çš„æ‹…å¿§ä¼šæ˜¯ä»€ä¹ˆï¼Ÿ",
                                          placeholder="ä¾‹å¦‚: å·¥ä½œä¸ç¨³å®šã€ä¸æ˜¯é“é¥­ç¢—ã€ç¦»å®¶å¤ªè¿œç­‰")
            if st.button("å¼€å§‹æ¨¡æ‹Ÿ"):
                if not my_choice or not family_concern:
                    st.warning("è¯·è¾“å…¥æ‚¨çš„èŒä¸šé€‰æ‹©å’Œé¢„æƒ³çš„å®¶äººæ‹…å¿§ã€‚")
                else:
                    st.session_state.my_choice = my_choice;
                    st.session_state.family_concern = family_concern
                    st.session_state.sim_started = True;
                    st.session_state.debrief_requested = False
                    initial_ai_prompt = f"å­©å­ï¼Œå…³äºä½ æƒ³åšâ€œ{my_choice}â€è¿™ä¸ªäº‹ï¼Œæˆ‘æœ‰äº›æ‹…å¿ƒã€‚æˆ‘ä¸»è¦æ˜¯è§‰å¾—å®ƒâ€œ{family_concern}â€ã€‚æˆ‘ä»¬èƒ½èŠèŠå—ï¼Ÿ"
                    get_session_history("communication_session").add_ai_message(initial_ai_prompt);
                    st.rerun()
    if st.session_state.get('sim_started', False):
        st.success(f"æ¨¡æ‹Ÿå¼€å§‹ï¼AIæ­£åœ¨æ‰®æ¼”æ‹…å¿§æ‚¨é€‰æ‹© â€œ{st.session_state.my_choice}â€ çš„å®¶äººã€‚")
        history = get_session_history("communication_session")
        with st.container():
            for msg in history.messages:
                avatar = "ğŸ§‘â€ğŸ’»" if isinstance(msg, HumanMessage) else "ğŸ§“";
                st.chat_message(msg.type, avatar=avatar).markdown(msg.content)
        if not st.session_state.get('debrief_requested', False):
            communication_prompt = ChatPromptTemplate.from_messages([("system",
                                                                      GLOBAL_PERSONA + f"ç°åœ¨ï¼Œä½ å°†æ‰®æ¼”ä¸€ä¸ªå…³å¿ƒå­©å­ä½†æ€æƒ³ç•¥æ˜¾ä¼ ç»Ÿçš„å®¶äººï¼ˆçˆ¶äº²/æ¯äº²ï¼‰ã€‚\nä½ çš„èƒŒæ™¯ï¼šä½ éå¸¸çˆ±è‡ªå·±çš„å­©å­ï¼Œä½†å¯¹æ–°å…´èŒä¸šä¸å¤ªäº†è§£ï¼Œæ›´çœ‹é‡ç¨³å®šã€ä½“é¢çš„å·¥ä½œã€‚\nä½ çš„ä»»åŠ¡ï¼š\n1. ä½ çš„å¼€åœºç™½å·²ç»ç”±ç³»ç»Ÿç»™å‡ºã€‚\n2. åœ¨æ¥ä¸‹æ¥çš„å¯¹è¯ä¸­ï¼ŒæŒç»­è¡¨è¾¾ä½ å¯¹å­©å­èŒä¸šé€‰æ‹©({st.session_state.my_choice})çš„æ‹…å¿§({st.session_state.family_concern})ã€‚\n3. ä½ çš„è¯­æ°”è¦çœŸè¯šã€å…³åˆ‡ï¼Œå¯ä»¥ç•¥å¸¦å›ºæ‰§ï¼Œä½†æœ€ç»ˆç›®çš„æ˜¯å¸Œæœ›å­©å­èƒ½è¿‡å¾—å¥½ã€‚\n4. æ ¹æ®ç”¨æˆ·çš„å›åº”è¿›è¡Œè¿½é—®ã€‚\n5. ä¿æŒä½ çš„è§’è‰²ï¼Œç›´åˆ°ç”¨æˆ·ç‚¹å‡»â€œç»“æŸæ¨¡æ‹Ÿâ€ã€‚"),
                                                                     MessagesPlaceholder(variable_name="history"),
                                                                     ("human", "{input}")])
            chain_with_history = RunnableWithMessageHistory(communication_prompt | llm,
                                                            lambda s: get_session_history(s),
                                                            input_messages_key="input", history_messages_key="history")
            if user_input := st.chat_input("ä½ çš„å›åº”:"):
                with st.spinner("..."): chain_with_history.invoke({"input": user_input}, config={
                    "configurable": {"session_id": "communication_session"}}); st.rerun()
            if len(history.messages) > 2:
                if st.button("ç»“æŸæ¨¡æ‹Ÿå¹¶è·å–å¤ç›˜å»ºè®®"): st.session_state.debrief_requested = True; st.rerun()
        else:
            with st.container(border=True):
                st.info("å¯¹è¯å·²ç»“æŸã€‚AIæ•™ç»ƒæ­£åœ¨ä¸ºæ‚¨å¤ç›˜åˆšæ‰çš„æ²Ÿé€šè¡¨ç°...")
                full_conversation = "\n".join(
                    [f"{'æˆ‘' if isinstance(msg, HumanMessage) else 'å®¶äºº'}: {msg.content}" for msg in history.messages])
                debrief_prompt = ChatPromptTemplate.from_template(
                    GLOBAL_PERSONA + "ä½ ç°åœ¨åˆ‡æ¢å›èŒä¸šå‘å±•æ•™ç»ƒçš„è§’è‰²ã€‚\nä»»åŠ¡ï¼šè¯·å¯¹ä»¥ä¸‹è¿™æ®µâ€œæˆ‘â€ä¸â€œå®¶äººâ€å…³äºèŒä¸šé€‰æ‹©çš„æ²Ÿé€šå¯¹è¯è¿›è¡Œå¤ç›˜ï¼Œå¹¶ç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„æ²Ÿé€šè¡¨ç°æŠ¥å‘Šã€‚\n\n**å·²çŸ¥èƒŒæ™¯:**\n- æˆ‘çš„èŒä¸šé€‰æ‹©: {my_choice}\n- å®¶äººé¢„è®¾çš„æ‹…å¿§: {family_concern}\n\n**æ²Ÿé€šè®°å½•:**\n{conversation_history}\n\n**å¤ç›˜æŠ¥å‘Šè¦æ±‚:**\n1.  **æ²Ÿé€šäº®ç‚¹ (åšå¾—å¥½çš„åœ°æ–¹):**\n    -   è¯†åˆ«å¹¶è¡¨æ‰¬æˆ‘åœ¨å¯¹è¯ä¸­ä½¿ç”¨çš„æœ‰æ•ˆæ²Ÿé€šæŠ€å·§ã€‚\n2.  **å¯æå‡ç‚¹ (å¯ä»¥åšå¾—æ›´å¥½çš„åœ°æ–¹):**\n    -   å»ºè®¾æ€§åœ°æŒ‡å‡ºæ²Ÿé€šä¸­å¯ä»¥æ”¹è¿›çš„åœ°æ–¹ã€‚\n3.  **æ ¸å¿ƒç­–ç•¥å»ºè®®:**\n    -   æä¾› 2-3æ¡å…·ä½“çš„ã€å¯æ“ä½œçš„æ²Ÿé€šç­–ç•¥ã€‚\n\næŠ¥å‘Šé£æ ¼éœ€ä¸“ä¸šã€å®¢è§‚ã€å¯Œæœ‰å»ºè®¾æ€§ã€‚")
                debrief_chain = debrief_prompt | llm
                with st.spinner("æ­£åœ¨ç”Ÿæˆæ²Ÿé€šå¤ç›˜æŠ¥å‘Š..."):
                    response_stream = debrief_chain.stream(
                        {"my_choice": st.session_state.my_choice, "family_concern": st.session_state.family_concern,
                         "conversation_history": full_conversation})
                    st.subheader("ğŸ“‹ æ²Ÿé€šè¡¨ç°å¤ç›˜æŠ¥å‘Š");
                    st.write_stream(response_stream)


def render_company_info_mode(llm):
    st.header("æ¨¡å¼å››: ä¼ä¸šä¿¡æ¯é€Ÿè§ˆ")
    with st.container(border=True):
        st.info("è¯·è¾“å…¥å…¬å¸å…¨åï¼ŒAIå°†ä¸ºæ‚¨ç»¼åˆç½‘ç»œä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½æ ¸å¿ƒä¿¡æ¯é€Ÿè§ˆæŠ¥å‘Šã€‚")
        chain = ChatPromptTemplate.from_template(
            GLOBAL_PERSONA + "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å•†ä¸šåˆ†æå¸ˆAIã€‚\nä»»åŠ¡ï¼šè¯·ä¸ºç”¨æˆ·æŸ¥è¯¢å¹¶ç”Ÿæˆä¸€ä»½å…³äº **{company_name}** çš„æ ¸å¿ƒä¿¡æ¯é€Ÿè§ˆæŠ¥å‘Šã€‚\n\n**æŠ¥å‘Šå¿…é¡»åŒ…å«ä»¥ä¸‹éƒ¨åˆ†:**\n1.  **ä¸€å¥è¯æ€»ç»“:** ç”¨ä¸€å¥è¯ç²¾å‡†æ¦‚æ‹¬è¯¥å…¬å¸çš„æ ¸å¿ƒä¸šåŠ¡å’Œå¸‚åœºåœ°ä½ã€‚\n2.  **å…¬å¸ç®€ä»‹:** ç®€è¦ä»‹ç»å…¬å¸çš„æˆç«‹èƒŒæ™¯ã€ä¸»è¥ä¸šåŠ¡ã€å…³é”®äº§å“æˆ–æœåŠ¡ã€‚\n3.  **è¿‘æœŸåŠ¨æ€ä¸æ–°é—»:**\n    -   æ€»ç»“ 1-2 æ¡è¯¥å…¬å¸è¿‘æœŸçš„é‡è¦åŠ¨æ€ã€æˆ˜ç•¥è°ƒæ•´æˆ–ç›¸å…³çš„è¡Œä¸šæ–°é—»ã€‚\n4.  **çƒ­æ‹›æ–¹å‘åˆ†æ:**\n    -   åˆ†æè¯¥å…¬å¸è¿‘æœŸçš„æ‹›è˜è¶‹åŠ¿ï¼ŒæŒ‡å‡º 2-3 ä¸ªé‡ç‚¹æ‹›è˜çš„èŒèƒ½æ–¹å‘æˆ–å²—ä½ç±»å‹ã€‚\n5.  **SWOTåˆ†æ (ç®€ç‰ˆ):**\n    -   **ä¼˜åŠ¿(S):** æœ€ä¸»è¦çš„ç«äº‰ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ\n    -   **åŠ£åŠ¿(W):** é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æˆ–ä¸è¶³æ˜¯ä»€ä¹ˆï¼Ÿ\n    -   **æœºä¼š(O):** å¤–éƒ¨ç¯å¢ƒå¸¦æ¥äº†å“ªäº›å‘å±•æœºä¼šï¼Ÿ\n    -   **å¨èƒ(T):** å¸‚åœºæˆ–ç«äº‰å¸¦æ¥äº†å“ªäº›æ½œåœ¨å¨èƒï¼Ÿ\n\nè¯·ç¡®ä¿æŠ¥å‘Šå†…å®¹å®¢è§‚ã€ä¿¡æ¯å‡ç»ƒã€æ¡ç†æ¸…æ™°ã€‚") | llm
        company_name = st.text_input("è¯·è¾“å…¥å…¬å¸åç§°:", placeholder="ä¾‹å¦‚ï¼šé˜¿é‡Œå·´å·´ã€è…¾è®¯ã€å­—èŠ‚è·³åŠ¨")
        if st.button("ç”Ÿæˆé€Ÿè§ˆæŠ¥å‘Š", use_container_width=True):
            if not company_name:
                st.warning("è¯·è¾“å…¥å…¬å¸åç§°ã€‚")
            else:
                with st.spinner(f"æ­£åœ¨ä¸ºæ‚¨åˆ†æâ€œ{company_name}â€..."):
                    response_stream = chain.stream({"company_name": company_name})
                    st.markdown("---");
                    st.subheader(f"ğŸ“„ {company_name} - æ ¸å¿ƒä¿¡æ¯é€Ÿè§ˆ");
                    st.write_stream(response_stream)


def render_panoramic_mode(llm):
    st.header("æ¨¡å¼äº”: èŒä¸šè·¯å¾„å…¨æ™¯è§„åˆ’")
    history = get_session_history("panoramic_session")
    stage = st.session_state.get('panoramic_stage', 1)
    for msg in history.messages:
        avatar = "ğŸ§‘â€ğŸ’»" if isinstance(msg, HumanMessage) else "ğŸ¤–"
        with st.chat_message(msg.type, avatar=avatar):
            if msg.type == 'ai' and "```mermaid" in msg.content:
                parts = msg.content.split("```mermaid");
                st.markdown(parts[0])
                mermaid_section = "```mermaid" + parts[1];
                mermaid_match = re.search("```mermaid\n(.*?)\n```", mermaid_section, re.DOTALL)
                if mermaid_match:
                    mermaid_code = mermaid_match.group(1);
                    st.subheader("äº§ä¸šé“¾å¯è§†åŒ–å›¾è¡¨")
                    with st.container(border=True): st_mermaid(mermaid_code.strip())
                after_diagram_content = mermaid_section.split("```")[-1]
                if after_diagram_content.strip(): st.markdown(after_diagram_content)
            else:
                st.markdown(msg.content)
    meta_prompt_template = GLOBAL_PERSONA + "You are an expert career strategist, guiding the user through a multi-stage panoramic career path analysis. You are currently in Stage {current_stage}.\nUser's Core Competency Profile: {user_profile}\nUser's Chosen Profession(s): {chosen_professions}\nUser's Chosen Region(s): {chosen_region}\n\nYour Task is to execute the current stage's logic.\n--- STAGE-SPECIFIC INSTRUCTIONS ---\n**Stage 1:** Do not respond.\n**Stage 2 (Profession Concretization):** Based on the user's profile, present 3-5 concrete professions and prompt the user to select one or two.\n**Stage 3 (Enterprise & Region Targeting):** Based on the chosen profession, identify representative companies and primary geographic clusters in China. Prompt the user for their geographical preference.\n**Stage 4 (Final Comprehensive Report):** The user has provided all inputs. Generate a single, comprehensive report with the following sections:\n    1.  **äº§ä¸šé“¾ä½ç½®åˆ†æ:** Explain the role's position in the industry chain. Then, generate a Mermaid flowchart (`graph TD`). **CRITICAL SYNTAX RULE:** To create a line break inside a node's text, you MUST use the `<br>` HTML tag, and the entire text MUST be enclosed in double quotes.\n    2.  **è¡Œä¸šè¶‹åŠ¿ä¸â€œ365ç†è®ºâ€å®šæ€§:** Analyze industry trends and classify the industry as 'æˆ˜ç•¥å‹', 'æ”¯æŸ±å‹', or 'è¶‹åŠ¿å‹'.\n    3.  **ç›®æ ‡èŒèƒ½è¦æ±‚ä¸å·®è·åˆ†æ:** List typical requirements and perform a gap analysis.\n    4.  **ä¸ªäººå‘å±•è“å›¾:** Provide 2-3 actionable suggestions.\n    5.  **æ€»ç»“ä¸æˆ˜ç•¥è§„åˆ’:** Provide a concluding summary.\n    6.  **ã€CRITICALã€‘æˆ˜ç•¥æ€§æ€è€ƒç‚¹:** Finally, conclude with this section, providing 2-3 introspective questions for the user's long-term reflection. **DO NOT ask the user to answer them now.**"
    chain = ChatPromptTemplate.from_template(meta_prompt_template) | llm
    if stage == 1:
        st.markdown("> ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„èŒä¸šè·¯å¾„è§„åˆ’åŠ©æ‰‹ã€‚è®©æˆ‘ä»¬ä»è®¤è¯†ä½ è‡ªå·±å¼€å§‹ã€‚")
        with st.form("profile_form"):
            st.subheader("è¯·æ ¹æ®ä»¥ä¸‹äº”ä¸ªç»´åº¦ï¼Œæè¿°ä½ çš„â€œæ ¸å¿ƒèƒ½åŠ›â€ï¼š");
            edu = st.text_area("å­¦å†èƒŒæ™¯", placeholder="ä½ çš„ä¸“ä¸šã€å­¦ä½ã€ä»¥åŠç›¸å…³çš„æ ¸å¿ƒè¯¾ç¨‹");
            skills = st.text_area("æ ¸å¿ƒæŠ€èƒ½", placeholder="ä½ æœ€æ“…é•¿çš„3-5é¡¹ç¡¬æŠ€èƒ½æˆ–è½¯æŠ€èƒ½");
            exp = st.text_area("ç›¸å…³ç»éªŒ", placeholder="ç›¸å…³çš„å®ä¹ ã€å·¥ä½œé¡¹ç›®ã€æˆ–ä¸ªäººä½œå“é›†");
            char = st.text_area("å“è¡Œç‰¹è´¨", placeholder="ä½ è®¤ä¸ºè‡ªå·±æœ€é‡è¦çš„èŒä¸šå“è¡Œæˆ–å·¥ä½œé£æ ¼");
            motiv = st.text_area("å†…åœ¨åŠ¨æœº", placeholder="åœ¨å·¥ä½œä¸­ï¼Œä»€ä¹ˆæœ€èƒ½ç»™ä½ å¸¦æ¥æˆå°±æ„Ÿï¼Ÿ")
            if st.form_submit_button("æäº¤æˆ‘çš„èƒ½åŠ›ç”»åƒ", use_container_width=True):
                if all([edu, skills, exp, char, motiv]):
                    profile_text = f"å­¦å†èƒŒæ™¯: {edu}\næ ¸å¿ƒæŠ€èƒ½: {skills}\nç›¸å…³ç»éªŒ: {exp}\nå“è¡Œç‰¹è´¨: {char}\nå†…åœ¨åŠ¨æœº: {motiv}"
                    st.session_state.user_profile = profile_text;
                    history.add_user_message(f"è¿™æ˜¯æˆ‘çš„èƒ½åŠ›ç”»åƒï¼š\n{profile_text}")
                    st.session_state.panoramic_stage = 2;
                    st.rerun()
                else:
                    st.warning("è¯·å¡«å†™æ‰€æœ‰äº”ä¸ªç»´åº¦çš„ä¿¡æ¯ã€‚")
    elif stage in [2, 3]:
        if len(history.messages) % 2 != 0:
            with st.chat_message("ai", avatar="ğŸ¤–"):
                with st.spinner("AI æ­£åœ¨ä¸ºæ‚¨åˆ†æ..."):
                    response_stream = chain.stream(
                        {"current_stage": stage, "user_profile": st.session_state.user_profile,
                         "chosen_professions": st.session_state.get('chosen_professions', 'N/A'),
                         "chosen_region": st.session_state.get('chosen_region', 'N/A')})
                    response_content = st.write_stream(response_stream);
                    history.add_ai_message(response_content)
            st.rerun()
        st.info("ğŸ‘‡ è¯·åœ¨ä¸‹æ–¹çš„è¾“å…¥æ¡†ä¸­è¾“å…¥æ‚¨çš„é€‰æ‹©æˆ–æƒ³æ³•...", icon="ğŸ’¡")
        if user_input := st.chat_input("è¯·åœ¨æ­¤è¾“å…¥æ‚¨çš„é€‰æ‹©æˆ–æƒ³æ³•..."):
            history.add_user_message(user_input)
            if stage == 2:
                st.session_state.chosen_professions = user_input
            elif stage == 3:
                st.session_state.chosen_region = user_input
            st.session_state.panoramic_stage += 1;
            st.rerun()
    elif stage == 4:
        if len(history.messages) % 2 != 0:
            with st.chat_message("ai", avatar="ğŸ¤–"):
                st.markdown("å¥½çš„ï¼Œå·²æ”¶åˆ°æ‚¨çš„æ‰€æœ‰ä¿¡æ¯ã€‚ç°åœ¨ï¼Œæˆ‘å°†ä¸ºæ‚¨ç”Ÿæˆä¸€ä»½å®Œæ•´çš„ç»¼åˆåˆ†ææŠ¥å‘Š...")
                with st.spinner("AI æ­£åœ¨ä¸ºæ‚¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š..."):
                    response_stream = chain.stream({"current_stage": 4, "user_profile": st.session_state.user_profile,
                                                    "chosen_professions": st.session_state.get('chosen_professions',
                                                                                               'N/A'),
                                                    "chosen_region": st.session_state.get('chosen_region', 'N/A')})
                    response_content = st.write_stream(response_stream);
                    history.add_ai_message(response_content)
            st.session_state.panoramic_stage += 1;
            st.rerun()
    elif stage == 5:
        st.success("æ­å–œï¼æ‚¨å·²å®Œæˆæœ¬æ¬¡èŒä¸šè·¯å¾„å…¨æ™¯è§„åˆ’ã€‚")
        st.info("æ‚¨å¯ä»¥å‘ä¸Šæ»šåŠ¨æŸ¥çœ‹ä¸ºæ‚¨ç”Ÿæˆçš„å®Œæ•´æŠ¥å‘Šã€‚")
        if len(history.messages) > 0 and history.messages[-1].type == 'ai':
            report_content = history.messages[-1].content
            text_only_report = re.sub("```mermaid\n(.*?)\n```", "\n[æ­¤å¤„åŸä¸ºå¯è§†åŒ–å›¾è¡¨]\n", report_content,
                                      flags=re.DOTALL)
            st.download_button(label="ğŸ“¥ ä¸‹è½½å®Œæ•´æŠ¥å‘Š (.md)", data=text_only_report.encode('utf-8'),
                               file_name="æˆ‘çš„èŒä¸šè·¯å¾„è§„åˆ’æŠ¥å‘Š.md", mime="text/markdown")


# ----------------------------------------------------------------
# --- æ¨¡å¼å…­ï¼šä¸“ä¸šåŸ¹å…»æ–¹æ¡ˆè§£æ (æ•´åˆOCRçš„æœ€ç»ˆç‰ˆ) ---
# ----------------------------------------------------------------
def render_curriculum_mode(llm):
    st.header("æ¨¡å¼å…­: ä¸“ä¸šåŸ¹å…»æ–¹æ¡ˆè§£æ")
    st.markdown("---")
    history = get_session_history("curriculum_session")
    stage = st.session_state.get('curriculum_stage', 1)

    for msg in history.messages:
        avatar = "ğŸ§‘â€ğŸ’»" if isinstance(msg, HumanMessage) else "ğŸ¤–"
        with st.chat_message(msg.type, avatar=avatar):
            if msg.type == 'ai' and "```mermaid" in msg.content:
                parts = msg.content.split("```mermaid")
                st.markdown(parts[0], unsafe_allow_html=True)
                mermaid_section = "```mermaid" + parts[1]
                mermaid_match = re.search("```mermaid\n(.*?)\n```", mermaid_section, re.DOTALL)
                if mermaid_match:
                    mermaid_code = mermaid_match.group(1)
                    st.subheader("é‡ç‚¹è¯¾ç¨‹å­¦ä¹ è·¯å¾„å›¾")
                    with st.container(border=True):
                        st_mermaid(mermaid_code.strip(), height="500px")
                after_diagram_content = mermaid_section.split("```")[-1]
                if after_diagram_content.strip():
                    st.markdown(after_diagram_content, unsafe_allow_html=True)
            else:
                st.markdown(msg.content, unsafe_allow_html=True)

    if stage == 1:
        st.info("è¯·ä¸Šä¼ æ‚¨ä¸“ä¸šçš„æœ¬ç§‘äººæ‰åŸ¹å…»æ–¹æ¡ˆï¼ˆPDFæˆ–TXTæ ¼å¼ï¼‰ï¼ŒAIå­¦ä¸šå¯¼å¸ˆå°†ä¸ºæ‚¨æ·±åº¦è§£æã€‚")
        uploaded_file = st.file_uploader("ç‚¹å‡»æ­¤å¤„ä¸Šä¼ æ–‡ä»¶...", type=['pdf', 'txt'], label_visibility="collapsed")

        if uploaded_file is not None:
            if st.button("ç¬¬ä¸€æ­¥ï¼šåˆ†æäººæ‰åŸ¹å…»æ–¹å‘", use_container_width=True, type="primary"):
                content = ""
                with st.spinner(f"æ­£åœ¨è¯»å–æ–‡ä»¶ '{uploaded_file.name}'..."):
                    try:
                        import PyPDF2
                        uploaded_file.seek(0)
                        pdf_reader = PyPDF2.PdfReader(uploaded_file)
                        for page in pdf_reader.pages:
                            page_text = page.extract_text()
                            if page_text:
                                content += page_text + "\n\n"

                        if not content.strip():
                            st.info("å¿«é€Ÿè¯»å–å¤±è´¥ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢è‡³AIæ–‡å­—è¯†åˆ«(OCR)æ¨¡å¼ï¼Œå¤„ç†æ‰«æä»¶é€Ÿåº¦è¾ƒæ…¢ï¼Œè¯·ç¨å€™...")
                            import pytesseract
                            from pdf2image import convert_from_bytes

                            uploaded_file.seek(0)
                            images = convert_from_bytes(uploaded_file.read())
                            ocr_texts = []
                            for i, image in enumerate(images):
                                with st.spinner(f"æ­£åœ¨è¯†åˆ«ç¬¬ {i + 1}/{len(images)} é¡µ..."):
                                    text = pytesseract.image_to_string(image, lang='chi_sim+eng')
                                    ocr_texts.append(text)
                            content = "\n\n--- Page Break ---\n\n".join(ocr_texts)

                        if not content.strip():
                            st.error("æ— æ³•ä»æ–‡ä»¶ä¸­æå–æœ‰æ•ˆæ–‡æœ¬å†…å®¹ï¼Œå³ä½¿å°è¯•äº†OCRä¹Ÿå¤±è´¥äº†ã€‚è¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æŸåæˆ–è¿‡äºæ¨¡ç³Šã€‚")
                            st.stop()

                        st.session_state.curriculum_content = content
                        history.add_user_message("è¿™æ˜¯æˆ‘çš„ä¸“ä¸šåŸ¹å…»æ–¹æ¡ˆï¼Œè¯·å¸®æˆ‘åˆ†æã€‚")

                        prompt = ChatPromptTemplate.from_template(
                            """
                            æ ¸å¿ƒè§’è‰²: ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å¤§å­¦å­¦ä¸šå¯¼å¸ˆå’ŒèŒä¸šè§„åˆ’ä¸“å®¶ã€‚
                            ä»»åŠ¡: è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„ï¼Œç”Ÿæˆä¸€ä»½å…³äºè¿™ä»½æœ¬ç§‘äººæ‰åŸ¹å…»æ–¹æ¡ˆçš„åˆ†ææŠ¥å‘Šã€‚
                            **ç¬¬ä¸€éƒ¨åˆ†ï¼šäººæ‰åŸ¹å…»æ–¹å‘åˆ†ææŠ¥å‘Š**
                            1. **åŸ¹å…»ç›®æ ‡æ¦‚æ‹¬**: ç²¾ç‚¼åœ°æ€»ç»“è¯¥ä¸“ä¸šçš„æ ¸å¿ƒåŸ¹å…»ç›®æ ‡ã€‚
                            2. **æ ¸å¿ƒèƒ½åŠ›è¦æ±‚**: æ ¹æ®â€œæ¯•ä¸šè¦æ±‚â€ï¼Œæç‚¼å‡ºå­¦ç”Ÿéœ€è¦æŒæ¡çš„3-4é¡¹æœ€æ ¸å¿ƒçš„èƒ½åŠ›ã€‚
                            **ç¬¬äºŒéƒ¨åˆ†ï¼šå»ºè®®çš„èŒä¸šå‘å±•æ–¹å‘**
                            - åŸºäºä¸Šè¿°åˆ†æï¼Œç‰¹åˆ«æ˜¯åŸ¹å…»ç›®æ ‡ä¸­æåˆ°çš„å°±ä¸šé¢†åŸŸï¼Œæå‡º 3-5 ä¸ªå…·ä½“çš„èŒä¸šå‘å±•æ–¹å‘å»ºè®®ã€‚
                            - ä»¥é¡¹ç›®ç¬¦å·åˆ—è¡¨çš„å½¢å¼æ¸…æ™°å‘ˆç°ã€‚
                            æœ€åï¼Œè¯·æ˜ç¡®å¼•å¯¼ç”¨æˆ·ï¼šâ€œè¯·ä»ä»¥ä¸Šæ–¹å‘ä¸­é€‰æ‹©ä¸€ä¸ªæ‚¨æœ€æ„Ÿå…´è¶£çš„ï¼Œæˆ‘å°†ä¸ºæ‚¨ç”Ÿæˆä¸“å±çš„å­¦ä¹ è·¯å¾„è§„åˆ’å›¾ã€‚â€
                            åŸ¹å…»æ–¹æ¡ˆå…¨æ–‡å¦‚ä¸‹: {curriculum_content}
                            """
                        )
                        chain = prompt | llm
                        with st.chat_message("ai", avatar="ğŸ¤–"):
                            with st.spinner("AIå¯¼å¸ˆæ­£åœ¨æ·±åº¦åˆ†æåŸ¹å…»æ–¹æ¡ˆ..."):
                                response = st.write_stream(chain.stream({"curriculum_content": content}))
                                history.add_ai_message(response)
                        st.session_state.curriculum_stage = 2
                        st.rerun()

                    except Exception as e:
                        if "pytesseract" in str(e) or "pdf2image" in str(e):
                            st.error("é”™è¯¯ï¼šç¼ºå°‘OCRç›¸å…³åº“ã€‚è¯·è¿è¡Œ `pip install pytesseract pdf2image`ã€‚")
                        elif "Tesseract is not installed" in str(e) or "poppler" in str(e).lower():
                            st.error("é”™è¯¯ï¼šTesseract OCRå¼•æ“æˆ–Popplerå·¥å…·æœªå®‰è£…æˆ–æœªåœ¨ç³»ç»Ÿè·¯å¾„ä¸­ã€‚è¯·å‚è€ƒè¯´æ˜å®Œæˆå®‰è£…ã€‚")
                        else:
                            st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        else:
            st.button("ç¬¬ä¸€æ­¥ï¼šåˆ†æäººæ‰åŸ¹å…»æ–¹å‘", use_container_width=True, disabled=True)

    elif stage == 2:
        if user_input := st.chat_input("è¯·è¾“å…¥æ‚¨é€‰æ‹©çš„èŒä¸šæ–¹å‘..."):
            st.session_state.chosen_career = user_input
            history.add_user_message(user_input)
            prompt = ChatPromptTemplate.from_template(
                """
                æ ¸å¿ƒè§’è‰²: ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å¤§å­¦å­¦ä¸šå¯¼å¸ˆã€‚
                ä»»åŠ¡: ç”¨æˆ·é€‰æ‹©äº† **â€œ{career_path}â€** ä½œä¸ºèŒä¸šæ–¹å‘ã€‚è¯·ä¸ºä»–ç”Ÿæˆä¸€ä»½é‡ç‚¹ä¸“ä¸šç§‘ç›®å­¦ä¹ è§„åˆ’ã€‚
                ä½ çš„å›ç­”å¿…é¡»åŒ…å«ä»¥ä¸‹éƒ¨åˆ†:
                1.  **å­¦ä¹ è·¯å¾„è§„åˆ’è¯´æ˜**: é¦–å…ˆï¼Œç®€è¦é˜è¿°é’ˆå¯¹â€œ{career_path}â€æ–¹å‘ï¼Œå­¦ä¹ çš„é‡ç‚¹å’Œå»ºè®®çš„å…ˆåé¡ºåºã€‚
                2.  **å­¦ä¹ è·¯å¾„å…³è”å›¾ (Mermaid)**:
                    -   åˆ›å»ºä¸€ä¸ª `graph TD` ç±»å‹çš„Mermaidæµç¨‹å›¾ã€‚
                    -   **ã€è¯­æ³•é“å¾‹ã€‘**: å¦‚æœè¯¾ç¨‹åç§°ï¼ˆèŠ‚ç‚¹æ–‡æœ¬ï¼‰ä¸­åŒ…å«æ‹¬å· `()` æˆ–å…¶ä»–ç‰¹æ®Šç¬¦å·ï¼Œåˆ™**å¿…é¡»**å°†æ•´ä¸ªæ–‡æœ¬ç”¨åŒå¼•å· `""` æ‹¬èµ·æ¥ã€‚ä¾‹å¦‚ï¼š`C1["äººé™…äº¤å¾€å¿ƒç†å­¦(ç ”è®¨è¯¾)"]`ã€‚
                    -   **å¿…é¡»è¿›è¡Œé¢œè‰²æ ‡æ³¨**: å°† **æ ¸å¿ƒä¸“ä¸šè¯¾** èŠ‚ç‚¹èƒŒæ™¯è‰²è®¾ä¸º `#D1E8FF` (æ·¡è“è‰²)ï¼Œå°† **ç›¸å…³åŸºç¡€è¯¾** èŠ‚ç‚¹èƒŒæ™¯è‰²è®¾ä¸º `#FFF2CC` (æ·¡é»„è‰²)ã€‚
                    -   åœ¨Mermaidä»£ç å—çš„ **æœ€ä¸‹æ–¹**ï¼Œä½¿ç”¨ `style` å‘½ä»¤æ¥å®šä¹‰é¢œè‰²ã€‚
                    -   åœ¨å›¾è¡¨ä¸‹æ–¹ï¼Œå¿…é¡»æ·»åŠ å›¾ä¾‹è¯´æ˜ã€‚
                3.  **æ ¸å¿ƒè¯¾ç¨‹åˆ—è¡¨ (é‡è¦)**: åœ¨å›¾è¡¨å’Œå›¾ä¾‹ä¹‹åï¼Œè¯·å¦èµ·ä¸€è¡Œï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹**ä¸€å­—ä¸å·®çš„å›ºå®šæ ¼å¼**åˆ—å‡ºæ‰€æœ‰è¢«ä½ è¯†åˆ«ä¸ºâ€œæ ¸å¿ƒä¸“ä¸šè¯¾â€ï¼ˆå³æ·¡è“è‰²èŠ‚ç‚¹ï¼‰çš„è¯¾ç¨‹åç§°ã€‚è¿™æ˜¯ç¨‹åºèƒ½å¦ç»§ç»­è¿è¡Œçš„å…³é”®ã€‚
                    æ ¼å¼:
                    ### æ ¸å¿ƒè¯¾ç¨‹åˆ—è¡¨
                    - è¯¾ç¨‹A
                    - è¯¾ç¨‹B
                    - è¯¾ç¨‹C
                åŸ¹å…»æ–¹æ¡ˆå…¨æ–‡å‚è€ƒ: {curriculum_content}
                """
            )
            chain = prompt | llm
            with st.chat_message("ai", avatar="ğŸ¤–"):
                with st.spinner(f"æ­£åœ¨ä¸ºâ€œ{user_input}â€æ–¹å‘è§„åˆ’å­¦ä¹ è·¯å¾„..."):
                    response = st.write_stream(chain.stream({
                        "career_path": user_input,
                        "curriculum_content": st.session_state.curriculum_content
                    }))
                    history.add_ai_message(response)

                    # --- ä¼˜åŒ–çš„æ ¸å¿ƒè¯¾ç¨‹æå–é€»è¾‘ ---
                    key_courses = []
                    if "### æ ¸å¿ƒè¯¾ç¨‹åˆ—è¡¨" in response:
                        content_after_heading = response.split("### æ ¸å¿ƒè¯¾ç¨‹åˆ—è¡¨")[1]
                        matches = re.findall(r"^\s*[-*]\s+(.*)", content_after_heading, re.MULTILINE)
                        if matches:
                            key_courses = [course.strip() for course in matches]

                    if key_courses:
                        st.session_state.key_courses_identified = key_courses
                    else:
                        st.session_state.key_courses_identified = None  # ç¡®ä¿å¤±è´¥æ—¶çŠ¶æ€ä¸ºç©º

            st.session_state.curriculum_stage = 3
            st.rerun()

    elif stage == 3:
        st.info("å­¦ä¹ è·¯å¾„å›¾å·²ç”Ÿæˆã€‚ç°åœ¨ï¼ŒAIå°†ä¸ºæ‚¨è¯¦ç»†è§£è¯»å…¶ä¸­çš„æ ¸å¿ƒè¯¾ç¨‹ã€‚")
        if st.button("ç¬¬äºŒæ­¥ï¼šç”Ÿæˆæ ¸å¿ƒè¯¾ç¨‹æ•™å­¦ç›®çš„æŠ¥å‘Š", use_container_width=True, type="primary"):
            key_courses = st.session_state.get('key_courses_identified')
            if not key_courses:
                st.error("æœªèƒ½ä»ä¸Šä¸€æ­¥ä¸­è¯†åˆ«å‡ºæ ¸å¿ƒè¯¾ç¨‹åˆ—è¡¨ï¼Œè¯·è¿”å›ä¸Šä¸€æ­¥é‡è¯•ã€‚")
                st.stop()
            history.add_user_message(f"è¯·ä¸ºæˆ‘è¯¦ç»†è§£è¯»è¿™äº›æ ¸å¿ƒè¯¾ç¨‹ï¼š{', '.join(key_courses)}")
            prompt = ChatPromptTemplate.from_template(
                """
                æ ¸å¿ƒè§’è‰²: ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¯¾ç¨‹æ•™å­¦è®¾è®¡å¸ˆã€‚
                ä»»åŠ¡: è¯·ä¸ºä»¥ä¸‹ **æ ¸å¿ƒä¸“ä¸šè¯¾ç¨‹** ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„æ•™å­¦ç›®çš„ä¸è¦æ±‚æŠ¥å‘Šã€‚
                æ ¸å¿ƒè¯¾ç¨‹åˆ—è¡¨: **{key_courses_list}**
                è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ï¼Œä¸ºåˆ—è¡¨ä¸­çš„ **æ¯ä¸€é—¨** è¯¾ç¨‹è¿›è¡Œé˜è¿°:
                ### è¯¾ç¨‹åç§°ï¼š[ä¾‹å¦‚ï¼šå’¨è¯¢å¿ƒç†å­¦]
                -   **ğŸ“– çŸ¥è¯†ç›®æ ‡**: å­¦ç”Ÿé€šè¿‡æœ¬è¯¾ç¨‹å°†æŒæ¡å“ªäº›æ ¸å¿ƒç†è®ºã€æ¦‚å¿µå’ŒçŸ¥è¯†ä½“ç³»ã€‚
                -   **ğŸ› ï¸ èƒ½åŠ›ç›®æ ‡**: æœ¬è¯¾ç¨‹æ—¨åœ¨åŸ¹å…»å­¦ç”Ÿçš„å“ªäº›å…·ä½“æŠ€èƒ½ã€‚
                -   **ğŸŒŸ ç´ å…»ç›®æ ‡**: æœ¬è¯¾ç¨‹å¦‚ä½•å¸®åŠ©å­¦ç”Ÿå»ºç«‹æ­£ç¡®çš„ä»·å€¼è§‚ã€èŒä¸šé“å¾·æˆ–ç§‘å­¦ç²¾ç¥ã€‚
                ä½ éœ€è¦ç»“åˆæ•´ä¸ªåŸ¹å…»æ–¹æ¡ˆçš„ä¸Šä¸‹æ–‡æ¥è¿›è¡Œæ¨æ–­å’Œé˜è¿°ã€‚
                åŸ¹å…»æ–¹æ¡ˆå…¨æ–‡å‚è€ƒ: {curriculum_content}
                """
            )
            chain = prompt | llm
            with st.chat_message("ai", avatar="ğŸ¤–"):
                with st.spinner("æ­£åœ¨ç”Ÿæˆæ ¸å¿ƒè¯¾ç¨‹çš„è¯¦ç»†æ•™å­¦ç›®çš„æŠ¥å‘Š..."):
                    response = st.write_stream(chain.stream({
                        "key_courses_list": ", ".join(key_courses),
                        "curriculum_content": st.session_state.curriculum_content
                    }))
                    history.add_ai_message(response)
            st.session_state.curriculum_stage = 4
            st.rerun()

    elif stage == 4:
        st.success("ğŸ‰ ä¸“ä¸šåŸ¹å…»æ–¹æ¡ˆè§£æå·²å…¨éƒ¨å®Œæˆï¼å¸Œæœ›è¿™ä»½è¯¦ç»†çš„å­¦ä¸šè§„åˆ’æŠ¥å‘Šèƒ½ä¸ºä½ çš„å­¦ä¹ ä¹‹æ—…ç‚¹äº®ä¸€ç›æ˜ç¯ã€‚")


def main():
    llm = get_llm_instance()
    if not llm:
        st.error("æ— æ³•åˆå§‹åŒ–è¯­è¨€æ¨¡å‹ï¼Œåº”ç”¨ç¨‹åºæ— æ³•å¯åŠ¨ã€‚è¯·æ£€æŸ¥æ‚¨çš„ API Key è®¾ç½®ã€‚")
        st.stop()
    with st.sidebar:
        if st.session_state.get("current_mode", "menu") != "menu":
            if st.button("â†©ï¸ è¿”å›ä¸»èœå•"):
                st.session_state.clear()
                st.session_state.current_mode = "menu"
                st.rerun()
        st.markdown("---")
        st.caption("Â© 2025 æ™ºæ…§èŒä¸šè¾…å¯¼ V14.3 (ç¨³å®šç‰ˆ)")
    modes = {
        "menu": render_menu,
        "exploration": render_exploration_mode,
        "decision": render_decision_mode,
        "communication": render_communication_mode,
        "company_info": render_company_info_mode,
        "panoramic": render_panoramic_mode,
        "curriculum_analysis": render_curriculum_mode,
    }
    mode_func = modes.get(st.session_state.get("current_mode", "menu"), render_menu)
    if st.session_state.get("current_mode", "menu") == 'menu':
        mode_func()
    else:
        mode_func(llm)


if __name__ == "__main__":
    main()