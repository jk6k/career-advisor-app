import os
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
# --- UPDATED IMPORT for ChatMessageHistory ---
# The warning suggests importing from langchain_community.chat_message_histories
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
# --- UPDATED IMPORTS ---
# We will use the generic ChatOpenAI client which allows specifying a custom API endpoint.
from langchain_openai import ChatOpenAI
import time
import textwrap
# ADDED: Import the dotenv library to load the .env file
from dotenv import load_dotenv

# ADDED: Load environment variables from the .env file
load_dotenv()

# --- FIX for LangSmith Error ---
# Explicitly disable LangSmith tracing to prevent connection errors
# if the user doesn't have it configured.
os.environ["LANGCHAIN_TRACING_V2"] = "false"

# --- 1. ä»è®¾è®¡æ–‡æ¡£ä¸­æå–å„é˜¶æ®µçš„æç¤ºè¯ ---
# --- UPDATED: Rephrased all prompts to give the AI its own voice ---
PROMPTS = {
    1: {
        "title": "é˜¶æ®µä¸€ï¼šå¯åŠ¨ä¸å¯¼å…¥ â€”â€” â€œæˆ‘æ˜¯è°ï¼Ÿâ€",
        "prompt": """ä½ å¥½ï¼æˆ‘æ˜¯ä¸€æ¬¾èŒä¸šç›®æ ‡è§„åˆ’è¾…åŠ©AIã€‚æˆ‘å°†é€šè¿‡ä¸€ä¸ªç»è¿‡éªŒè¯çš„åˆ†ææ¡†æ¶ï¼Œå¼•å¯¼ä½ æ›´å…·ä½“ã€æ›´ç³»ç»Ÿåœ°æ€è€ƒâ€˜èŒä¸šç›®æ ‡æ˜¯æ€ä¹ˆæ¥çš„â€™ï¼Œå¹¶æœ€ç»ˆæ‰¾åˆ°å±äºä½ è‡ªå·±çš„æ–¹å‘ã€‚

è®©æˆ‘ä»¬ä»æ ¸å¿ƒå¼€å§‹ï¼Œä¹Ÿå°±æ˜¯â€˜æˆ‘â€™ã€‚è¯·ä½ ç”¨å‡ ä¸ªå…³é”®è¯æˆ–çŸ­å¥å…·ä½“æè¿°ä¸€ä¸‹ï¼š

1. ä½ çš„ä¸“ä¸š/ä¸ªäººå…´è¶£ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ (ä¾‹å¦‚ï¼šæ³•å¾‹ä¸­çš„å•†æ³•ã€AIç»˜ç”»ã€å¸‚åœºè¥é”€æ¡ˆä¾‹åˆ†æ)
2. ä½ è®¤ä¸ºè‡ªå·±æœ€æ“…é•¿çš„ä¸‰é¡¹èƒ½åŠ›æ˜¯ä»€ä¹ˆï¼Ÿ (ä¾‹å¦‚ï¼šé€»è¾‘åˆ†æã€å…¬å¼€æ¼”è®²ã€è§†é¢‘å‰ªè¾‘)
3. åœ¨æœªæ¥çš„å·¥ä½œä¸­ï¼Œä½ æœ€çœ‹é‡çš„æ˜¯ä»€ä¹ˆï¼Ÿ(å¯å¤šé€‰) (ä¾‹å¦‚ï¼šé«˜è–ªé…¬ã€å·¥ä½œç¨³å®šã€èƒ½å¸®åŠ©ä»–äººã€æŒç»­å­¦ä¹ ã€åˆ›é€ æ€§å¼º)"""
    },
    2: {
        "title": "é˜¶æ®µäºŒï¼šå¯åˆ†æå› ç´ è§£æ„ â€”â€” â€œæˆ‘æ‹¥æœ‰ä»€ä¹ˆå¹³å°å’Œæœºä¼šï¼Ÿâ€",
        "prompt": """ç°åœ¨ï¼Œæˆ‘ä»¬æ¥åˆ†æä¸€äº›ä½ å¯ä»¥é˜¶æ®µæ€§å®¡è§†çš„å¤–éƒ¨å› ç´ ã€‚

1. å…³äºã€å¤§å­¦å¹³å°ã€‘:
   * ä½ æ‰€åœ¨çš„å¤§å­¦æˆ–ä¸“ä¸šï¼Œåœ¨å“ªäº›é¢†åŸŸè¢«è®¤ä¸ºæ˜¯ä¼˜åŠ¿å­¦ç§‘ï¼Ÿ(ä¾‹å¦‚ï¼Œä¸€æ‰€ä»¥æ³•å­¦è§é•¿çš„å¤§å­¦ï¼Œå¯èƒ½åœ¨ä»²è£ã€è­¦åŠ¡ç­‰é¢†åŸŸæœ‰ç‹¬ç‰¹çš„å°±ä¸šä¼˜åŠ¿)
   * é€šå¸¸æœ‰å“ªäº›ç±»å‹çš„ä¼ä¸šä¼šæ¥ä½ ä»¬å­¦æ ¡æˆ–å­¦é™¢è¿›è¡Œæ‹›è˜ï¼Ÿè¿™ç»™äº†ä½ ä»€ä¹ˆå…·ä½“çš„å¯å‘ï¼Ÿ
   * ä½ æ‰€åœ¨çš„åŸå¸‚ä¸ºä½ æä¾›äº†å“ªäº›ç‹¬ç‰¹çš„äº§ä¸šæœºä¼šï¼Ÿ(ä¾‹å¦‚ï¼Œå†…è’™å¤é„‚å°”å¤šæ–¯çš„ç…¤ç‚­äº§ä¸š)

2. å…³äºã€è¶‹åŠ¿ã€‘:
   * ä½ æ³¨æ„åˆ°äº†å“ªäº›æ­£åœ¨å‘ç”Ÿçš„ç¤¾ä¼šæˆ–æŠ€æœ¯è¶‹åŠ¿ï¼Ÿ(ä¾‹å¦‚ï¼šAIå‘å±•ã€è€é¾„åŒ–ã€æ–°èƒ½æº)
   * è¯·å…·ä½“æ€è€ƒï¼Œè¿™äº›è¶‹åŠ¿å¯èƒ½ä¼šå‚¬ç”Ÿå‡ºå“ªäº›ä¸ä½ çš„ä¸“ä¸šæˆ–å…´è¶£ç›¸å…³çš„**æ–°è¡Œä¸š**æˆ–**æ–°å²—ä½**ï¼Ÿ(ä¾‹å¦‚ï¼Œè€é¾„åŒ–è¶‹åŠ¿è®©å…»è€äº§ä¸šçš„éœ€æ±‚å˜å¾—æ›´æ—ºç››)

3. å…³äºã€æœºç¼˜ã€‘:
   * å›æƒ³ä¸€ä¸‹ï¼Œæ˜¯å¦æœ‰æŸæ¬¡å¶ç„¶çš„ç»å†ï¼ˆå¦‚ä¸€æ¬¡è®²åº§ã€ä¸æŸäººçš„è°ˆè¯ã€ä¸€ä¸ªé¡¹ç›®ï¼‰è®©ä½ å¯¹æŸä¸ªèŒä¸šé¢†åŸŸäº§ç”Ÿäº†æ–°çš„å…´è¶£ï¼Ÿæœ‰æ—¶å€™ï¼Œä¸€æ¬¡ä¸ç»æ„çš„æ¥è§¦å°±å¯èƒ½å¼€å¯ä¸€æ‰‡èŒä¸šå¤§é—¨ã€‚"""
    },
    3: {
        "title": "é˜¶æ®µä¸‰ï¼šéœ€æŒç»­è§‰å¯Ÿå› ç´ çš„æŒ–æ˜ â€”â€” â€œæˆ‘è¢«ä»€ä¹ˆæ‰€å½±å“ï¼Ÿâ€",
        "prompt": """æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¢è®¨ä¸€äº›éœ€è¦æŒç»­â€˜è§‰å¯Ÿâ€™çš„ã€æ›´æ„Ÿæ€§çš„å½±å“å› ç´ ã€‚è¿™éƒ¨åˆ†è®¨è®ºå¯èƒ½æ¯”è¾ƒä¸ªäººåŒ–ï¼Œè¯·å¦è¯šåœ°é¢å¯¹è‡ªå·±çš„æƒ³æ³•ã€‚

1. å…³äºã€å®¶åº­ä¸ç¤¾ä¼šæœŸå¾…ã€‘:
   * ä½ çš„é•¿è¾ˆæˆ–å®¶åº­å¯¹ä½ çš„èŒä¸šæœ‰ä»€ä¹ˆ**å…·ä½“**çš„æœŸæœ›å—ï¼Ÿ(ä¾‹å¦‚ï¼šå¸Œæœ›ä½ è€ƒå…¬åŠ¡å‘˜ï¼Œè¿›å…¥æŸä¸ªç‰¹å®šè¡Œä¸š)
   * ä½ èº«è¾¹çš„æœ‹å‹æˆ–â€˜åœˆå­â€™ï¼Œä»–ä»¬çš„èŒä¸šè§„åˆ’æ˜¯æ€æ ·çš„ï¼Ÿè¿™å¯¹ä½ äº§ç”Ÿäº†ä»€ä¹ˆå½±å“ï¼Ÿ

2. å…³äºã€æ¦œæ ·ã€‘:
   * æ˜¯å¦æœ‰ä½ éå¸¸æ•¬ä½©çš„æŸä¸ªæ¦œæ ·äººç‰©ï¼ˆå…¬ä¼—äººç‰©ã€è€å¸ˆã€å­¦é•¿å­¦å§ç­‰ï¼‰ï¼Ÿ
   * ä½ æ¬£èµçš„æ˜¯ä»–/å¥¹çš„**èŒä¸šæœ¬èº«**ï¼Œè¿˜æ˜¯ä»–/å¥¹å·¥ä½œå¸¦æ¥çš„æŸç§**çŠ¶æ€**æˆ–å…¶**â€˜å‘¨è¾¹â€™**ä»·å€¼ï¼ˆå¦‚ç¤¾ä¼šå½±å“åŠ›ã€ç”Ÿæ´»æ–¹å¼ï¼‰ï¼Ÿ"""
    },
    4: {
        "title": "é˜¶æ®µå››ï¼šæ ¸å¿ƒä¸‰è§’å…³ç³»æ•´åˆä¸å†³ç­–æ¨¡æ‹Ÿ",
        "prompt": """éå¸¸æ£’çš„æ·±å…¥æ€è€ƒï¼ç°åœ¨ï¼Œè®©æˆ‘ä»¬æŠŠè¿™äº›ç¢ç‰‡åŒ–çš„ä¿¡æ¯æ•´åˆèµ·æ¥ï¼Œçœ‹çœ‹â€˜æˆ‘â€™ã€â€˜ç¤¾ä¼šâ€™ã€â€˜å®¶åº­â€™è¿™ä¸ªæ ¸å¿ƒä¸‰è§’å…³ç³»ã€‚

æ ¹æ®æˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯ï¼Œæˆ‘ä¸ºä½ åšä¸€ä¸ªç®€å•çš„å›é¡¾ä¸æ€»ç»“ï¼š
{history}

**å†³ç­–æ¨¡æ‹Ÿï¼š**
1. ç»“åˆä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œè¯·å°è¯•æ„æ€1-2ä¸ªå¯èƒ½çš„èŒä¸šæ–¹å‘ã€‚
2. é’ˆå¯¹ä½ æ„æ€çš„èŒä¸šæ–¹å‘ï¼Œæˆ‘ä»¬æ¥æ€è€ƒä¸€ä¸ªéå¸¸ç°å®çš„é—®é¢˜ï¼š**â€˜å¦‚ä½•(æ”¶å…¥)â€™**ã€‚ä½ æœŸæœ›çš„å›æŠ¥æ˜¯ä»€ä¹ˆï¼Ÿæ˜¯ç›´æ¥çš„é‡‘é’±ï¼Œè¿˜æ˜¯åŒ…æ‹¬ç¤¾ä¼šä»·å€¼ã€ä¸ªäººæˆå°±æ„Ÿåœ¨å†…çš„é—´æ¥å›æŠ¥ï¼Ÿ
3. å¦‚æœä½ çš„ç†æƒ³æ–¹å‘ä¸å®¶åº­æœŸå¾…å­˜åœ¨çŸ›ç›¾ï¼Œç›´æ¥å¯¹æŠ—å¾€å¾€æ•ˆæœä¸ä½³ã€‚ä¸€ä¸ªæ›´æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ï¼Œé€šè¿‡ç³»ç»Ÿçš„åˆ†æï¼Œæ‹¿å‡ºæ•°æ®å’Œäº‹å®è¿›è¡Œæ²Ÿé€šã€‚ä½ è®¤ä¸ºå¯ä»¥æ”¶é›†å“ªäº›ä¿¡æ¯æ¥å’Œå®¶äººå¼€ä¸€æ¬¡æœ‰ç†æœ‰æ®çš„â€˜å®¶åº­ä¼šè®®â€™å‘¢ï¼Ÿ"""
    },
    5: {
        "title": "é˜¶æ®µäº”ï¼šæ€»ç»“ä¸è¡ŒåŠ¨ â€”â€” â€œå¦‚ä½•åšåˆ°åšå®šè€Œçµæ´»ï¼Ÿâ€",
        "prompt": """æˆ‘ä»¬çš„æ¢è®¨å³å°†ç»“æŸã€‚èŒä¸šè§„åˆ’ä¸æ˜¯ä¸€æ¬¡æ€§çš„ç»ˆç‚¹ï¼Œè€Œæ˜¯ä¸€ä¸ªæŒç»­ä¼˜åŒ–çš„è¿‡ç¨‹ã€‚æ ¸å¿ƒæ˜¯è¾¾åˆ°**â€˜åšå®šè€Œçµæ´»â€™**çš„çŠ¶æ€ï¼šåŸºäºç†æ€§åˆ†æè€Œæ¥çš„æ–¹å‘æ˜¯â€˜åšå®šâ€™çš„ï¼Œä½†éšæ—¶å‡†å¤‡æ ¹æ®ç¯å¢ƒå˜åŒ–å’Œæ–°çš„è®¤çŸ¥è¿›è¡Œâ€˜çµæ´»â€™è°ƒæ•´ã€‚

è¯·è®°ä½ï¼Œå¤§å­¦é˜¶æ®µæ˜¯è¿›è¡ŒèŒä¸šæ¢ç´¢å’Œâ€˜è¯•é”™â€™æˆæœ¬æœ€ä½çš„æ—¶æœŸã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬ä»Šå¤©è®¨è®ºå‡ºçš„èŒä¸šæ–¹å‘ï¼Œä½ å¯ä»¥ç«‹å³é‡‡å–çš„**ç¬¬ä¸€ä¸ªå°æ­¥éª¤**æ˜¯ä»€ä¹ˆï¼Ÿ

* A. é€šè¿‡å®ä¹ æˆ–å‹¤å·¥ä¿­å­¦å»äº²èº«ä½“éªŒï¼Ÿ
* B. æ‰¾ä¸€ä½è¯¥é¢†åŸŸçš„å­¦é•¿å­¦å§æˆ–ä»ä¸šè€…æ·±å…¥äº¤æµï¼Ÿ
* C. å­¦ä¹ ä¸€é¡¹ä¸è¯¥æ–¹å‘ç›¸å…³çš„å…·ä½“æŠ€èƒ½ï¼ˆå¦‚è§†é¢‘å‰ªè¾‘ã€æ•°æ®åˆ†æï¼‰ï¼Ÿ
* D. å…¶ä»–ï¼š__________

è¯·é€‰æ‹©å¹¶å…·ä½“æè¿°ä½ çš„è¡ŒåŠ¨è®¡åˆ’ã€‚è®°ä½ï¼Œâ€˜æœºç¼˜æ¥è‡ªäºæ¥è§¦â€™ï¼Œè¡ŒåŠ¨èµ·æ¥æ‰èƒ½è®©è§„åˆ’å˜å¾—çœŸæ­£å…·ä½“ï¼"""
    }
}

# --- ADDED: Test Case Data ---
TEST_ANSWERS = [
    "æˆ‘çš„ä¸“ä¸šæ˜¯æ³•å­¦ï¼Œä½†æˆ‘å¯¹AIæŠ€æœ¯å’Œå®ƒå¦‚ä½•å½±å“ç¤¾ä¼šå¾ˆæ„Ÿå…´è¶£ã€‚æˆ‘æ“…é•¿èµ„æ–™ç ”ç©¶ã€é€»è¾‘åˆ†æå’Œå†™ä½œã€‚æˆ‘å¸Œæœ›æœªæ¥å·¥ä½œèƒ½æœ‰æŒ‘æˆ˜æ€§ï¼Œå¹¶ä¸”èƒ½æŒç»­å­¦ä¹ æ–°çŸ¥è¯†ã€‚",
    "æˆ‘ä»¬å­¦æ ¡çš„æ³•å­¦é™¢å¾ˆæœ‰åï¼Œç»å¸¸æœ‰å¾‹æ‰€å’Œæ³•é™¢æ¥æ‹›è˜ã€‚ä½†æˆ‘æ‰€åœ¨çš„åŸå¸‚ä¹Ÿæ˜¯ä¸€ä¸ªç§‘æŠ€ä¸­å¿ƒï¼Œæœ‰å¾ˆå¤šAIåˆ›ä¸šå…¬å¸ã€‚æˆ‘æ³¨æ„åˆ°AIæ­£åœ¨é¢ è¦†æ³•å¾‹è¡Œä¸šï¼Œæ¯”å¦‚AIåˆåŒå®¡æŸ¥å·¥å…·ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªæ–°çš„æ–¹å‘ã€‚æˆ‘æœ€è¿‘å‚åŠ äº†ä¸€ä¸ªå…³äº'æ³•å¾‹ç§‘æŠ€'çš„è®²åº§ï¼Œæ„Ÿè§‰å¾ˆæœ‰å¯å‘ã€‚",
    "æˆ‘çš„å®¶äººå¸Œæœ›æˆ‘èƒ½æˆä¸ºä¸€åç¨³å®šçš„å¾‹å¸ˆæˆ–è€ƒå…¬åŠ¡å‘˜ã€‚æˆ‘æ•¬ä½©çš„ä¸€ä½æ¦œæ ·æ˜¯ä¸€ä½ç”¨æŠ€æœ¯åˆ›ä¸šçš„å¾‹å¸ˆï¼Œä»–åˆ›åŠäº†ä¸€ä¸ªåœ¨çº¿æ³•å¾‹æœåŠ¡å¹³å°ï¼Œæˆ‘è§‰å¾—ä»–ä¸ä»…æ‡‚æ³•å¾‹ï¼Œè¿˜å¾ˆæœ‰å•†ä¸šå¤´è„‘ï¼Œè¿™å¾ˆé…·ã€‚",
    "æˆ‘æƒ³æˆ‘å¯èƒ½ä¼šè€ƒè™‘ä¸¤ä¸ªæ–¹å‘ï¼š1. æˆä¸ºä¸€åä¸“æ³¨äºç§‘æŠ€ã€åª’ä½“å’Œç”µä¿¡ï¼ˆTMTï¼‰é¢†åŸŸçš„å¾‹å¸ˆã€‚2. åŠ å…¥ä¸€å®¶æ³•å¾‹ç§‘æŠ€å…¬å¸çš„æ³•åŠ¡æˆ–äº§å“éƒ¨é—¨ã€‚å¯¹äºæ”¶å…¥ï¼Œæˆ‘æ›´çœ‹é‡é•¿æœŸçš„æˆé•¿æ½œåŠ›å’Œå·¥ä½œçš„åˆ›é€ æ€§ã€‚å¦‚æœå’Œå®¶äººæ²Ÿé€šï¼Œæˆ‘ä¼šå‡†å¤‡ä¸€äº›å…³äºæ³•å¾‹ç§‘æŠ€è¡Œä¸šå‘å±•è¶‹åŠ¿çš„æŠ¥å‘Šï¼Œä»¥åŠä¸€äº›æ–°å‹æ³•å¾‹å²—ä½çš„è–ªé…¬æ•°æ®æ¥å’Œä»–ä»¬è®¨è®ºã€‚",
    "æˆ‘ä¼šé€‰æ‹© B å’Œ Cã€‚æˆ‘è®¡åˆ’å…ˆå»è”ç³»é‚£ä½åšæ³•å¾‹ç§‘æŠ€åˆ›ä¸šçš„å­¦é•¿ï¼Œå‘ä»–è¯·æ•™ç»éªŒã€‚åŒæ—¶ï¼Œæˆ‘ä¼šå¼€å§‹å­¦ä¹ ä¸€äº›åŸºç¡€çš„Pythonç¼–ç¨‹çŸ¥è¯†ï¼Œäº†è§£æŠ€æœ¯äº§å“æ˜¯å¦‚ä½•å¼€å‘çš„ã€‚"
]


# --- 2. è¾…åŠ©å‡½æ•°å’Œä¸»é€»è¾‘ ---

def print_formatted(text, prefix="AI: "):
    """Helper function to print text with wrapping."""
    wrapped_text = textwrap.fill(text, width=80, subsequent_indent='    ', replace_whitespace=False)
    print(f"\n{prefix}{wrapped_text}\n")


def get_llm_instance():
    """Initializes and returns the LLM instance."""
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        print("\n" + "â”€" * 80)
        print(">>>>> é”™è¯¯ï¼šæœªæ‰¾åˆ°APIå¯†é’¥! <<<<<")
        print("\nè¯·ç¡®è®¤æ‚¨çš„é¡¹ç›®æ–‡ä»¶å¤¹ä¸­åŒ…å« .env æ–‡ä»¶ï¼Œå¹¶ä¸”è¯¥æ–‡ä»¶ä¸­å·²è®¾ç½® DEEPSEEK_API_KEYã€‚")
        print("â”€" * 80 + "\n")
        return None

    try:
        llm = ChatOpenAI(
            model="deepseek-r1-250528",
            temperature=0.7,
            api_key=api_key,
            base_url="https://ark.cn-beijing.volces.com/api/v3"
        )
        print("æ­£åœ¨è¿æ¥ç«å±±æ–¹èˆŸï¼ˆVolcEngine Arkï¼‰API...")
        llm.invoke("Hello")
        print("è¿æ¥æˆåŠŸï¼")
        return llm
    except Exception as e:
        print("\n" + "â”€" * 80)
        print(">>>>> åˆå§‹åŒ–æ¨¡å‹æ—¶å‡ºé”™! <<<<<")
        print(f"Error Details: {e}")
        print("\n[å¯èƒ½çš„åŸå› ä¸è§£å†³æ–¹æ³•]")
        print("1. APIå¯†é’¥æˆ–Endpointé”™è¯¯: 'Authentication Fails' æˆ–ç±»ä¼¼é”™è¯¯è¡¨æ˜æ‚¨çš„å¯†é’¥æˆ–APIåœ°å€ä¸æ­£ç¡®ã€‚")
        print("   -> è¯·æ£€æŸ¥æ‚¨çš„ .env æ–‡ä»¶ä¸­çš„ DEEPSEEK_API_KEY æ˜¯å¦æ­£ç¡®ã€‚")
        print("   -> ç¡®è®¤APIåœ°å€ 'https://ark.cn-beijing.volces.com/api/v3' æ˜¯å¦ä¸ºæ‚¨çš„æœåŠ¡å•†æä¾›çš„æ­£ç¡®åœ°å€ã€‚")
        print("2. ç½‘ç»œé—®é¢˜: æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥æ˜¯å¦å¯ä»¥è®¿é—®ç«å±±æ–¹èˆŸçš„APIæœåŠ¡å™¨ã€‚")
        print("3. ä¾èµ–åº“é—®é¢˜: è¯·ç¡®ä¿æ‚¨å·²å®‰è£… `langchain-openai` (pip install langchain-openai)ã€‚")
        print("â”€" * 80 + "\n")
        return None


def run_prototype(llm):
    """Runs the main interactive prototype."""
    store = {}

    def get_session_history(session_id: str) -> ChatMessageHistory:
        if session_id not in store:
            store[session_id] = ChatMessageHistory()
        return store[session_id]

    current_stage = 1
    while current_stage <= len(PROMPTS):
        stage_info = PROMPTS[current_stage]
        title = stage_info["title"]
        system_prompt = stage_info["prompt"]

        if current_stage == 4:
            history_messages = get_session_history("session").messages
            history_summary = "\n".join([f"{msg.type.capitalize()}: {msg.content}" for msg in history_messages])
            final_system_prompt = system_prompt.format(history=history_summary)
        else:
            final_system_prompt = system_prompt

        meta_prompt = f"""You are a thoughtful and insightful career planning coach... (rest of meta_prompt)"""
        prompt = ChatPromptTemplate.from_messages(
            [("system", meta_prompt), MessagesPlaceholder(variable_name="history"), ("human", "{input}")])
        chain = prompt | llm
        runnable_with_history = RunnableWithMessageHistory(chain, get_session_history, input_messages_key="input",
                                                           history_messages_key="history")

        print("â”€" * 80)
        print(f"                                   {title}")
        print("â”€" * 80)

        if current_stage > 1:
            pass
        else:
            print_formatted(system_prompt, prefix="")

        print("\nğŸ’¡ è¯·é’ˆå¯¹ä»¥ä¸Šé—®é¢˜ï¼Œå°†ä½ çš„æƒ³æ³•å’Œæ€è€ƒä¸€æ¬¡æ€§è¾“å…¥ï¼Œç„¶åæŒ‰Enteré”®ã€‚")
        user_input = input("ä½ çš„å›ç­”: ")

        if user_input.lower() in ['quit', 'exit']:
            print_formatted("å¥½çš„ï¼Œå¯¹è¯ç»“æŸã€‚ç¥ä½ ä¸€åˆ‡é¡ºåˆ©ï¼")
            break

        print("\n[AIæ­£åœ¨åˆ†ææ‚¨çš„å›ç­”å¹¶æä¾›å»ºè®®ï¼Œè¯·ç¨å€™...]")
        response = runnable_with_history.invoke({"input": user_input},
                                                config={"configurable": {"session_id": "session"}})
        print_formatted(response.content)

        time.sleep(2)
        current_stage += 1

        if current_stage <= len(PROMPTS):
            print_formatted(PROMPTS[current_stage]["prompt"], prefix="")

    print("â”€" * 80)
    print("æ‰€æœ‰é˜¶æ®µå·²å®Œæˆï¼Œæ„Ÿè°¢æ‚¨çš„å‚ä¸ï¼")
    print("â”€" * 80)


def run_test_case(llm):
    """Runs an automated test case with predefined answers."""
    print("\n" + "=" * 30 + " å¼€å§‹è‡ªåŠ¨åŒ–æµ‹è¯• " + "=" * 30)

    # Use a copy of the test answers for the run
    test_answers_for_run = TEST_ANSWERS.copy()

    store = {}

    def get_session_history(session_id: str) -> ChatMessageHistory:
        if session_id not in store:
            store[session_id] = ChatMessageHistory()
        return store[session_id]

    current_stage = 1
    while current_stage <= len(PROMPTS):
        if not test_answers_for_run:
            print("\n[æµ‹è¯•é”™è¯¯] æµ‹è¯•ç­”æ¡ˆä¸è¶³ï¼Œæµ‹è¯•æå‰ä¸­æ­¢ã€‚")
            break

        stage_info = PROMPTS[current_stage]
        title = stage_info["title"]
        system_prompt = stage_info["prompt"]

        if current_stage == 4:
            history_messages = get_session_history("test_session").messages
            history_summary = "\n".join([f"{msg.type.capitalize()}: {msg.content}" for msg in history_messages])
            final_system_prompt = system_prompt.format(history=history_summary)
        else:
            final_system_prompt = system_prompt

        meta_prompt = f"""You are a thoughtful and insightful career planning coach... (rest of meta_prompt)"""
        prompt = ChatPromptTemplate.from_messages(
            [("system", meta_prompt), MessagesPlaceholder(variable_name="history"), ("human", "{input}")])
        chain = prompt | llm
        runnable_with_history = RunnableWithMessageHistory(chain, get_session_history, input_messages_key="input",
                                                           history_messages_key="history")

        print("â”€" * 80)
        print(f"                                   {title}")
        print("â”€" * 80)

        if current_stage > 1:
            pass
        else:
            print_formatted(system_prompt, prefix="")

        # Simulate user input from the test data
        user_input = test_answers_for_run.pop(0)
        print("\nğŸ’¡ é¢„è®¾çš„å›ç­”æ˜¯:")
        print_formatted(user_input, prefix="æ¨¡æ‹Ÿç”¨æˆ·: ")

        print("\n[AIæ­£åœ¨åˆ†ææ‚¨çš„å›ç­”å¹¶æä¾›å»ºè®®ï¼Œè¯·ç¨å€™...]")
        response = runnable_with_history.invoke({"input": user_input},
                                                config={"configurable": {"session_id": "test_session"}})
        print_formatted(response.content)

        time.sleep(2)
        current_stage += 1

        if current_stage <= len(PROMPTS):
            print_formatted(PROMPTS[current_stage]["prompt"], prefix="")

    print("\n" + "=" * 30 + " è‡ªåŠ¨åŒ–æµ‹è¯•ç»“æŸ " + "=" * 30)


if __name__ == "__main__":
    print("AIèŒä¸šè§„åˆ’åŠ©æ‰‹åŸå‹å¯åŠ¨ (ä½¿ç”¨ç«å±±æ–¹èˆŸDeepSeekæ¨¡å‹)...")

    llm_instance = get_llm_instance()

    if llm_instance:
        while True:
            choice = input("\nè¯·é€‰æ‹©è¿è¡Œæ¨¡å¼: \n1. äº¤äº’æ¨¡å¼\n2. è¿è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•ç”¨ä¾‹\n\nè¯·è¾“å…¥é€‰é¡¹ (1æˆ–2): ")
            if choice == '1':
                run_prototype(llm_instance)
                break
            elif choice == '2':
                run_test_case(llm_instance)
                break
            else:
                print("æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 1 æˆ– 2ã€‚")

